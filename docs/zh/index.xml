<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>KubeSphere | 面向云原生应用的容器混合云 on </title>
    <link>https://cnimages.github.io/website/zh/</link>
    <description>Recent content in KubeSphere | 面向云原生应用的容器混合云 on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    
	<atom:link href="https://cnimages.github.io/website/zh/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What is Workspace</title>
      <link>https://cnimages.github.io/website/zh/docs/workspace-administration/what-is-workspace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/workspace-administration/what-is-workspace/</guid>
      <description>Workspace is a logical unit to organize your projects, DevOps projects and application management. It is the place for you to manage resource access and share resources within your team in a secure way.
It is a best practice to create a new workspace for a tenant (non-cluster admin). A tenant can work in multiple workspaces, while a workspace allows multiple tenants to acces in different access modes.</description>
    </item>
    
    <item>
      <title>Release Notes For 3.0.0</title>
      <link>https://cnimages.github.io/website/zh/docs/release/release-v300/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/release/release-v300/</guid>
      <description>How to get v3.0.0  Install KubeSphere v3.0.0 on Linux Install KubeSphere v3.0.0 on existing Kubernetes  Release Notes Installer FEATURES  A brand-new installer: KubeKey, v1.0.0, which is a turnkey solution to installing Kubernetes with KubeSphere on different platforms. It is more easy to use and reduces the dependency on OS environment  UPGRADES &amp;amp; ENHANCEMENTS  Be compatible with Kubernetes 1.15.x, 1.16.x, 1.17.x and 1.18.x for ks-installer, v3.0.0 KubeKey officially supports Kubernetes 1.</description>
    </item>
    
    <item>
      <title>Upload Helm-based Application</title>
      <link>https://cnimages.github.io/website/zh/docs/workspace-administration/upload-helm-based-application/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/workspace-administration/upload-helm-based-application/</guid>
      <description>KubeSphere provides full lifecycle management for applications. You can upload or create new app templates and test them quickly. In addition, you can publish your apps to App Store so that other users can deploy with one click. You can upload Helm Chart to develop app templates.
Prerequisites You need to create a workspace and project-admin account. Please refer to the Create Workspace, Project, Account and Role if not yet.</description>
    </item>
    
    <item>
      <title>Import Helm Repository</title>
      <link>https://cnimages.github.io/website/zh/docs/workspace-administration/import-helm-repository/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/workspace-administration/import-helm-repository/</guid>
      <description>KubeSphere builds application repository services on OpenPitrix, the open source cross-cloud application management platform from QingCloud, which supports for Kubernetes applications based on Helm Chart. In an application repository, each application is a base package repository and if you want to use OpenPitrix for application management, you need to create the repository first. You can store packages to an HTTP/HTTPS server, a minio, or an S3 object storage. The application repository is an external storage independent of OpenPitrix, which can be minio, QingCloud&amp;rsquo;s QingStor object storage, or AWS object storage, in which the contents are the configuration packages of the application developed by developers.</description>
    </item>
    
    <item>
      <title>Release Notes For 2.1.1</title>
      <link>https://cnimages.github.io/website/zh/docs/release/release-v211/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/release/release-v211/</guid>
      <description>KubeSphere 2.1.1 was released on Feb 23rd, 2020, which has fixed known bugs and brought some enhancements. For the users who have installed versions of 2.0.x or 2.1.0, make sure to read the user manual carefully about how to upgrade before doing that, and feel free to raise any questions on GitHub.
What&amp;rsquo;s New in 2.1.1 Installer UPGRADE &amp;amp; ENHANCEMENT  Support Kubernetes v1.14.x、v1.15.x、v1.16.x、v1.17.x，also solve the issue of Kubernetes API Compatibility#1829 Simplify the steps of installation on existing Kubernetes, and remove the step of specifying cluster&amp;rsquo;s CA certification, also specifying Etcd certification is no longer mandatory step if users don&amp;rsquo;t need Etcd monitoring metrics Backup the configuration of CoreDNS before upgrading  BUG FIXES  Fix the issue of importing apps to App Store  App Store UPGRADE &amp;amp; ENHANCEMENT  Upgrade OpenPitrix to v0.</description>
    </item>
    
    <item>
      <title>StorageClass</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/storageclass/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/storageclass/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cluster Visibility and Authorization</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/cluster-visibility-and-authorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/cluster-visibility-and-authorization/</guid>
      <description>Objective This guide demonstrates how to set up cluster visibility. You can limit which clusters workspace can use with cluster visibility settings.
Prerequisites  You need to enable Multi-cluster Management. You need to create at least one workspace.  Set cluster visibility In KubeSphere, clusters can be authorized to multiple workspaces, and workspaces can also be associated with multiple clusters.
Set up available clusters when creating workspace  Log in to an account that has permission to create a workspace, such as ws-manager.</description>
    </item>
    
    <item>
      <title>Release Notes For 2.1.0</title>
      <link>https://cnimages.github.io/website/zh/docs/release/release-v210/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/release/release-v210/</guid>
      <description>KubeSphere 2.1.0 was released on Nov 11th, 2019, which fixes known bugs, adds some new features and brings some enhancement. If you have installed versions of 2.0.x, please upgrade it and enjoy the better user experience of v2.1.0.
Installer Enhancement  Decouple some components and make components including DevOps, service mesh, app store, logging, alerting and notification optional and pluggable Add Grafana (v5.2.4) as the optional component Upgrade Kubernetes to 1.</description>
    </item>
    
    <item>
      <title>Role and Member Management</title>
      <link>https://cnimages.github.io/website/zh/docs/workspace-administration/role-and-member-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/workspace-administration/role-and-member-management/</guid>
      <description>This guide demonstrates how to manage roles and members in your workspace. For more information about KubeSphere roles, see Overview of Role Management.
In workspace scope, you can grant the following resources&amp;rsquo; permissions to a role:
 Projects DevOps Access Control Apps Management Workspace Settings  Prerequisites At least one workspace has been created, such as demo-workspace. Besides, you need an account of the workspace-admin role (e.g. ws-admin) at the workspace level.</description>
    </item>
    
    <item>
      <title>节点管理</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/nodes/</guid>
      <description>Kubernetes通过将容器放入容器组中并在节点上运行来运行工作负载。节点可以是虚拟机，也可以是物理机，这取决于集群环境。每个节点都包含运行容器组所需的服务，这些服务由控制平面管理。有关节点的更多信息，请参见Kubernetes的官方文档。
本教程演示了集群管理员对节点可以查看和执行的操作。
前提条件 您需要一个被授予集群管理角色的帐户。 例如，您可以直接以admin身份登录控制台或使用授权创建新角色并将其分配给帐户。
节点状态 只有集群管理员可以访问集群节点。有些节点指标对集群非常重要。因此，管理员有责任监控这些指标并确保节点的可用性。请按照以下步骤查看节点状态。
  单击左上角的平台管理，然后选择集群管理。   如果您已经在导入成员集群时启用了多集群特性，那么您可以选择一个特定集群以查看其应用程序资源。 如果尚未启用该特性，请直接参考下一步。   选择节点管理下的集群节点，您可以在其中查看节点状态的详细信息。  名称：节点名称和子网IP地址。 状态：节点的当前状态，标识节点是否可用。 角色：节点的角色，标识节点是工作节点还是主节点。 CPU：节点的实时CPU使用率。 内存：节点的实时内存使用率。 容器组：节点上容器组的实时使用率。 已分配CPU：该指标是根据节点上容器组总CPU请求计算得出的。即使工作负载使用较少的CPU资源，它也表示该节点上为工作负载预留的CPU量。这个指标对于Kubernetes调度器(kube-scheduler)非常重要，在大多数情况下，它在调度容器组时支持分配较少CPU资源的节点。有关更多细节，请参阅管理容器的资源。 已分配内存：该指标是根据节点上容器组的总内存请求计算得出的。即使工作负载使用较少的内存资源，它也表示为该节点上的工作负载预留的内存量。  备注
CPU和已分配CPU在大多数情况下是不同的，内存和已分配内存也是不同的，这是正常的。作为集群管理员，您需要专注于两个指标，而不仅仅是一个。最佳实践是为每个节点设置资源请求和限制以匹配其实际使用情况。过度分配资源可能导致集群资源利用率低，而分配不足可能导致集群压力大，使集群不健康。   节点管理 从列表中单击一个节点，然后可以转到其详细信息页面。   停止调度/启用调度：在节点重新引导或其他维护期间，将节点标记为不可调度非常有用。如果标记为不可调度，则Kubernetes调度程序不会将新容器组调度到该节点。 此外，这不会影响节点上已经存在的现有工作负载。在KubeSphere中，通过单击节点详细信息页面上的停止调度将节点标记为不可调度。如果再次单击此按钮（启用调度），则该节点将会标记为可调度。
  标签：当您想将容器组分配给特定节点时，节点标签会非常有用。首先标记节点（例如，使用node-role.kubernetes.io/gpu-node标记GPU节点），然后在创建工作负载时在高级设置中添加此标签，以便可以明确容器组在有GPU标记的节点上运行。要添加节点标签，请单击更多操作，然后选择编辑标签。   污点：污染允许节点排斥一系列的容器组。 您可以在节点详细信息页面上添加或删除节点污点。 要添加或删除污点，请单击更多操作，然后从下拉菜单中选择污点管理。
备注
添加污点时请小心，因为它们可能会导致意外行为，从而导致服务不可用。有关更多信息，请参见污点和容忍.   添加和删除节点 在当前版本，您不能直接通过KubeSphere控制台添加或删除节点，但是可以使用KubeKey来完成。 有关更多信息，请参见添加新节点和删除节点。</description>
    </item>
    
    <item>
      <title>Glossary</title>
      <link>https://cnimages.github.io/website/zh/docs/reference/glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/reference/glossary/</guid>
      <description>This glossary includes technical terms that are specific to KubeSphere, as well as more general terms that provide useful context.
General   Workspace A logical unit to organize a tenant&amp;rsquo;s workload projects / Kubernetes namespaces, DevOps projects, manage resource access and share information within the team.
  System Workspace The special place to organize system projects from KubeSphere, Kubernetes and optional components such as OpenPitrix, Istio, monitorng etc.</description>
    </item>
    
    <item>
      <title>KubeSphere API</title>
      <link>https://cnimages.github.io/website/zh/docs/reference/api-docs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/reference/api-docs/</guid>
      <description>In KubeSphere v3.0, we move the functionalities of ks-apigateway, ks-account into ks-apiserver to make the architecture more compact and straight forward. In order to use KubeSphere API, you need to expose ks-apiserver to your client.
Expose KubeSphere API service If you are going to access KubeSphere inside the cluster, you can skip the following section and just using the KubeSphere API server endpoint http://ks-apiserver.kubesphere-system.svc.
But if not, you need to expose the KubeSphere API server endpoint to the outside of the cluster first.</description>
    </item>
    
    <item>
      <title>Logging</title>
      <link>https://cnimages.github.io/website/zh/docs/reference/api-changes/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/reference/api-changes/logging/</guid>
      <description>Time format The time format for query parameters must be in Unix timestamp, which is the number of seconds that have elapsed since the Unix epoch. Millisecond is no longer allowed. The change affects the parameters start_time and end_time.
Deprecated APIs The following APIs are removed:
 GET /workspaces/{workspace} GET /namespaces/{namespace} GET /namespaces/{namespace}/workloads/{workload} GET /namespaces/{namespace}/pods/{pod} The whole log setting API group  Fluent Bit Operator In KubeSphere 3.0.0, the whole log setting APIs are removed from the KubeSphere core since the project Fluent Bit Operator is refactored in an incompatible way.</description>
    </item>
    
    <item>
      <title>Monitoring</title>
      <link>https://cnimages.github.io/website/zh/docs/reference/api-changes/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/reference/api-changes/monitoring/</guid>
      <description>API Version The monitoring API version is bumped to v1alpha3.
Time format The time format for query parameters must be in Unix timestamp, which is the number of seconds that have elapsed since the Unix epoch. Decimal is no longer allowed. The change affects the parameters start, end and time.
Deprecated Metrics In KubeSphere 3.0.0, the metrics on the left have been renamed into the ones on the right.
   V2.</description>
    </item>
    
    <item>
      <title>Release Notes For 2.0.2</title>
      <link>https://cnimages.github.io/website/zh/docs/release/release-v202/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/release/release-v202/</guid>
      <description>KubeSphere 2.0.2 was released on July 9, 2019, which fixes known bugs and enhances existing feature. If you have installed versions of 1.0.x, 2.0.0 or 2.0.1, please download KubeSphere installer v2.0.2 to upgrade.
What&amp;rsquo;s New in 2.0.2 Enhanced Features  API docs are available on the official website. Block brute-force attacks. Standardize the maximum length of resource names. Upgrade the gateway of project (Ingress Controller) to the version of 0.24.1. Support Ingress grayscale release.</description>
    </item>
    
    <item>
      <title>集群状态监控</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-status-monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-status-monitoring/</guid>
      <description>KubeSphere 提供了对集群的 CPU、内存、网络和磁盘等相关指标的监控。在集群状态监控页面中，您还可以查看历史监控数据并根据节点的使用情况按不同的指标对节点进行排序。
前提条件 您需要一个被授予集群管理角色的帐户。 例如，您可以直接以admin身份登录控制台或使用授权创建新角色并将其分配给帐户。
集群状态监控   单击左上角的平台管理，然后选择集群管理。   如果您已经在导入成员集群时启用了多集群特性，那么您可以选择一个特定集群以查看其应用程序资源。 如果尚未启用该特性，请直接参考下一步。   在监控告警下拉选项里选择“集群状态”以查看集群状态监控的概览，包括集群节点状态、组件状态、集群资源使用情况、ETCD监控和服务组件监控，如下图所示：   集群节点状态   集群节点状态显示在线结点/所有结点状态。 您可以通过单击节点在线状态跳转到如下所示的集群节点页以查看所有节点的实时资源使用情况。   在集群节点中，单击节点名称可以查看结点运行状态中的使用详细信息，包括当前节点中的CPU、内存、容器组、本地存储的信息及其健康状态。   单击监控选项卡，可以根据不同的指标查看节点在特定时期内的运行情况，这些指标包括CPU使用率、CPU平均负载、内存使用率、磁盘使用率、inode使用率、IOPS、磁盘吞吐和网络带宽，如下图所示：   提示
您可以从右上角的下拉列表中自定义时间范围查看历史数据。 组件状态 KubeSphere监控集群中各种服务组件的运行健康状况。 当关键组件发生故障时，系统可能变的不可用。 KubeSphere的监控机制确保该平台可以在组件出现故障时将任何发生的问题通知租户，以便他们可以快速定位问题并采取相应的措施。
  在集群状态监控&amp;rsquo;页面上，单击组件状态里的组件（下面绿色框中的部分）以查看服务组件的状态。   您可以看到所有组件都列在这一页面中。标记为绿色的组件是正常运行的组件，而标记为橙色的组件则需要特别注意，因为它表示此组件存在潜在问题。   提示
标记为橙色的组件可能会在一段时间后变为绿色，原因可能会有所不同，例如重试拉取镜像或重新创建实例。 您可以单击该组件查看其服务详细信息。 群集资源使用情况 集群资源使用情况显示的信息包括集群中所有节点的CPU使用率、内存使用率、磁盘使用率和容器组数量变化。 单击左侧的饼图以切换指标，在右侧的曲线图中显示一段时间内的趋势。 物理资源监控 物理资源监控中的监控数据可以帮助用户更好地观察自己的物理资源，并据此建立正常的资源和集群性能使用阀值。KubeSphere允许用户查看最近7天的集群监控数据，包括CPU使用情况、内存使用情况、CPU平均负载(1分钟/5分钟/15分钟)、inode使用率、磁盘吞吐量(读写)、IOPS(读写)、网络带宽和容器组运行状态。您可以在KubeSphere中查看自定义时间范围和时间间隔内的物理资源历史监控数据。 以下各节简要介绍每个监控指标。 CPU利用率 CPU利用率显示一段时间内CPU资源的使用率。 如果您注意到某一段时间平台的CPU使用率飙升，您必须首先定位占用CPU资源最多的进程。 例如，对于Java应用程序，代码中出现内存泄漏或无限循环的情况可能会出现CPU使用率飙高。 内存利用率 内存是服务器上的重要部件之一，是与CPU通信的桥梁。 因此，内存的性能对机器有很大的影响。 当程序运行时，数据加载、线程并发和I/O缓冲都依赖于内存。 可用内存的大小决定了程序是否可以正常运行以及它是如何运行的。 内存利用率反映了集群内存资源的整体使用情况，显示为给定时刻使用的可用内存的百分比。
CPU平均负载 CPU平均负载是单位时间内系统中处于可运行状态和不中断状态的平均进程数。 也就是说，它是活动进程的平均数量。 请注意，CPU平均负载和CPU利用率之间没有直接关系。 理想情况下，平均负载应该等于CPU的数量。 因此，在查看平均负载时，需要考虑CPU的数量。 只有当平均负载大于cpu数量时，系统才会超载。 KubeSphere为用户提供了三个不同的时间段来查看平均负载：1分钟，5分钟和15分钟。 通常，建议您查看所有这些参数，以全面了解平均负载：</description>
    </item>
    
    <item>
      <title>Release Notes For 2.0.1</title>
      <link>https://cnimages.github.io/website/zh/docs/release/release-v201/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/release/release-v201/</guid>
      <description>KubeSphere 2.0.1 was released on June 9th, 2019.
Bug Fix  Fix the issue that CI/CD pipeline cannot recognize correct special characters in the code branch. Fix CI/CD pipeline&amp;rsquo;s issue of being unable to check logs. Fix no-log data output problem caused by index document fragmentation abnormity during the log query. Fix prompt exceptions when searching for logs that do not exist. Fix the line-overlap problem on traffic governance topology and fixed invalid image strategy application.</description>
    </item>
    
    <item>
      <title>应用资源监控</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/application-resources-monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/application-resources-monitoring/</guid>
      <description>除了在物理资源级别监控数据外，集群管理员还需要密切跟踪整个平台上的应用资源，例如DevOps项目的数量，以及特定类型的工作负载和服务的数量。应用资源监控提供了平台的资源使用情况和应用程序级别趋势的摘要。
前提条件 您需要一个被授予集群管理角色的帐户。 例如，您可以直接以admin身份登录控制台或使用授权创建新角色并将其分配给帐户。
资源使用情况   单击左上角的平台管理，然后选择集群管理。   如果您已经在导入成员集群时启用了多集群特性，那么您可以选择一个特定集群以查看其应用程序资源。 如果尚未启用该特性，请直接参考下一步。   在监控告警下拉选项里选择应用资源以查看应用程序资源监控的概览，包括群集中所有资源使用情况的摘要，如下图所示。   其中，群集资源使用情况和应用资源使用情况保留最近7天的监控数据，而且支持自定义时间范围查询。   单击特定资源以查看特定时间段内的详细使用情况和趋势，例如集群资源使用情况下的CPU。详细信息页面允许您按项目查看特定的监控数据。 高度交互的仪表板使用户可以自定义时间范围，显示给定时间点的确切资源使用情况。   用量排行 用量排行支持对项目资源使用情况进行排序，因此平台管理员可以了解当前集群中每个项目的资源使用情况，包括CPU使用量，内存使用量，容器组数量以及网络流出速率和网络流入速率。您可以通过下拉列表中的任一指标按升序或降序对项目进行排序。 此功能对于快速查找消耗大量CPU或内存的应用程序（容器组）非常有用。 </description>
    </item>
    
    <item>
      <title>Release Notes For 2.0.0</title>
      <link>https://cnimages.github.io/website/zh/docs/release/release-v200/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/release/release-v200/</guid>
      <description>KubeSphere 2.0.0 was released on May 18th, 2019.
What&amp;rsquo;s New in 2.0.0 Component Upgrades  Support Kubernetes Kubernetes 1.13.5 Integrate QingCloud Cloud Controller. After installing load balancer, QingCloud load balancer can be created through KubeSphere console and the backend workload is bound automatically.  Integrate QingStor CSI v0.3.0 storage plugin and support physical NeonSAN storage system. Support SAN storage service with high availability and high performance. Integrate QingCloud CSI v0.2.1 storage plugin and support many types of volume to create QingCloud block services.</description>
    </item>
    
    <item>
      <title>什么是 KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/introduction/what-is-kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/introduction/what-is-kubesphere/</guid>
      <description>概述 KubeSphere 是在 Kubernetes 之上构建的面向云原生应用的分布式操作系统，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。它的架构可以非常方便地使第三方应用与云原生生态组件进行即插即用 (plug-and-play) 的集成。
作为全栈化容器部署与多租户管理平台，KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。它拥有 Kubernetes 企业级服务所需的最常见功能，例如 Kubernetes 资源管理、DevOps、多集群部署与管理、应用生命周期管理、微服务治理、日志查询与收集、服务与网络、多租户管理、监控告警、事件审计、存储、访问控制、GPU 支持、网络策略、镜像仓库管理以及安全管理等。
KubeSphere 围绕 Kubernetes 集成了各种生态系统的工具，提供了一致的用户体验以降低复杂性。同时，它还具备 Kubernetes 尚未提供的新功能，旨在解决 Kubernetes 本身存在的存储、网络、安全和易用性等痛点。KubeSphere 不仅允许开发人员和 DevOps 团队在统一的控制台中使用他们喜欢的工具，而且最重要的是，这些功能与平台松散耦合，因为他们可以选择是否安装这些可拔插组件。
支持在任意平台运行 KubeSphere 作为一个轻量级平台，KubeSphere 对不同云生态系统的支持变得更加友好，因为它没有对 Kubernetes 本身有任何的 Hack。换句话说，KubeSphere 可以部署并运行在任何基础架构以及所有兼容现有版本的 Kubernetes 集群上，包括虚拟机、裸机、本地环境、公有云和混合云等。KubeSphere 用户可以选择在云和容器平台（例如阿里云、AWS、青云QingCloud、腾讯云、华为云和 Rancher 等）上安装 KubeSphere，甚至可以导入和管理使用 Kubernetes 发行版创建的现有 Kubernetes 集群。KubeSphere 可以在不修改用户当前的资源或资产、不影响其业务的情况下与现有 Kubernetes 平台无缝集成。有关更多信息，请参见在 Linux 上安装和在 Kubernetes 上安装。
KubeSphere 为用户屏蔽了基础设施底层复杂的技术细节，帮助企业在各类基础设施之上无缝地部署、更新、迁移和管理现有的容器化应用。通过这种方式，KubeSphere 使开发人员能够专注于应用程序开发，使运维团队能够通过企业级可观察性功能和故障排除机制、统一监控和日志记录、集中式存储和网络管理，以及易用的 CI/CD 流水线来加快 DevOps 自动化工作流程和交付流程等。
3.0 新增功能   多集群管理：随着我们迎来混合云时代，多集群管理已成为我们时代的主题。作为 Kubernetes 上最必要的功能之一，多集群管理可以满足用户的迫切需求。在最新版本 3.0 中，我们为 KubeSphere 配备了多集群功能，该功能可以为部署在不同云中的集群提供一个中央控制面板。用户可以导入和管理在主流基础设施提供商（例如 Amazon EKS 和 Google GKE 等）平台上创建的现有 Kubernetes 集群。通过简化操作和维护流程，这将大大降低用户们的学习成本。Solo 和 Federation 是多集群管理的两个特有模式，使 KubeSphere 在同类产品中脱颖而出。</description>
    </item>
    
    <item>
      <title>平台功能</title>
      <link>https://cnimages.github.io/website/zh/docs/introduction/features/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/introduction/features/</guid>
      <description>概览 KubeSphere 作为开源的企业级全栈化容器平台，为用户提供了一个健壮、安全、功能丰富、具备极致体验的 Web 控制台。拥有企业级 Kubernetes 所需的最常见的功能，如工作负载管理，网络策略配置，微服务治理（基于 Istio），DevOps 工程 (CI/CD) ，安全管理，Source to Image/Binary to Image，多租户管理，多维度监控，日志查询和收集，告警通知，审计，应用程序管理和镜像管理、应用配置密钥管理等功能模块。
它还支持各种开源存储和网络解决方案以及云存储服务。例如，KubeSphere 为用户提供了功能强大的云原生工具负载均衡器插件 Porter，这是为 Kubernetes 集群开发的 CNCF 认证的负载均衡插件。
有了易于使用的图形化 Web 控制台，KubeSphere 简化了用户的学习曲线并推动了更多的企业使用 Kubernetes 。
以下从专业的角度详解各个模块的功能服务。有关详细信息，请参阅本指南中的相应章节。
部署和维护 Kubernetes 部署 Kubernetes 集群 KubeKey 允许用户直接在基础架构上部署 Kubernetes，为 Kubernetes 群集提供高可用性。建议在生产环境至少配置三个主节点。
Kubernetes 资源管理 对底层 Kubernetes 中的多种类型的资源提供可视化的展示与监控数据，以向导式 UI 实现工作负载管理、镜像管理、服务与应用路由管理 (服务发现)、密钥配置管理等，并提供弹性伸缩 (HPA) 和容器健康检查支持，支持数万规模的容器资源调度，保证业务在高峰并发情况下的高可用性。
由于 KubeSphere 3.0 具有增强的可观察性，用户可以从多租户角度跟踪资源，例如自定义监视、事件、审核日志、告警通知。
集群升级和扩展 KubeKey 提供了一种简单的安装，管理和维护方式。它支持 Kubernetes 集群的滚动升级，以便集群服务在升级时始终可用。另外，也可以使用 KubeKey 将新节点添加到 Kubernetes 集群中以使用更多工作负载。
多集群管理和部署 随着IT界越来越多的企业使用云原生应用程序来重塑软件产品组合，用户更倾向于跨位置、地理位置和云部署集群。在此背景下，KubeSphere 进行了重大升级，以其全新的多集群功能满足用户的迫切需求。
借助 KubeSphere的图形化 Web 控制台，用户可以管理底层的基础架构，例如添加或删除集群。可以使用相同的方式管理部署在任何基础架构（例如 Amazon EKS和Google Kubernetes Engine）上的异构集群。</description>
    </item>
    
    <item>
      <title>架构说明</title>
      <link>https://cnimages.github.io/website/zh/docs/introduction/architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/introduction/architecture/</guid>
      <description>前后端分离 KubeSphere 将 前端 与 后端 分开，实现了面向云原生的设计，后端的各个功能组件可通过 REST API 对接外部系统。 可参考 API文档。下图是系统架构图。 KubeSphere 无底层的基础设施依赖，可以运行在任何 Kubernetes、私有云、公有云、VM 或物理环境（BM）之上。 此外，它可以部署在任何 Kubernetes 发行版上。
组件列表    后端组件 功能说明     ks-apiserver 整个集群管理的 API 接口和集群内部各个模块之间通信的枢纽，以及集群安全控制。   ks-console 提供 KubeSphere 的控制台服务。   ks-controller-manager 实现业务逻辑的，例如创建企业空间时，为其创建对应的权限；或创建服务策略时，生成对应的 Istio 配置等。   metrics-server Kubernetes 的监控组件，从每个节点的 Kubelet 采集指标信息。   Prometheus 提供群集，节点，工作负载，API对象的监视指标和服务。   Elasticsearch 提供集群的日志索引、查询、数据管理等服务，在安装时也可对接您已有的 ES 减少资源消耗。   Fluent Bit 提供日志接收与转发，可将采集到的⽇志信息发送到 ElasticSearch、Kafka。   Jenkins 提供 CI/CD 流水线服务。   SonarQube 可选安装项，提供代码静态检查与质量分析。   Source-to-Image 将源代码自动将编译并打包成 Docker 镜像，方便快速构建镜像。   Istio 提供微服务治理与流量管控，如灰度发布、金丝雀发布、熔断、流量镜像等。   Jaeger 收集 Sidecar 数据，提供分布式 Tracing 服务。   OpenPitrix 提供应用程序生命周期管理，例如应用模板、应用部署与管理的服务等。   Alert 提供集群、Workload、Pod、容器级别的自定义告警服务。   Notification 是一项综合通知服务； 它当前支持邮件传递方法。   Redis 将 ks-console 与 ks-account 的数据存储在内存中的存储系统。   MySQL 集群后端组件的数据库，监控、告警、DevOps、OpenPitrix 共用 MySQL 服务。   PostgreSQL SonarQube 和 Harbor 的后端数据库。   OpenLDAP 负责集中存储和管理用户账号信息与对接外部的 LDAP。   Storage 内置 CSI 插件对接云平台存储服务，可选安装开源的 NFS/Ceph/Gluster 的客户端。   Network 可选安装 Calico/Flannel 等开源的网络插件，支持对接云平台 SDN。    服务组件 以上列表中每个功能组件下还有多个服务组件，关于服务组件的说明，可参考 服务组件说明。</description>
    </item>
    
    <item>
      <title>Advantages</title>
      <link>https://cnimages.github.io/website/zh/docs/introduction/advantages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/introduction/advantages/</guid>
      <description>Vision Kubernetes has become the de facto standard for deploying containerized applications at scale in private, public and hybrid cloud environments. However, many people can easily get confused when they start to use Kubernetes as it is complicated and has many additional components to manage. Some components need to be installed and deployed by users themselves, such as storage and network services. At present, Kubernetes only provides open-source solutions or projects, which can be difficult to install, maintain and operate to some extent.</description>
    </item>
    
    <item>
      <title>Use Cases</title>
      <link>https://cnimages.github.io/website/zh/docs/introduction/scenarios/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/introduction/scenarios/</guid>
      <description>KubeSphere is applicable in a variety of scenarios. For enterprises that deploy their business system on bare metal, their business modules are tightly coupled with each other. That means it is extremely difficult for resources to be horizontally scaled. In this connection, KubeSphere provides enterprises with containerized environments with a complete set of features for management and operation. It empowers enterprises to rise to the challenges in the middle of their digital transformation, including agile software development, automated operation and maintenance, microservices governance, traffic management, autoscaling, high availability, as well as DevOps and CI/CD.</description>
    </item>
    
    <item>
      <title>名词解释</title>
      <link>https://cnimages.github.io/website/zh/docs/introduction/glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/introduction/glossary/</guid>
      <description>本文档介绍了KubeSphere中一些常用的词汇表，如下所示：
   KubeSphere Kubernetes 对照释义     项目 Namespace， 为 Kubernetes 集群提供虚拟的隔离作用，详见 Namespace。   容器组 Pod，是 Kubernetes 进行资源调度的最小单位，每个 Pod 中运行着一个或多个密切相关的业务容器，详见 Pods。   部署 Deployments，表示用户对 Kubernetes 集群的一次更新操作，详见 Deployment。   有状态副本集 StatefulSet，用于管理有状态的应用程序（例如MySQL），可以保证部署和 scale 的顺序，详见 StatefulSet。   守护进程集 DaemonSet 保证在每个 Node 上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用，例如 fluentd 或 logstash，详见 DaemonSet。   任务 Jobs，在 Kubernetes 中用来控制批处理型任务的资源对象，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。任务管理的 Pod 根据用户的设置将任务成功完成就自动退出了。比如在创建工作负载前，执行任务，将镜像上传至镜像仓库。详见 Job。   定时任务 CronJob，是基于时间的 Job，就类似于 Linux 系统的 crontab，在指定的时间周期运行指定的 Job，在给定时间点只运行一次或周期性地运行。详见 CronJob。   服务 Service，一个 Kubernete 服务是一个最小的对象，类似 Pod，和其它的终端对象一样 详见 Service。   应用路由 Ingress，是授权入站连接到达集群服务的规则集合。可通过 Ingress 配置提供外部可访问的 URL、负载均衡、SSL、基于名称的虚拟主机等，详见 Ingress。   镜像仓库 Image Registries，镜像仓库用于存放 Docker 镜像，包括公共镜像仓库（如 DockerHub）和私有镜像仓库（如 Harbor），详见 Image。   存储卷 PersistentVolumeClaim（PVC），满足用户对于持久化存储的需求，用户将 Pod 内需要持久化的数据挂载至存储卷，删除 Pod 后，数据仍保留在存储卷内。Kubesphere 推荐使用动态分配存储，当集群管理员配置存储类型后，集群用户可一键式分配和回收存储卷，无需关心存储底层细节。详见 PVC。   存储类型 StorageClass，为管理员提供了描述存储 “Class（类）” 的方法，包含 Provisioner、 ReclaimPolicy 和 Parameters 。详见 StorageClass。   流水线 Jenkins Pipeline，简单来说就是一套运行在 Jenkins 上的 CI/CD 工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。详见 Pipeline。   企业空间 Workspace，是 KubeSphere 实现多租户模式的基础，是您管理项目、 DevOps 工程和企业成员的基本单位。   主机 Node，Kubernetes 集群中的计算能力由 Node 提供，Kubernetes 集群中的 Node 是所有 Pod 运行所在的工作主机，可以是物理机也可以是虚拟机。详见 Node。    </description>
    </item>
    
    <item>
      <title>Manage alerts with Alertmanager in KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/alertmanager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/alertmanager/</guid>
      <description>Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integration such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. For more details, please refer to Alertmanager guide.
KubeSphere has been using Prometheus as its monitoring service&amp;rsquo;s backend from the first release. Starting from v3.0, KubeSphere adds Alertmanager to its monitoring stack to manage alerts sent from Prometheus as well as other components such as kube-events and kube-auditing.</description>
    </item>
    
    <item>
      <title>Manage multi-tenant notifications with Notification Manager</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/notification-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/notification-manager/</guid>
      <description>Notification Manager manages notifications in KubeSphere. It receives alerts or notifications from different senders and then sends notifications to different users.
Supported senders includes:
 Prometheus Alertmanager Custom sender (Coming soon)  Supported receivers includes:
 Email Wechat Work Slack Webhook (Coming soon)  QuickStart Config Prometheus Alertmanager to send alerts to Notification Manager Notification Manager uses port 19093 and API path /api/v2/alerts to receive alerts sent from Prometheus Alertmanager of Kubesphere.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/introduction/</guid>
      <description>KubeSphere provides a flexible log collection configuration method. Powered by FluentBit Operator, users can add/modify/delete/enable/disable Elasticsearch, Kafka and Fluentd receivers with ease. Once a receiver is added, logs will be sent to this receiver.
Prerequisite Before adding a log receiver, you need to enable any of the logging, events or auditing components following Enable Pluggable Components.
Add Log Receiver (aka Collector) for container logs To add a log receiver:
 Login with an account of platform-admin role Click Platform -&amp;gt; Clusters Management Select a cluster if multiple clusters exist Click Cluster Settings -&amp;gt; Log Collections Log receivers can be added by clicking Add Log Collector  备注</description>
    </item>
    
    <item>
      <title>介绍</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/introduction/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/introduction/overview/</guid>
      <description>作为KubeSphere为用户提供即插即用架构承诺的一部分，您可以轻松地将KubeSphere安装在现有的Kubernetes集群上。更具体地说，KubeSphere可以部署在托管在云上（例如AWS EKS，QingCloud QKE和Google GKE）或本地上的Kubernetes中。KubeSphere不会对已有Kubernetes集群进行任何侵入性修改，它仅与Kubernetes API交互以管理Kubernetes集群资源。换句话说，KubeSphere可以安装在任何本地Kubernetes集群或Kubernetes发行版上。
本节描述了在Kubernetes上安装KubeSphere的一般步骤。有关在不同环境中的特定安装方式的更多信息，请参阅在托管Kubernetes上安装和在本地Kubernetes上安装。
备注
在现有Kubernetes群集上安装KubeSphere之前，请阅读先决条件。 部署 KubeSphere 确保现有的Kubernetes群集满足所有要求之后，可以使用kubectl执行KubeSphere的默认最小安装。
 执行以下命令以开始安装：  kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/kubesphere-installer.yaml kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/cluster-configuration.yaml 备注
如果您的服务器无法访问GitHub，则可以分别复制kubesphere-installer.yaml和cluster-configuration.yaml中的内容并将其粘贴到本地文件中。然后，您可以使用kubectl apply -f 执行本地文件来安装KubeSphere。  检查安装日志：  kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&amp;#39;{.items[0].metadata.name}&amp;#39;) -f  使用kubectl get pod --all-namespaces，查看所有pod在KubeSphere相关的命名空间运行是否正常。如果是，请通过以下命令检查控制台的端口（默认为30880）：  kubectl get svc/ks-console -n kubesphere-system  确保在安全组中打开了30880端口，并使用（IP:30880）以及默认帐户和密码（admin/P@88w0rd）通过NodePort访问Web控制台。  启用可插拔组件（可选） 如果从默认的最小安装开始，请参阅启用可插拔组件以安装其他组件。
提示
 您可以在执行KuberSphere安装之前或之后启用可插拔组件。请参阅示例文件cluster-configuration.yaml以获取更多详细信息。 请确保集群中的节点有足够的CPU和内存。 强烈建议您安装这些可插拔组件，以体验KubeSphere提供的全栈功能。   </description>
    </item>
    
    <item>
      <title>ConfigMaps</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/configuration/configmaps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/configuration/configmaps/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://cnimages.github.io/website/zh/docs/devops-user-guide/introduction/credential/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/devops-user-guide/introduction/credential/</guid>
      <description>KubeSphere is an enterprise-grade multi-tenant container platform built on Kubernetes. It provides an easy-to-use UI for users to manage application workloads and computing resources with a few clicks, which greatly reduces the learning curve and the complexity of daily work such as development, testing, operation and maintenance. KubeSphere aims to alleviate the pain points of Kubernetes including storage, network, security and ease of use, etc.
KubeSphere supports installing on cloud-hosted and on-premises Kubernetes cluster, e.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://cnimages.github.io/website/zh/docs/devops-user-guide/introduction/pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/devops-user-guide/introduction/pipeline/</guid>
      <description>KubeSphere is an enterprise-grade multi-tenant container platform built on Kubernetes. It provides an easy-to-use UI for users to manage application workloads and computing resources with a few clicks, which greatly reduces the learning curve and the complexity of daily work such as development, testing, operation and maintenance. KubeSphere aims to alleviate the pain points of Kubernetes including storage, network, security and ease of use, etc.
KubeSphere supports installing on cloud-hosted and on-premises Kubernetes cluster, e.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/overview/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Project Quotas</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-quota/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-quota/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Volumes</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/storage/volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/storage/volumes/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>概述</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/intro/</guid>
      <description>对于在 Linux 上的安装，KubeSphere 既可以安装在云中也可以安装在本地环境中，例如 AWS EC2，Azure VM 和裸机。 用户可以在配置新的 Kubernetes 集群时在 Linux 主机上安装 KubeSphere。 安装过程简单而友好。 同时，KubeSphere不仅提供在线安装程序或 KubeKey ，而且还为无法访问 Internet 的环境提供了离线的安装解决方案。
作为 GitHub 上的开源项目， KubeSphere 是一个有成千上万的社区用户的聚集地。 他们中的许多人把KubeSphere 运行在生产环境中。
为用户提供了多个安装选项。 请注意，并非所有选项都是互斥的。 例如，您可以在离线环境中的多个节点上以最小化部署 KubeSphere。
 All-in-One: 在单个节点上安装 KubeSphere 。 仅用于用户快速熟悉 KubeSphere。 Multi-Node: 在多个节点上安装 KubeSphere 。 用于测试或开发。 Install KubeSphere on Air-gapped Linux: 把 KubeSphere 的所有镜像打包，方便再 Linux 上离线安装。 High Availability Installation: 在用于生产环境的多个节点上安装高可用性 KubeSphere。 Minimal Packages: 仅安装 KubeSphere 所需的最少系统组件。 以下是最低资源要求:  2vCPUs 4GB RAM 40GB Storage   Full Packages: 安装 KubeSphere 的所有可用系统组件，例如 DevOps，ServiceMesh 和告警。  有关在 Kubernetes 上进行安装，请参阅在 Kubernetes 上进行安装概述。</description>
    </item>
    
    <item>
      <title>多节点安装</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/multioverview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/multioverview/</guid>
      <description>All-in-one 是为新用户体验 KubeSphere 而提供的快速且简单的安装方式，在正式环境中，单节点群集因受限于资源和计算能力的不足而无法满足大多数需求，因此不建议将单节点群集用于大规模数据处理。多节点安装环境通常包括至少一个主节点和多个工作节点，如果是生产环境则需要安装主节点高可用的方式。
本节概述了多节点安装，包括概念，KubeKey 及安装步骤。有关主节点高可用安装的信息，请参阅在公有云上安装或在本地环境中安装，如在阿里云 ECS 安装高可用 KubeSphere 或 在 VMware vSphere 部署高可用 KubeSphere。
概念 多节点群集由至少一个主节点和一个工作节点组成，可以使用任何节点作为“任务箱”来执行安装任务。您可以在安装之前或之后根据需要添加其他节点（例如，为了实现高可用性）。
 Master. 主节点，通常托管控制面，控制和管理整个系统。 Worker. 工作节点，运行在其上部署实际应用程序。  为什么选择 KubeKey 如果您不熟悉 Kubernetes 组件，可能会发现部署多节点 Kubernetes 集群并不容易。从版本 3.0.0 开始，KubeSphere 使用了一个全新的安装工具 KubeKey，替换以前基于 ansible 的安装程序，更加方便用户快速部署多节点集群。具体来说，下载 KubeKey 之后，用户只需配置很少的信息如节点信息（IP 地址和节点角色），然后一条命令即可安装。
优势  之前基于 ansible 的安装程序具有许多软件依赖性，例如 Python。KubeKey 是使用 Go 语言开发的，可以消除各种环境中的问题，并确保安装成功。 KubeKey 使用 Kubeadm 在节点上尽可能多地并行安装 Kubernetes 集群，以降低安装复杂性并提高效率。与较早的安装程序相比，它将大大节省安装时间。 借助 KubeKey 用户可以自由伸缩集群，包括将集群从单节点群集扩展到多节点群集，甚至是主节点高可用群集。 KubeKey 未来计划将集群管理封装成一个对象，即 Cluster as an Object (CaaO)。  步骤1：准备 Linux 主机 安装之前请参阅下面对硬件和操作系统的要求准备至少三台主机。
系统要求    系统 最低要求（每个节点）     Ubuntu 16.</description>
    </item>
    
    <item>
      <title>端口要求</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/port-firewall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/port-firewall/</guid>
      <description>KubeSphere 需要某些端口用于服务之间的通信。 如果您的网络配置有防火墙规则，则需要确保基础组件可以通过特定端口相互通信。
   Service Protocol Action Start Port End Port Notes     ssh TCP allow 22     etcd TCP allow 2379 2380    apiserver TCP allow 6443     calico TCP allow 9099 9100    bgp TCP allow 179     nodeport TCP allow 30000 32767    master TCP allow 10250 10258    dns TCP allow 53     dns UDP allow 53     local-registry TCP allow 5000  For offline environment   local-apt TCP allow 5080  For offline environment   rpcbind TCP allow 111  Required if NFS is used   ipip IPENCAP / IPIP allow   Calico needs to allow the ipip protocol    备注</description>
    </item>
    
    <item>
      <title>Prerequisites</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/introduction/prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/introduction/prerequisites/</guid>
      <description>Not only can KubeSphere be installed on virtual machines and bare metal with provisioned Kubernetes, but also supports installing on cloud-hosted and on-premises existing Kubernetes clusters as long as your Kubernetes cluster meets the prerequisites below.
 Kubernetes version: 1.15.x, 1.16.x, 1.17.x, 1.18.x; CPU &amp;gt; 1 Core; Memory &amp;gt; 2 G; A default Storage Class in your Kubernetes cluster is configured; use kubectl get sc to verify it. The CSR signing feature is activated in kube-apiserver when it is started with the --cluster-signing-cert-file and --cluster-signing-key-file parameters.</description>
    </item>
    
    <item>
      <title>Blue-green Deployment</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/blue-green-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/blue-green-deployment/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Canary Release</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/canary-release/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/canary-release/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Image Registry</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/configuration/image-registry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/configuration/image-registry/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Kubernetes集群配置</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/vars/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/vars/</guid>
      <description>本教程介绍了当您开始使用 KubeKey 来配置集群时，如何在config-example.yaml 中自定义 Kubernetes 集群配置。 您可以参考以下部分以了解每个参数。
######################### Kubernetes ######################### kubernetes: version: v1.17.9 # The default k8s version is v1.17.9, you can specify 1.15.2, v1.16.13, v1.18.6 as you want imageRepo: kubesphere # DockerHub Repo clusterName: cluster.local # Kubernetes Cluster Name masqueradeAll: false # masqueradeAll tells kube-proxy to SNAT everything if using the pure iptables proxy mode. [Default: false] maxPods: 110 # maxPods is the number of pods that can run on this Kubelet.</description>
    </item>
    
    <item>
      <title>Project Gateway</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-gateway/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-gateway/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Project Members</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-members/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-members/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Secrets</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/configuration/secrets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/configuration/secrets/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Traffic Mirroring</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/traffic-mirroring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/grayscale-release/traffic-mirroring/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Project Roles</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-roles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/project-administration/project-roles/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Volume Snapshots</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/storage/volume-snapshots/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/storage/volume-snapshots/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Workspace Network Isolation</title>
      <link>https://cnimages.github.io/website/zh/docs/workspace-administration/workspace-network-isolation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/workspace-administration/workspace-network-isolation/</guid>
      <description>Prerequisites  You have already enabled Network Policy. Please refer to network-policy it is not ready yet. Use an account of the workspace-admin role. For example, use the account ws-admin created in Create Workspace, Project, Account and Role. 备注
For the implementation of the network policy, you can refer to kubesphere-network-policy  Enable/Disable Workspace Network Isolation Workspace network isolation is disabled by default. You can turn on network isolation in Basic Info under Workspace Settings.</description>
    </item>
    
    <item>
      <title>Persistent Storage Configuration</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/storage-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/introduction/storage-configuration/</guid>
      <description>Overview Persistent volumes are a Must for installing KubeSphere. KubeKey lets KubeSphere be installed on different storage systems by the add-on mechanism. General steps of installing KubeSphere by KubeKey on Linux are:
 Install Kubernetes. Install the add-on plugin for KubeSphere. Install Kubesphere by ks-installer.  In KubeKey configurations, spec.persistence.storageClass of ClusterConfiguration needs to be set for ks-installer to create a PersistentVolumeClaim (PVC) for KubeSphere. If it is empty, the default StorageClass (annotation storageclass.</description>
    </item>
    
    <item>
      <title>Add Elasticsearch as receiver (aka Collector)</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/add-es-as-receiver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/add-es-as-receiver/</guid>
      <description>KubeSphere supports using Elasticsearch, Kafka and Fluentd as log receivers. This doc will demonstrate how to add an Elasticsearch receiver.
Prerequisite Before adding a log receiver, you need to enable any of the logging, events or auditing components following Enable Pluggable Components. The logging component is enabled as an example in this doc.
 To add a log receiver:   Login KubeSphere with an account of platform-admin role Click Platform -&amp;gt; Clusters Management Select a cluster if multiple clusters exist Click Cluster Settings -&amp;gt; Log Collections Log receivers can be added by clicking Add Log Collector  Choose Elasticsearch and fill in the Elasticsearch service address and port like below:  Elasticsearch appears in the receiver list of Log Collections page and its status becomes Collecting.</description>
    </item>
    
    <item>
      <title>Application Template</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/app-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/app-template/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>在Azure VM实例上部署KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/install-ks-on-azure-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/install-ks-on-azure-vms/</guid>
      <description>Before You Begin Technically, you can either install and manage Kubernetes yourself or adopt a managed Kubernetes solution. If you are looking for a hands-off approach to taking advantage of Kubernetes, a fully-managed platform solution may suit you best. Please see Deploy KubeSphere on AKS for more details. However, if you want a bit more control over your configuration and set up a highly-available cluster on Azure, this instruction will help you to create a production-ready Kubernetes and KubeSphere cluster.</description>
    </item>
    
    <item>
      <title>Deploy KubeSphere on QingCloud Instance</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/kubesphere-on-qingcloud-instance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/kubesphere-on-qingcloud-instance/</guid>
      <description>Introduction For a production environment, we need to consider the high availability of the cluster. If the key components (e.g. kube-apiserver, kube-scheduler, and kube-controller-manager) are all running on the same master node, Kubernetes and KubeSphere will be unavailable once the master node goes down. Therefore, we need to set up a high-availability cluster by provisioning load balancers with multiple master nodes. You can use any cloud load balancer, or any hardware load balancer (e.</description>
    </item>
    
    <item>
      <title>Deployments</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/deployments/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/deployments/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>KubeSphere 在华为云 ECS 高可用实例</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/install-ks-on-huaweicloud-ecs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/install-ks-on-huaweicloud-ecs/</guid>
      <description>由于对于生产环境，我们需要考虑集群的高可用性。教你部署如何在华为云 ECS 实例服务快速部署一套高可用的生产环境 Kubernetes 服务需要做到高可用，需要保证 kube-apiserver 的 HA ，推荐华为云负载均衡器服务.
前提条件  请遵循该指南，确保您已经知道如何将 KubeSphere 与多节点集群一起安装。有关用于安装的 config.yaml 文件的详细信息。本教程重点介绍配置华为云负载均衡器服务高可用安装。 考虑到数据的持久性，对于生产环境，我们不建议您使用存储OpenEBS，建议 NFS、GlusterFS、Ceph 等存储(需要提前准备)。文章为了进行开发和测试，集成了 OpenEBS 将 LocalPV 设置为默认的存储服务。 SSH 可以互相访问所有节点。 所有节点的时间同步。  创建主机 本示例创建 6 台 Ubuntu 18.04 server 64bit 的云服务器，每台配置为 4 核 8 GB
   主机IP 主机名称 角色     192.168.1.10 master1 master1, etcd   192.168.1.11 master2 master2, etcd   192.168.1.12 master3 master3, etcd   192.168.1.13 node1 node   192.</description>
    </item>
    
    <item>
      <title>KubeSphere 在阿里云 ECS 高可用实例</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/install-kubesphere-on-ali-ecs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/public-cloud/install-kubesphere-on-ali-ecs/</guid>
      <description>对于生产环境，我们需要考虑集群的高可用性。本文教你部署如何在多台阿里 ECS 实例快速部署一套高可用的生产环境。要满足 Kubernetes 集群服务需要做到高可用，需要保证 kube-apiserver 的 HA ，可使用以下下列两种方式：
 阿里云 SLB （推荐） keepalived + haproxy keepalived &amp;#43; haproxy对 kube-apiserver 进行负载均衡，实现高可用 kubernetes 集群。  前提条件  考虑到数据的持久性，对于生产环境，我们建议您准备持久化存储。若搭建开发和测试，您可以直接使用默认集成的 OpenEBS 准备 LocalPV； SSH 可以访问所有节点； 所有节点的时间同步； Red Hat 在其 Linux 发行版本中包括了 SELinux，建议关闭 SELinux 或者将 SELinux 的模式切换为 Permissive [宽容]工作模式。  部署架构 创建主机 本示例创建 SLB + 6 台 CentOS Linux release 7.6.1810 (Core) 的虚拟机，每台配置为 2 Core 4 GB 40 G，仅用于最小化安装，若资源充足建议使用每台配置 4 Core 8 GB 100 G 以上的虚拟机。</description>
    </item>
    
    <item>
      <title>Air-Gapped Installation</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/app-developer-guide/helm-developer-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/app-developer-guide/helm-developer-guide/</guid>
      <description>The air-gapped installation is almost the same as the online installation except it creates a local registry to host the Docker images. We will demonstrate how to install KubeSphere and Kubernetes on air-gapped environment.
 Note: The dependencies in different operating systems may cause upexpected problems. If you encounter any installation problems on air-gapped environment, please describe your OS information and error logs on GitHub.
 Prerequisites  If your machine is behind a firewall, you need to open the ports by following the document Ports Requirements for more information.</description>
    </item>
    
    <item>
      <title>Air-Gapped Installation</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/app-developer-guide/helm-specification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/app-developer-guide/helm-specification/</guid>
      <description>The air-gapped installation is almost the same as the online installation except it creates a local registry to host the Docker images. We will demonstrate how to install KubeSphere and Kubernetes on air-gapped environment.
 Note: The dependencies in different operating systems may cause upexpected problems. If you encounter any installation problems on air-gapped environment, please describe your OS information and error logs on GitHub.
 Prerequisites  If your machine is behind a firewall, you need to open the ports by following the document Ports Requirements for more information.</description>
    </item>
    
    <item>
      <title>Air-Gapped Installation</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/on-prem-kubernetes/install-ks-on-linux-airgapped/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/on-prem-kubernetes/install-ks-on-linux-airgapped/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>GitLab App</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/gitlab-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/gitlab-app/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>OAuth2 Identity Provider</title>
      <link>https://cnimages.github.io/website/zh/docs/access-control-and-account-management/oauth2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/access-control-and-account-management/oauth2/</guid>
      <description>概览 KubeSphere 可以通过标准的 OAuth2 协议对接外部的 OAuth2 Provider，通过外部 OAuth2 Server 完成账户认证后可以关联登录到 KubeSphere。 完整的认证流程如下：
GitHubIdentityProvider KubeSphere 默认提供了 GitHubIdentityProvider 做为 OAuth2 认证插件的开发示例，配置及使用方式如下:
参数配置 IdentityProvider 的参数通过 kubesphere-system 项目下 kubesphere-config 这个 ConfigMap 进行配置
通过 kubectl -n kubesphere-system edit cm kubesphere-config 进行编辑，配置示例:
apiVersion: v1 data: kubesphere.yaml: | authentication: authenticateRateLimiterMaxTries: 10 authenticateRateLimiterDuration: 10m0s loginHistoryRetentionPeriod: 7d maximumClockSkew: 10s multipleLogin: true kubectlImage: kubesphere/kubectl:v1.0.0 jwtSecret: &amp;#34;jwt secret&amp;#34; oauthOptions: accessTokenMaxAge: 1h accessTokenInactivityTimeout: 30m identityProviders: - name: github type: GitHubIdentityProvider mappingMethod: mixed provider: clientID: &amp;#39;Iv1.</description>
    </item>
    
    <item>
      <title>Role and Member Management</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/platform-settings/role-member-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/platform-settings/role-member-management/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Role and Member Management</title>
      <link>https://cnimages.github.io/website/zh/docs/devops-user-guide/devops-administration/role-and-member-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/devops-user-guide/devops-administration/role-and-member-management/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>StatefulSets</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/statefulsets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/statefulsets/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Uninstalling KubeSphere from Kubernetes</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/uninstalling/uninstalling-kubesphere-from-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/uninstalling/uninstalling-kubesphere-from-k8s/</guid>
      <description>The air-gapped installation is almost the same as the online installation except it creates a local registry to host the Docker images. We will demonstrate how to install KubeSphere and Kubernetes on air-gapped environment.
 Note: The dependencies in different operating systems may cause upexpected problems. If you encounter any installation problems on air-gapped environment, please describe your OS information and error logs on GitHub.
 Prerequisites  If your machine is behind a firewall, you need to open the ports by following the document Ports Requirements for more information.</description>
    </item>
    
    <item>
      <title>在 KubeSphere 中部署 MongoDB</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/%E5%9C%A8kubesphere%E4%B8%AD%E9%83%A8%E7%BD%B2mongodb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/%E5%9C%A8kubesphere%E4%B8%AD%E9%83%A8%E7%BD%B2mongodb/</guid>
      <description>本文介绍在 kubesphere 中通过应用商店部署 MongoDB 的操作步骤。
前提条件  已安装了 OpenPitrix（应用商店）功能组件。若未开启应用商店功能组件，可参考开启组件进行开启。 已创建了单集群企业空间、单集群项目和普通用户 project-regular 账号。 使用项目管理员 project-admin 邀请项目普通用户 project-regular 加入项目并授予 operator 角色。  操作说明   使用 project-regular 账号进入已创建的项目 demo-project  后，选择应用负载，点击应用，点击 部署新应用，然后在弹窗中选择 来自应用商店。
  进入应用商店页面。
  选择并点击 mongodb 应用，进入应用信息页面。
在应用详情&amp;ndash;配置文件中，可以查看 mongodb 应用的 helm chart 的配置文件。
  点击页面右上角的部署按钮，进入基本信息设置页面。
在此页面中，你可进行如下设置：
 在应用名称输入框中，可修改应用名称； 当应用包含多个版本时，可点击应用版本下拉框，选择所需的版本进行部署； 在描述信息中，可设置部署应用的描述信息； 点击部署位置，可选择应用部署的企业空间和项目。    点击下一步，进入到应用配置页面。
目前，支持 2 种配置方式：可视化配置和 yaml 配置。通过点击 YAML 按钮，来实现配置方式的转换。
在应用配置页面中，可设置 mongdb 的存储大小、root 用户名和密码。
  点击部署，自动跳转到 demo-project 的应用列表页面。</description>
    </item>
    
    <item>
      <title>在 KubeSphere 中部署 Nginx</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/%E5%9C%A8kubesphere%E4%B8%AD%E9%83%A8%E7%BD%B2nginx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/%E5%9C%A8kubesphere%E4%B8%AD%E9%83%A8%E7%BD%B2nginx/</guid>
      <description>本文介绍在 kubesphere 中通过应用商店部署 Nginx 的操作步骤。
前提条件  已安装了 OpenPitrix（应用商店）功能组件。若未开启应用商店功能组件，可参考开启组件进行开启。 已创建了单集群企业空间、单集群项目和普通用户 project-regular 账号。 使用项目管理员 project-admin 邀请项目普通用户 project-regular 加入项目并授予 operator 角色。  操作说明   使用 project-regular 账号进入已创建的项目 demo-project  后，选择应用负载，点击应用，点击 部署新应用，然后在弹窗中选择 来自应用商店。
  进入应用商店页面。
  选择并点击 nginx 应用，进入应用信息页面。
在应用详情&amp;ndash;配置文件中，可以查看 Nginx 应用的 helm chart 的配置文件。
  点击页面右上角的部署按钮，进入基本信息设置页面。
在此页面中，你可进行如下设置：
 在应用名称输入框中，可修改应用名称； 当应用包含多个版本时，可点击应用版本下拉框，选择所需的版本进行部署； 在描述信息中，可设置部署应用的描述信息； 点击部署位置，可选择应用部署的企业空间和项目。    点击下一步，进入到应用配置页面。
目前，支持 2 种配置方式：可视化配置和 yaml 配置。通过点击 YAML 按钮，来实现配置方式的转换。
在应用配置页面中，可设置 nginx 的副本数，并设置是否开启 Ingress。
  点击部署，自动跳转到 demo-project 的应用列表页面。</description>
    </item>
    
    <item>
      <title>在 KubeSphere 中部署 Redis</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/%E5%9C%A8kubesphere%E4%B8%AD%E9%83%A8%E7%BD%B2redis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/%E5%9C%A8kubesphere%E4%B8%AD%E9%83%A8%E7%BD%B2redis/</guid>
      <description>本文介绍在 kubesphere 中通过应用商店部署 Redis 的操作步骤。
前提条件  已安装了 OpenPitrix（应用商店）功能组件。若未开启应用商店功能组件，可参考开启组件进行开启。 已创建了单集群企业空间、单集群项目和普通用户 project-regular 账号。 使用项目管理员 project-admin 邀请项目普通用户 project-regular 加入项目并授予 operator 角色。  操作说明   使用 project-regular 账号进入已创建的项目 demo-project  后，选择应用负载，点击应用，点击 部署新应用，然后在弹窗中选择 来自应用商店。
  进入应用商店页面。
  选择并点击 redis 应用，进入应用信息页面。
在应用详情&amp;ndash;配置文件中，可以查看 redis 应用的 helm chart 的配置文件。
  点击页面右上角的部署按钮，进入基本信息设置页面。
在此页面中，你可进行如下设置：
 在应用名称输入框中，可修改应用名称； 当应用包含多个版本时，可点击应用版本下拉框，选择所需的版本进行部署； 在描述信息中，可设置部署应用的描述信息； 点击部署位置，可选择应用部署的企业空间和项目。    点击下一步，进入到应用配置页面。
目前，支持 2 种配置方式：可视化配置和 yaml 配置。通过点击 YAML 按钮，来实现配置方式的转换。
在应用配置页面中，可设置 redis 的密码。
  点击部署，自动跳转到 demo-project 的应用列表页面。</description>
    </item>
    
    <item>
      <title>Harbor App</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/harbor-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/harbor-app/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Redis App</title>
      <link>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/redis-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/application-store/built-in-apps/redis-app/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>在 Oracle OKE 上部署 KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-oke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-oke/</guid>
      <description>本文演示在 Oracle Kubernetes Engine 上部署 KubeSphere 的步骤。
创建 Kubernetes 集群  在 OKE 上创建一个标准的 Kubernetes 集群是安装 KubeSphere 的前提条件。在导航栏中，请参考下图创建集群。  在弹出窗口中，选择快速创建并点击启动工作流。  备注
本示例演示快速创建，Oracle Cloud 通过此模式会为集群自动创建所必需的资源。如果您选择定制创建，您需要自己创建所有资源（例如 VCN 和负载均衡器子网）。 接下来，您需要为集群设置基本信息（可参考以下图例）。完成后，请点击下一步。  备注
 KubeSphere 3.0.0 所支持的 Kubernetes 版本：1.15.x, 1.16.x, 1.17.x, 1.18.x。 建议您在可见性类型中选择公共，即每个节点会分配到一个公共 IP 地址，此地址之后可用于访问 KubeSphere Web 控制台。 在 Oracle Cloud 中，配置定义了一个实例会分配到的 CPU 和内存等资源量，本示例使用 VM.Standard.E2.2 (2 CPUs and 16G Memory)。有关更多信息，请参见 Standard Shapes。 本示例包含 3 个节点，您可以根据需求自行添加节点（尤其是生产环境）。   检查集群信息，确认无需修改后点击创建集群。  集群创建后，点击关闭。  确保集群状态为活动后，点击访问集群。  在弹出窗口中，选择 Cloud Shell 访问权限。点击启动 Cloud Shell，并将 Oracle Cloud 所提供的命令复制到 Cloud Shell。  在 Cloud Shell 中，粘贴该命令以便之后可以执行 KubeSphere 安装命令。  警告</description>
    </item>
    
    <item>
      <title>DaemonSets</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/daemonsets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/daemonsets/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>在腾讯云 TKE 安装 KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-ks-on-tencent-tke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-ks-on-tencent-tke/</guid>
      <description>本指南将介绍如何在腾讯云 TKE 上部署并使用 KubeSphere 3.0.0 平台。
腾讯云 TKE 环境准备 创建 Kubernetes 集群 首先按使用环境的资源需求创建 Kubernetes 集群，满足以下一些条件即可（如已有环境并满足条件可跳过本节内容）：
 KubeSphere 3.0.0 默认支持的 Kubernetes 版本为 1.15.x, 1.16.x, 1.17.x, 1.18.x，需要选择其中支持的版本进行集群创建（如 1.16.3, 1.18.4）； 工作节点机型配置规格方面选择 SA2.LARGE8 的 4核｜8GB 配置即可，并按需扩展工作节点数量（通常生产环境需要 3 个及以上工作节点）。  创建公网 kubectl 证书  创建完集群后，进入 容器服务 &amp;gt; 集群 界面，选择刚创建的集群，在 基本信息 面板中， 集群APIServer信息 中开启 外网访问 。 然后在下方 kubeconfig 列表项中点击 下载，即可获取公用可用的 kubectl 证书。   获取 kubectl 配置文件后，可通过 kubectl 命令行工具来验证集群连接：  $ kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;18&amp;#34;, GitVersion:&amp;#34;v1.</description>
    </item>
    
    <item>
      <title>在华为云 CCE 安装 KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-huaweicloud-cce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-huaweicloud-cce/</guid>
      <description>本指南将介绍如果在华为云 CCE 容器引擎上部署并使用 KubeSphere 3.0.0 平台。
华为云 CCE 环境准备 创建 Kubernetes 集群 首先按使用环境的资源需求创建 Kubernetes 集群，满足以下一些条件即可（如已有环境并满足条件可跳过本节内容）：
 KubeSphere 3.0.0 默认支持的 Kubernetes 版本为 1.15.x, 1.16.x, 1.17.x, 1.18.x，需要选择其中支持的版本进行集群创建（如 v1.15.11, v1.17.9）； 需要确保 Kubernetes 集群所使用的云主机的网络可以，可以通过在创建集群的同时 “自动创建” 或 “使用已有” 弹性 IP；或者在集群创建后自行配置网络（如配置 NAT 网关）； 工作节点规格方面建议选择 s3.xlarge.2 的 4核｜8GB 配置，并按需扩展工作节点数量（通常生产环境需要 3 个及以上工作节点）。  创建公网 kubectl 证书  创建完集群后，进入 资源管理 &amp;gt; 集群管理 界面，在 基本信息 &amp;gt; 网络 面板中，绑定 公网apiserver地址； 而后在右侧面板中，选择 kubectl 标签页，并在 下载kubectl配置文件 列表项中 点击此处下载，即可获取公用可用的 kubectl 证书。  获取 kubectl 配置文件后，可通过 kubectl 命令行工具来验证集群连接：</description>
    </item>
    
    <item>
      <title>Composing an App for Microservices</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/composing-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/composing-app/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>CronJobs</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/cronjob/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/cronjob/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Ingress</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/ingress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/ingress/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>s2i-template</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/s2i-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/s2i-template/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Services</title>
      <link>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/services/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/project-user-guide/application-workloads/services/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Deploy KubeSphere on AWS EKS</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-eks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-eks/</guid>
      <description>This guide walks you through the steps of deploying KubeSphere on AWS EKS.
Install the AWS CLI Tht aws EKS does not have a web terminal like GKE, so we must install aws cli first. Take a example for macOS and other operating system can according Getting Started EKS
pip3 install awscli --upgrade --user Check it with aws --version Prepare a EKS Cluster  A standard Kubernetes cluster in AWS is a prerequisite of installing KubeSphere.</description>
    </item>
    
    <item>
      <title>Deploy KubeSphere on DigitalOcean</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-do/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-do/</guid>
      <description>This guide walks you through the steps of deploying KubeSphere on DigitalOcean Kubernetes.
Prepare a DOKS Cluster A Kubernetes cluster in DO is a prerequisite for installing KubeSphere. Go to your DO account and, in the navigation menu, refer to the image below to create a cluster.
You need to select:
 Kubernetes version (e.g. 1.18.6-do.0) Datacenter region (e.g. Frankfurt) VPC network (e.g. default-fra1) Cluster capacity (e.g. 2 standard nodes with 2 vCPUs and 4GB of RAM each) A name for the cluster (e.</description>
    </item>
    
    <item>
      <title>Deploy KubeSphere on GKE</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-gke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-gke/</guid>
      <description>This guide walks you through the steps of deploying KubeSphere on Google Kubernetes Engine.
Prepare a GKE Cluster  A standard Kubernetes cluster in GKE is a prerequisite of installing KubeSphere. Go to the navigation menu and refer to the image below to create a cluster.   In Cluster basics, select a Master version. The static version 1.15.12-gke.2 is used here as an example.   In default-pool under Node Pools, define 3 nodes in this cluster.</description>
    </item>
    
    <item>
      <title>Deploy KubeSphere on AKS</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-aks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/hosted-kubernetes/install-kubesphere-on-aks/</guid>
      <description>This guide walks you through the steps of deploying KubeSphere on Azure Kubernetes Service.
Prepare an AKS cluster Azure can help you implement infrastructure as code by providing resource deployment automation options. Commonly adopted tools include ARM templates and Azure CLI. In this guide, we will use Azure CLI to create all the resources that are needed for the installation of KubeSphere.
Use Azure Cloud Shell You don&amp;rsquo;t have to install Azure CLI on your machine as Azure provides a web-based terminal.</description>
    </item>
    
    <item>
      <title>Add Kafka as Receiver (aka Collector)</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/add-kafka-as-receiver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/add-kafka-as-receiver/</guid>
      <description>KubeSphere supports using Elasticsearch, Kafka and Fluentd as log receivers. This doc will demonstrate:
 Deploy strimzi-kafka-operator and then create a Kafka cluster and a Kafka topic by creating Kafka and KafkaTopic CRDs. Add Kafka log receiver to receive logs sent from Fluent Bit Verify whether the Kafka cluster is receiving logs using Kafkacat  Prerequisite Before adding a log receiver, you need to enable any of the logging, events or auditing components following Enable Pluggable Components.</description>
    </item>
    
    <item>
      <title>Import Aliyun ACK Cluster</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/import-cloud-hosted-k8s/import-aliyun-ack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/import-cloud-hosted-k8s/import-aliyun-ack/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Import Kubeadm Kubernetes Cluster</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/import-on-prem-k8s/import-kubeadm-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/import-on-prem-k8s/import-kubeadm-k8s/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>Remove a Cluster from KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/remove-cluster/kubefed-in-kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/remove-cluster/kubefed-in-kubesphere/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>导入 AWS EKS 集群</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/import-cloud-hosted-k8s/import-aws-eks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/import-cloud-hosted-k8s/import-aws-eks/</guid>
      <description>在本节中，我们将向您展示如何使用直接连接方法将 EKS 导入 KubeSphere。
备注
如果您打算使用代理连接导入 EKS，则可以跳过本章节并按照代理连接的文档逐步进行。 Amazon EKS不像标准 kubeadm 集群那样提供内置的 kubeconfig 文件。但是您可以通过参考此文档自动创建 kubeconfig。生成的 kubeconfig 将如下所示，
apiVersion: v1 clusters: - cluster: server: &amp;lt;endpoint-url&amp;gt; certificate-authority-data: &amp;lt;base64-encoded-ca-cert&amp;gt; name: kubernetes contexts: - context: cluster: kubernetes user: aws name: aws current-context: aws kind: Config preferences: {} users: - name: aws user: exec: apiVersion: client.authentication.k8s.io/v1alpha1 command: aws args: - &amp;#34;eks&amp;#34; - &amp;#34;get-token&amp;#34; - &amp;#34;--cluster-name&amp;#34; - &amp;#34;&amp;lt;cluster-name&amp;gt;&amp;#34; # - &amp;#34;--role&amp;#34; # - &amp;#34;&amp;lt;role-arn&amp;gt;&amp;#34; # env: # - name: AWS_PROFILE # value: &amp;#34;&amp;lt;aws-profile&amp;gt;&amp;#34; 看起来不错，自动生成的 kubeconfig 只有一个问题，它要求在想要使用此 kubeconfig 的每台计算机上安装命令 aws（aws 命令行工具）。</description>
    </item>
    
    <item>
      <title>添加新节点</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/cluster-operation/add-new-nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/cluster-operation/add-new-nodes/</guid>
      <description>使用 KubeSphere 一段时间后，很可能需要随着工作负载的增加来扩展集群。 在这种情况下，KubeSphere 提供了将新节点添加到集群的脚本。 基本上，该操作基于 Kubelet 的注册机制，即新节点将自动加入现有的Kubernetes 集群。
提示
从 v3.0.0 起，全新的安装程序 KubeKey 支持从一个单节点集群扩展主节点和工作节点。 步骤1：修改主机配置 KubeSphere 支持混合环境，即新添加的主机操作系统可以是 CentOS 或 Ubuntu 。 准备好新机器后，在文件config-sample.yaml的和roleGroups中添加有关新机器信息的配置。
警告
添加新节点时，不允许修改原始节点（例如 master1）的主机名。 例如，如果您使用 all-in-one 开始安装，并且想要为单节点集群添加新节点 ，您可以使用KubeKey创建配置文件。
# Assume your original Kubernetes cluster is v1.17.9./kk create config --with-kubesphere --with-kubernetes v1.17.9以下部分以“ root”用户为例，演示如何添加两个节点（即“ node1”和“ node2”），并假设第一台计算机的主机名是“ master1”（用您的主机名替换以下主机名） 。
spec: hosts: - {name: master1, address: 192.168.0.3, internalAddress: 192.168.0.3, user: root, password: Qcloud@123} - {name: node1, address: 192.168.0.4, internalAddress: 192.</description>
    </item>
    
    <item>
      <title>删除节点</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/cluster-operation/remove-nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/cluster-operation/remove-nodes/</guid>
      <description>停止调度节点 将节点标记为不可调度可防止调度程序将新的容器放置到该节点上，但不会影响该节点上的现有容器。 这对于节点重新引导或其他维护之前的准备步骤很有用。
若要将节点标记为不可调度，可以从菜单中选择 节点管理→群集节点 ，然后找到要从群集中删除的节点，然后单击停止调度按钮。 它与命令kubectl cordon $NODENAME具有相同的效果，有关更多详细信息，请参见 Kubernetes Nodes。
备注
注意：作为 DaemonSet 一部分的 Pod 可以在无法调度的节点上运行。 守护程序集通常提供应在节点上运行的节点本地服务，即使正在耗尽工作负载应用程序也是如此。 删除节点 您可以通过以下命令删除节点：
./kk delete node &amp;lt;nodeName&amp;gt; -f config-sample.yaml</description>
    </item>
    
    <item>
      <title>Add Fluentd as Receiver (aka Collector)</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/add-fluentd-as-receiver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/log-collections/add-fluentd-as-receiver/</guid>
      <description>KubeSphere supports using Elasticsearch, Kafka and Fluentd as log receivers. This doc will demonstrate:
 How to deploy Fluentd as deployment and create corresponding service and configmap. How to add Fluentd as a log receiver to receive logs sent from Fluent Bit and then output to stdout. How to verify if Fluentd receives logs successfully.  Prerequisites  Before adding a log receiver, you need to enable any of the logging, events or auditing components following Enable Pluggable Components.</description>
    </item>
    
    <item>
      <title>卸载 KubeSphere 和 Kubernetes</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/uninstalling/uninstalling-kubesphere-and-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/uninstalling/uninstalling-kubesphere-and-kubernetes/</guid>
      <description>您可以通过以下命令删除集群。
提示
卸载将会从计算机中删除 KubeSphere 和 Kubernetes。 此操作是不可逆的，没有任何备份。 请谨慎操作。  如果您以快速入门 ( all-in-one )开始：  ./kk delete cluster 如果从高级模式开始（使用配置文件创建）：  ./kk delete cluster [-f config-sample.yaml]</description>
    </item>
    
    <item>
      <title>Configure Booster for Installation</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/faq/configure-booster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/faq/configure-booster/</guid>
      <description>If you have trouble downloading images from dockerhub.io, it is highly recommended that you configure a registry mirror (i.e. booster) beforehand to speed up downloads. You can refer to the official documentation of Docker or follow the steps below.
Get Booster URL To configure the booster, you need a registry mirror address. See the following example to see how you can get a booster URL from Alibaba Cloud.
 Log in the console of Alibaba Cloud and enter &amp;ldquo;container registry&amp;rdquo; in the search bar.</description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/faq/faq1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-kubernetes/faq/faq1/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>概述</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/introduction/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/introduction/overview/</guid>
      <description>如今，在不同的云服务提供商或者基础设施上运行和管理多个 Kubernetes 集群已经非常普遍。 由于每个 Kubernetes 集群都是一个相对独立的单元，上游社区正努力研发多集群管理解决方案。 也就是说，Kubernetes 集群联邦（Kubernetes Cluster Federation，简称 KubeFed）可能是其中一种可行的方法。
多集群管理最常见的用例包括服务流量负载均衡、开发和生产的隔离、数据处理和数据存储的分离、跨云备份和灾难恢复、计算资源的灵活分配、跨区域服务的低延迟访问以及厂商捆绑的防范。
KubeSphere 的开发旨在解决多集群和多云管理的难题，并实现后续的用户场景，为用户提供统一的控制平面，以将应用程序及其副本分发到从公有云到本地环境的多个集群。 KubeSphere 还提供跨多个集群的丰富的可观测性，包括集中式监控、日志、事件和审计日志。</description>
    </item>
    
    <item>
      <title>KubeSphere 中的 Kubernetes 联邦</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/introduction/kubefed-in-kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/introduction/kubefed-in-kubesphere/</guid>
      <description>多集群功能与多个集群之间的网络连接有关。 因此，了解集群的拓扑关系很重要，这样可以减少工作量。
在使用多集群功能之前，您需要创建一个主集群（Host Cluster，以下简称 H 集群），H 集群实际上是启用了多集群功能的 KubeSphere 集群。所有被 H 集群管理的集群称为成员集群（Member Cluster，以下简称 M 集群）。M 集群是未启用多集群功能的普通 KubeSphere 集群。只能有一个 H 集群存在，而多个 M 集群可以同时存在。 在多集群体系结构中，H 集群和 M 集群之间的网络可以直接连接，也可以通过代理连接。 M 集群之间的网络可以设置在完全隔离的环境中。</description>
    </item>
    
    <item>
      <title>Linux 上的 All-in-one 安装</title>
      <link>https://cnimages.github.io/website/zh/docs/quick-start/all-in-one-on-linux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/quick-start/all-in-one-on-linux/</guid>
      <description>对于那些刚接触 KubeSphere 的并且想快速上手的用户，all-in-one 安装模式是最佳的选择，它能够帮助您零配置快速部署 KubeSphere 和 Kubernetes。
前提条件 按照文档打开需要开放的端口。
步骤 1: 准备 Linux 机器 请参考下面对机器硬件和操作系统的要求准备一台主机。
建议的机器硬件配置    操作系统 最低要求     Ubuntu 16.04, 18.04 CPU: 2 Cores, Memory: 4 G, Disk Space: 40 G   Debian Buster, Stretch CPU: 2 Cores, Memory: 4 G, Disk Space: 40 G   CentOS 7.x CPU: 2 Cores, Memory: 4 G, Disk Space: 40 G   Red Hat Enterprise Linux 7 CPU: 2 Cores, Memory: 4 G, Disk Space: 40 G   SUSE Linux Enterprise Server 15/openSUSE Leap 15.</description>
    </item>
    
    <item>
      <title>直接连接</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/enable-multicluster/direct-connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/enable-multicluster/direct-connection/</guid>
      <description>前提条件 您已经安装了至少两个 KubeSphere 集群。如果尚未安装，请参阅在Linux上安装或者在Kubernetes上安装。
备注
多集群管理要求将 Kubesphere 安装在目标集群上。如果您已经有一个集群，则可以在上面部署 KubeSphere 最小安装，以便可以将其导入。有关详细信息，请参阅 Kubernetes 上的最小 KubeSphere。 直接连接 如果成员集群（以下简称 M 集群）的 kube-apiserver 地址可以在主集群（以下简称 H 集群）的任何节点上访问，则可以采用 直接连接。当 M 集群的 kube-apiserver 地址可以暴漏，或者 H 集群和 M 集群在同一专网或子网中时，此方法适用。
准备主集群 如果已经安装了独立的 KubeSphere，则可以通过编辑集群配置，将 clusterRole 的值设置为 host。您需要稍等片刻，以使更改生效。
  选项 A - 使用 web 控制台：
使用 admin 帐户登录控制台，然后进入集群管理页面上的 CRDs。输入关键字 ClusterConfiguration，然后转到其详细信息页面。编辑 ks-installer 的YAML，类似于启用可插拔组件。
  选项 B - 使用 Kubectl：
kubectl edit cc ks-installer -n kubesphere-system   向下滚动并将 clusterRole的值设置为 host，然后点击更新（如果使用 web 控制台）以使其生效：</description>
    </item>
    
    <item>
      <title>代理连接</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/enable-multicluster/agent-connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/enable-multicluster/agent-connection/</guid>
      <description>前提条件 您已经安装了至少两个 KubeSphere 集群。如果尚未安装，请参阅在 Linux 上安装或者在 Kubernetes 上安装。
备注
多集群管理要求将 Kubesphere 安装在目标集群上。如果您已经有一个集群，则可以在上面部署 KubeSphere 最小安装，以便可以将其导入。有关详细信息，请参见 Kubernetes 上的最小 KubeSphere。 代理连接 KubeSphere 的 Tower 组件用于代理连接。Tower 是通过代理在集群之间进行网络连接的工具。如果 H 集群无法直接访问 M 集群，则可以暴漏 H 集群的代理服务地址。这使 M 集群可以通过代理连接到 H 集群。当 M 集群处于私有环境（例如 IDC）并且 H 集群能够暴漏代理服务时，此方法适用。 当您的集群分布在不同的云服务提供商之间时，代理连接也适用。
准备主集群 如果已经安装了独立的 KubeSphere，则可以通过编辑集群配置，将 clusterRole 的值设置为 host 。您需要稍等片刻，以使更改生效。
 选项 A - 使用 web 控制台：  使用 admin 帐户登录控制台，然后转到集群管理页面上的 CRDs。输入关键字 ClusterConfiguration，然后转到其详细信息页面。编辑 ks-installer 的 YAML，类似于启用可插拔组件。
  选项 B - 使用 Kubectl：</description>
    </item>
    
    <item>
      <title>获取 KubeConfig</title>
      <link>https://cnimages.github.io/website/zh/docs/multicluster-management/enable-multicluster/retrieve-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/multicluster-management/enable-multicluster/retrieve-kubeconfig/</guid>
      <description>前提条件 您有一个 KubeSphere 集群。
浏览 KubeConfig 文件 进入 $HOME/.kube，检查目录中的文件，通常该目录下存在一个名为 config 的文件。使用以下命令获取 KubeConfig 文件：
cat $HOME/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EZ3dPREE1hqaVE3NXhwbGFQNUgwSm5ySk5peTBacFh6QWxjYzZlV2JlaXJ1VgpUbmZUVjZRY3pxaVcrS3RBdFZVbkl4MCs2VTgzL3FiKzdINHk2RnA0aVhUaDJxRHJ6Qkd4dG1UeFlGdC9OaFZlCmhqMHhEbHVMOTVUWkRjOUNmSFgzdGZJeVh5WFR3eWpnQ2g1RldxbGwxVS9qVUo2RjBLVVExZ1pRTFp4TVJMV0MKREM2ZFhvUGlnQ3BNaVRPVXl5SVNhWUVjYVNBMEo5VWZmSGd4ditVcXVleTc0cEM2emszS0lOT2tGMkI1MllxeApUa09OT2VkV2hDUExMZkUveVJqeGw1aFhPL1Z4REFaVC9HQ1Y1a0JZN0toNmRhendmUllOa21IQkhDMD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=hqaVE3NXhwbGFQNUgwSm5ySk5peTBacFh6QWxjYzZlV2JlaXJ1VgpUbmZUVjZRY3pxaVcrS3RBdFZVbkl4MCs2VTgzL3FiKzdINHk2RnA0aVhUaDJxRHJ6Qkd4dG1UeFlGdC9OaFZlCmhqMHhEbHVMOTVUWkRjOUNmSFgzdGZJeVh5WFR3eWpnQ2g1RldxbGwxVS9qVUo2RjBLVVExZ1pRTFp4TVJMV0MKREM2ZFhvUGlnQ3BNaVRPVXl5SVNhWUVjYVNBMEo5VWZmSGd4ditVcXVleTc0cEM2emszS0lOT2tGMkI1MllxeApUa09OT2VkV2hDUExMZkUveVJqeGw1aFhPL1Z4REFaVC9HQ1Y1a0JZN0toNmRhendmUllOa21IQkhDMD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://lb.kubesphere.local:6443 name: cluster.local contexts: - context: cluster: cluster.local user: kubernetes-admin name: kubernetes-admin@cluster.local current-context: kubernetes-admin@cluster.local kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBZ0lJRzd5REpscVdjdTh3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURBNE1EZ3dPVEkzTXpkYUZ3MHlNVEE0TURnd09USTNNemhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnsOTJBUkJDNTRSR3BsZ3VmCmw5a0hPd0lEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEQ2FUTXNBR1Vhdnhrazg0NDZnOGNRQUJpSmk5RTZiREV5TwphRnJubC8reGRzRmgvOTFiMlNpM3ZwaHFkZ2k5bXRYWkhhaWI5dnQ3aXdtSEFwbGQxUkhBU25sMFoxWFh1dkhzCmMzcXVIU0puY3dmc3JKT0I4UG9NRjVnaG10a0dPV3g0M2RHTTNHQnpGTVJ4ZGcrNmttNjRNUGhneXl6NTJjYUoKbzhPajNja1Uzd1NWNkxvempRcFVaUnZHV25qQjEwUXFPWXBtQUk4VCtlZkxKZzhuY0drK3V3UUVTeXBYWExpYwoxWVQ2QkFJeFhEK2tUUU1hOFhjdUhHZzlWRkdsUm9yK1EvY3l0S3RDeHVncFlxQ2xvbHVpckFUUnpsemRXamxYCkVQaHVjRWs2UUdIZEpObjd0M2NwRGkzSUdYYXJFdGxQQmFwck9nSGpkOHZVOStpWXdoQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=TJBUkJDNTRSR3BsZ3VmCmw5a0hPd0lEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEQ2FUTXNBR1Vhdnhrazg0NDZnOGNRQUJpSmk5RTZiREV5TwphRnJubC8reGRzRmgvOTFiMlNpM3ZwaHFkZ2k5bXRYWkhhaWI5dnQ3aXdtSEFwbGQxUkhBU25sMFoxWFh1dkhzCmMzcXVIU0puY3dmc3JKT0I4UG9NRjVnaG10a0dPV3g0M2RHTTNHQnpGTVJ4ZGcrNmttNjRNUGhneXl6NTJjYUoKbzhPajNja1Uzd1NWNkxvempRcFVaUnZHV25qQjEwUXFPWXBtQUk4VCtlZkxKZzhuY0drK3V3UUVTeXBYWExpYwoxWVQ2QkFJeFhEK2tUUU1hOFhjdUhHZzlWRkdsUm9yK1EvY3l0S3RDeHVncFlxQ2xvbHVpckFUUnpsemRXamxYCkVQaHVjRWs2UUdIZEpObjd0M2NwRGkzSUdYYXJFdGxQQmFwck9nSGpkOHZVOStpWXdoQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBeXBLWkdtdmdiSHdNaU9pVU80UHZKZXB2MTJaaE1yRUIxK2xlVnM0dHIzMFNGQ0p1Ck8wc09jL2lUNmFuWEJzUU1XNDF6V3hwV1B5elkzWXlUWEJMTlIrM01pWTl2SFhUeWJ6eitTWnNlTzVENytHL3MKQnR5NkovNGpJb2pZZlRZNTFzUUxyRVJydStmVnNGeUU0U2dXbE1HYWdqV0RIMFltM0VJsOTJBUkJDNTRSR3BsZ3VmCmw5a0hPd0lEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEQ2FUTXNBR1Vhdnhrazg0NDZnOGNRQUJpSmk5RTZiREV5TwphRnJubC8reGRzRmgvOTFiMlNpM3ZwaHFkZ2k5bXRYWkhhaWI5dnQ3aXdtSEFwbGQxUkhBU25sMFoxWFh1dkhzCmMzcXVIU0puY3dmc3JKT0I4UG9NRjVnaG10a0dPV3g0M2RHTTNHQnpGTVJ4ZGcrNmttNjRNUGhneXl6NTJjYUoKbzhPajNja1Uzd1NWNkxvempRcFVaUnZHV25qQjEwUXFPWXBtQUk4VCtlZkxKZzhuY0drK3V3UUVTeXBYWExpYwoxWVQ2QkFJeFhEK2tUUU1hOFhjdUhHZzlWRkdsUm9yK1EvY3l0S3RDeHVncFlxQ2xvbHVpckFUUnpsemRXamxYCkVQaHVjRWs2UUdIZEpObjd0M2NwRGkzSUdYYXJFdGxQQmFwck9nSGpkOHZVOStpWXdoQT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=Ygo3THE3a2tBMURKNTBld2pMUTNTd1Yxd2p6N2ZjeDYvbzUwRnJnK083dEJMVVdQNTNHaDQ1VjJpUEp2NkdPYk1uCjhIWElmem83cW5XRFQvU20ybW5HbitUdVY4THdLVWFXL2wya3FkRUNnWUVBcS9zRmR1RDk2Z3VoT2ZaRnczcWMKblZGekNGQ3JsMkUvVkdYQy92SmV1WnJLQnFtSUtNZFI3ajdLWS9WRFVlMnJocVd6MFh2Wm9Sa1FoMkdwWkdIawpDd3NzcENKTVl4L0hETTVaWlBvcittb1J6VE5HNHlDNGhTRGJ2VEFaTmV1VTZTK1hzL1JSTDJ6WnUwemNQQXk1CjJJRVgwelFpZ1JzK3VzS3Jkc1FVZXZrQ2dZQUUrQUNWeDJnMC94bmFsMVFJNmJsK3Y2TDJrZVJtVGppcHB4Wm0KS1JEd2xnaXpsWGxsTjhyQmZwSGNiK1ZnZ282anN2eHFrb0pkTEhBLzFDME5IMWVuS1NoUTlpZVFpeWNsZngwdQpKOE1oeW1JM0RBZUg1REJyOG1rZ0pwNnJwUXNBc1paYmVhOHlLTzV5eVdCYTN6VGxOVnQvNDRibGg5alpnTWNMCjNyUXFVUUtCZ1FETVlXdEt2S0hOQllXV0p5enFERnFPbS9qY3Z3andvcURibUZVMlU3UGs2aUdNVldBV3VYZ3cKSm5qQWtES01GN0JXSnJRUjR6RHVoQlhvQVMxWVhiQ2lGd2hTcXVjWGhFSGlwQ3Nib0haVVRtT1pXUUh4Vlp4bQowU1NiRXFZU2MvZHBDZ1BHRk9IaW1FdUVic05kc2JjRmRETDQyODZHb0psQUxCOGc3VWRUZUE9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= </description>
    </item>
    
    <item>
      <title>在 Kubernetes 最小化安装 KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/quick-start/minimal-kubesphere-on-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/quick-start/minimal-kubesphere-on-k8s/</guid>
      <description>除了在 Linux 机器上安装 KubeSphere 之外，您还可以将其直接部署在现有的 Kubernetes 集群上。本快速入门指南将引导您完成在 Kubernetes 上最小化安装 KubeSphere 的一般步骤。有关更多信息，请参阅在 Kubernetes 上安装。
备注
 Kubernetes 版本必须为 “1.15.x，1.16.x，1.17.x 或 1.18.x”； 确保您的计算机满足最低硬件要求：CPU &amp;gt; 1 核，内存 &amp;gt; 2 G； 在安装之前，需要配置 Kubernetes 集群中的默认存储类； 当使用 --cluster-signing-cert-file 和 --cluster-signing-key-file 参数启动时，在 kube-apiserver 中会激活 CSR 签名功能。 请参阅 RKE 安装问题； 有关在 Kubernetes 上安装 KubeSphere 的前提条件的详细信息，请参阅前提条件。   部署 KubeSphere 确保您的计算机满足前提条件之后，您可以按照以下步骤安装 KubeSphere。
 在执行命令开始安装之前，请阅读以下注释：  kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/kubesphere-installer.yaml kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/cluster-configuration.yaml 备注
如果您的服务器无法访问 GitHub，则可以分别复制 kubesphere-installer.yaml 和 cluster-configuration.</description>
    </item>
    
    <item>
      <title>Create Workspace, Project, Account and Role</title>
      <link>https://cnimages.github.io/website/zh/docs/quick-start/create-workspace-and-project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/quick-start/create-workspace-and-project/</guid>
      <description>Objective This guide demonstrates how to create roles and user accounts which are required for the following tutorials. Meanwhile, you will learn how to create projects and DevOps projects within your workspace where your workloads are running. After this tutorial, you will become familiar with KubeSphere multi-tenant management system.
Prerequisites KubeSphere needs to be installed in your machine.
Estimated Time About 15 minutes.
Architecture The multi-tenant system of KubeSphere features three levels of hierarchical structure which are cluster, workspace and project.</description>
    </item>
    
    <item>
      <title>Deploy Bookinfo and Manage Traffic</title>
      <link>https://cnimages.github.io/website/zh/docs/quick-start/deploy-bookinfo-to-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/quick-start/deploy-bookinfo-to-k8s/</guid>
      <description>Istio, as an open-source service mesh solution, provides powerful features of traffic management for microservices. Here is the introduction of traffic management from the official website of Istio:
Istio’s traffic routing rules let you easily control the flow of traffic and API calls between services. Istio simplifies configuration of service-level properties like circuit breakers, timeouts, and retries, and makes it easy to set up important tasks like A/B testing, canary rollouts, and staged rollouts with percentage-based traffic splits.</description>
    </item>
    
    <item>
      <title>创建 Wordpress 应用并发布至 Kubernetes</title>
      <link>https://cnimages.github.io/website/zh/docs/quick-start/wordpress-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/quick-start/wordpress-deployment/</guid>
      <description>WordPress 简介 WordPress 是使用 PHP 语言开发的内容管理系统软件，用户可以在支持 PHP 和 MySQL 数据库的服务器上使用自己的博客，一个完整的 Wordpress 应用程序包括以下 Kubernetes 对象。
目的 本教程演示如何在 KubeSphere 中创建应用程序（以 WordPress 为例）并在集群外访问它。
准备工作 需要一个 project regular 帐户，并在其中一个项目中分配角色 operator（该用户已被邀请参加该项目）。有关详细信息，请参见创建企业空间、项目、帐户和角色.
预计操作时间 大约15分钟。
动手操作 任务 1: 创建密钥 创建 MySQL 密钥 环境变量 WORDPRESS_DB_PASSWORD 是连接到 WORDPRESS 数据库的密码。在这一步中，您需要创建一个 ConfigMap 来存储将在 MySQL pod 模板中使用的环境变量。
 使用project regular帐户登录 KubeSphere 控制台。转到demo project的详细页面并导航到 配置。在 密钥 中，单击右侧的 创建。  输入基本信息 (例如，将其命名为 mysql-secret) ，然后单击 下一步。在下一页中, 选择 类型 为 默认 ，然后单击 添加数据 来添加一个键值对。 输入键（Key） MYSQL_ROOT_PASSWORD 和值（Value） 123456 单击右下角 √ 的确认按钮, 完成后，单击 创建 按钮并继续.</description>
    </item>
    
    <item>
      <title>启用可插拔功能组件</title>
      <link>https://cnimages.github.io/website/zh/docs/quick-start/enable-pluggable-components/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/quick-start/enable-pluggable-components/</guid>
      <description>本教程演示如何在安装前或安装后启用 KubeSphere 的可插拔组件。KubeSphere 具有以下列出的十个可插拔组件。
   配置项 功能组件 描述     alerting KubeSphere 告警通知系统 使用户能够自定义警报策略，以不同的时间间隔和警报级别及时向接收者发送消息。   auditing KubeSphere 审计日志系统 按时间顺序记录不同租户在平台中的操作活动。   devops KubeSphere DevOps 系统 一站式 DevOps 方案，内置 Jenkins 流水线与 B2I &amp;amp; S2I。   events KubeSphere 事件系统 提供一个图形化的web控制台，用于导出、过滤和警告多租户 Kubernetes 集群中的 Kubernetes 事件。   logging KubeSphere 日志系统 在统一的控制台中提供灵活的日志查询、收集和管理日志功能。可以添加其他日志收集器，例如Elasticsearch、Kafka 和 Fluentd。   metrics_server HPA 能够根据 pod 数量进行动态伸缩，使运行在上面的服务对指标的变化有一定的自适应能力。   networkpolicy 网络策略 可以在同一个集群内部之间设置网络策略（比如限制或阻止某些实例 pod 之间的网络请求）。   notification KubeSphere 通知系统 允许用户将 AlertManager 设置为发件人并发送告警邮件。可以使用的方式有：电子邮件、微信和 Slack。   openpitrix KubeSphere 应用商店 基于 Helm 的应用程序商店，允许用户在整个生命周期中管理应用程序。   servicemesh KubeSphere 服务网格 (基于 Istio) 支持灰度发布、流量拓扑、流量治理、Tracing。    有关每个组件的更多信息，请参见启用可插拔组件概述。</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/overview/</guid>
      <description>TBD</description>
    </item>
    
    <item>
      <title>KubeSphere 应用商店</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/app-store/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/app-store/</guid>
      <description>什么是 KubeSphere 应用商店 作为一个开源的、以应用为中心的容器平台，KubeSphere 在 OpenPitrix 的基础上，为用户提供了一个基于 Helm 的应用商店，用于应用生命周期管理，这是一个开源的基于网络的系统，用于打包、部署和管理不同类型的应用。KubeSphere 应用商店允许 ISV、开发者和用户在一站式服务中只需点击几下就可以上传、测试、部署和发布应用。
对内，KubeSphere 应用商店可以作为不同团队共享数据、中间件和办公应用的场所。对外，有利于制定行业标准的建设和交付。默认情况下，应用商店中有 15 个应用。启用该功能后，可以通过应用模板添加更多应用。
有关更多信息，请参阅应用商店。
在安装前启用应用商店 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用应用商店（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用应用商店。 在该文件中，搜寻到 openpitrix，并将 enabled 的 false 改为 true。完成后保存文件。  openpitrix: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 使用配置文件创建一个集群：  ./kk create cluster -f config-sample.yaml 在 Kubernetes 上安装 在 Kubernetes 上安装 KubeSphere 时，需要下载文件 cluster-configuration.</description>
    </item>
    
    <item>
      <title>KubeSphere DevOps 系统</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/devops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/devops/</guid>
      <description>什么是 KubeSphere DevOps 系统 KubeSphere DevOps 系统是专为 Kubernetes 中的 CI/CD 工作流设计的。基于 Jenkins，它提供了一站式的解决方案，帮助开发和运维团队以直接的方式构建、测试和发布应用到 Kubernetes。它还具有插件管理、二进制到图像（B2I）、源到图像（S2I）、代码依赖缓存、代码质量分析、流水线日志等功能。
DevOps 系统为用户提供了一个有利的环境，因为应用可以自动发布到同一个平台。它还兼容第三方私有镜像注册库（如 Harbor）和代码库（如 GitLab/GitHub/SVN/BitBucket）。因此，它通过为用户提供全面的、可视化的 CI/CD 管道来创造优秀的用户体验，这些管道在气垫环境中非常有用。
有关更多信息，请参阅 DevOps 管理。
在安装前启用 DevOps 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用 DevOps（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用 DevOps 系统。 在该文件中，搜寻到 devops，并将 enabled 的 false 改为 true。完成后保存文件。  devops: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 使用配置文件创建一个集群：  .</description>
    </item>
    
    <item>
      <title>KubeSphere 审计日志</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/auditing-logs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/auditing-logs/</guid>
      <description>什么是 KubeSphere 审计日志 KubeSphere 审计日志系统提供了一套与安全相关的按时间顺序排列的记录，记录了与单个用户、管理人员或系统其他组件相关的活动顺序。对 KubeSphere 的每个请求都会产生一个事件，然后写入 Webhook，并根据一定的规则进行处理。
有关更多信息，请参阅日志、事件和审计。
在安装前启用审计日志 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用审计日志（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用审计模式。 在该文件中，搜寻到 auditing，并将 enabled 的 false 改为 true。完成后保存文件。  auditing: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 备注
默认情况下，如果启用了审计功能，KubeKey 将在内部安装 Elasticsearch。对于生产环境，如果你想启用审计，强烈建议你在 config-sample.yaml 中设置以下值，尤其是 externalElasticsearchUrl 和 externalElasticsearchPort。一旦你在安装前提供以下信息，KubeKey 将直接整合你的外部 Elasticsearch，而不是安装一个内部 Elasticsearch。 es: # Storage backend for logging, tracing, events and auditing.</description>
    </item>
    
    <item>
      <title>KubeSphere 事件系统</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/events/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/events/</guid>
      <description>什么是 KubeSphere 事件系统 KubeSphere 事件系统允许用户跟踪集群内部发生的事情，如节点调度状态和镜像拉取结果。它们将被准确地记录下来，并在Web 控制台中显示具体的原因、状态和信息。要查询事件，用户可以快速启动 Web 工具箱，在搜索栏中输入相关信息，并有不同的过滤器（如关键字和项目）可供选择。事件也可以归档到第三方工具，如 Elasticsearch、Kafka 或 Fluentd。
有关更多信息，请参见日志记录、事件和审计系统。
在安装前启用事件系统 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用事件（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用事件系统。 在该文件中，搜寻到 events，并将 enabled 的 false 改为 true。完成后保存文件。  events: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 备注
默认情况下，如果启用了审计功能，KubeKey 将在内部安装 Elasticsearch。对于生产环境，如果你想启用事件，强烈建议你在 config-sample.yaml 中设置以下值，尤其是 externalElasticsearchUrl 和 externalElasticsearchPort。一旦你在安装前提供以下信息，KubeKey 将直接整合你的外部 Elasticsearch，而不是安装一个内部 Elasticsearch。 es: # Storage backend for logging, tracing, events and auditing.</description>
    </item>
    
    <item>
      <title>KubeSphere 日志系统</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/logging/</guid>
      <description>什么是 KubeSphere 日志系统 KubeSphere 为日志收集、查询和管理提供了一个强大的、整体的、易于使用的日志系统。它涵盖了不同层次的日志，包括租户、基础设施资源和应用。用户可以从项目、工作量、Pod 和关键字等不同维度对日志进行搜索。与 Kibana 相比，KubeSphere 基于租户的日志系统具有更好的隔离性和租户之间的安全性，因为每个租户只能查看自己的日志。除了 KubeSphere 自身的日志系统，容器平台还允许用户添加第三方日志收集器，如 Elasticsearch、Kafka 和 Fluentd。
有关更多信息，请参阅日志、事件和审计系统。
在安装前启用日志系统 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用日志系统（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用日志系统。 警告
如果你采用多节点安装，并且使用符号链接作为 Docker 根目录，请确保所有节点遵循完全相同的符号链接。日志代理在 DaemonSet 中部署到节点上。容器日志路径的任何差异都可能导致该节点的收集失败。 在该文件中，搜寻到 logging，并将 enabled 的 false 改为 true。完成后保存文件。  logging: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 备注</description>
    </item>
    
    <item>
      <title>日志系统</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/faq/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/faq/logging/</guid>
      <description>如何将日志存储改为外部弹性搜索并关闭内部弹性搜索 如果您使用的是 KubeSphere 内部的 Elasticsearch，并且想把它改成您的外部备用，请按照下面的指南操作。否则，如果你还没有启用日志系统，请到启用日志系统直接设置外部 Elasticsearch。
首先，更新 KubeKey 配置。
kubectl edit cc -n kubesphere-system ks-installer   将如下 es.elasticsearchDataXXX、es.elasticsearchMasterXXX 和 status.logging 的注释取消。
  将 es.externalElasticsearchUrl 设置为弹性搜索的地址，es.externalElasticsearchPort 设置为它的端口号。
  apiVersion: installer.kubesphere.io/v1alpha1 kind: ClusterConfiguration metadata: name: ks-installer namespace: kubesphere-system ... spec: ... common: es: # elasticsearchDataReplicas: 1 # elasticsearchDataVolumeSize: 20Gi # elasticsearchMasterReplicas: 1 # elasticsearchMasterVolumeSize: 4Gi elkPrefix: logstash logMaxAge: 7 externalElasticsearchUrl: &amp;lt;192.168.0.2&amp;gt; externalElasticsearchPort: &amp;lt;9200&amp;gt; ... status: ... # logging: # enabledTime: 2020-08-10T02:05:13UTC # status: enabled .</description>
    </item>
    
    <item>
      <title>KubeSphere 服务网格</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/service-mesh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/service-mesh/</guid>
      <description>什么是 KubeSphere 服务网格 在 Istio 的基础上，KubeSphere 服务网格将微服务治理和流量管理可视化。它拥有强大的工具包，包括断路、蓝绿部署、金丝雀发布、流量镜像、分布式跟踪、可观察性和流量控制。开发者无需任何代码黑客，即可轻松上手服务网格，Istio 的学习曲线大大降低。KubeSphere 服务网格的所有功能都是为了满足用户的业务需求。
更多信息请参见项目管理与使用中的相关章节。
在安装前启用服务网格 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用服务网格（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用服务网格。 在该文件中，搜寻到 servicemesh，并将 enabled 的 false 改为 true。完成后保存文件。  servicemesh: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 使用配置文件创建一个集群：  ./kk create cluster -f config-sample.yaml 在 Kubernetes 上安装 在 Kubernetes 上安装 KubeSphere 时，需要下载文件 cluster-configuration.</description>
    </item>
    
    <item>
      <title>监控系统</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/faq/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/faq/monitoring/</guid>
      <description>如何访问 KubeSphere Prometheus 控制台 KubeSphere 监控引擎由 Prometheus 提供支持。出于调试目的，您可能希望通过 NodePort 访问内置的 Prometheus 服务。要做到这一点，运行以下命令来编辑服务类型。
kubectl edit svc -n kubesphere-monitoring-system prometheus-k8s Node Exporter 引起的主机端口 9100 冲突 如果有进程占用主机端口 9100，kubespher-monitoring-system 下的 Node Exporter 会崩溃。为了解决冲突，你需要终止进程或将 Node Exporter 换到另一个可用端口。
要采用另一个主机端口，例如 29100，运行以下命令，将所有 9100 替换为 29100（需要更改 5 处）。
kubectl edit ds -n kubesphere-monitoring-system node-exporter apiVersion: apps/v1 kind: DaemonSet metadata: name: node-exporter namespace: kubesphere-monitoring-system ... spec: ... template: ... spec: containers: - name: node-exporter image: kubesphere/node-exporter:ks-v0.18.1 args: - --web.listen-address=127.0.0.1:9100 ... - name: kube-rbac-proxy image: kubesphere/kube-rbac-proxy:v0.</description>
    </item>
    
    <item>
      <title>KubeSphere 告警和通知系统</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/alerting-notification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/alerting-notification/</guid>
      <description>什么是 KubeSphere 告警和通知系统 告警和通知是可观察性的两个重要构件，与监控和日志密切相关。KubeSphere 中的告警系统与主动故障通知系统相结合，用户可以根据告警策略了解感兴趣的活动。当达到某个指标的预定义阈值时，会向预先配置的收件人发出警报，通知方式可以自行设置，包括 Email、企业微信和 Slack。有了高功能的预警和通知系统，您就可以在潜在的问题影响到您的业务之前，迅速发现并提前解决。
更多信息，请参见告警策略和消息。
备注
建议同时启用告警和通知功能，这样用户可以及时收到告警通知。 在安装前启用告警和通知系统 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用告警和通知（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用告警和通知系统。 在该文件中，搜寻到 alerting 和 notification，并将 enabled 的 false 改为 true。完成后保存文件。  alerting: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; notification: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 使用配置文件创建一个集群：  .</description>
    </item>
    
    <item>
      <title>网络策略</title>
      <link>https://cnimages.github.io/website/zh/docs/pluggable-components/network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/pluggable-components/network-policy/</guid>
      <description>什么是网络策略 从 v3.0.0 开始，用户可以在 KubeSphere 中配置原生 Kubernetes 的网络策略。网络策略是一种以应用为中心的构造，可以让你指定一个 Pod 如何被允许通过网络与各种网络实体进行通信。通过网络策略，用户可以在同一集群内实现网络隔离，这意味着可以在某些实例（Pod）之间设置防火墙。
备注
 在启用之前，请确保集群使用的 CNI 网络插件支持网络策略。支持网络策略的 CNI 网络插件有很多，包括 Calico、Cilium、Kube-router、Romana 和 Weave Net。 建议您在启用网络策略之前，使用 Calico 作为 CNI 插件。   更多信息请参见网络政策。
在安装前启用网络策略 在 Linux 上安装 当您在 Linux 上安装 KubeSphere 时，你需要创建一个配置文件，该文件列出了所有 KubeSphere 组件。
 基于在 Linux 上安装 KubeSphere 的教程，您创建了一个默认文件 config-sample.yaml。通过执行以下命令修改该文件：  vi config-sample.yaml 备注
如果采用 All-in-one 安装，则不需要创建 config-sample.yaml 文件，因为可以直接创建集群。一般来说，All-in-one 模式是为那些刚刚接触 KubeSphere 并希望熟悉系统的用户准备的。如果您想在这个模式下启用网络策略（比如出于测试的目的），可以参考下面的部分，看看安装后如何启用网络策略。 在该文件中，搜寻到 networkpolicy，并将 enabled 的 false 改为 true。完成后保存文件。  networkpolicy: enabled: true # Change &amp;#34;false&amp;#34; to &amp;#34;true&amp;#34; 使用配置文件创建一个集群：  .</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-overview/</guid>
      <description>Kubernetes KubeSphere v3.0.0 与 Kubernetes 1.15.x, 1.16.x, 1.17.x 以及 1.18.x 兼容：
  如果您的 KubeSphere v2.1.x 安装在 Kubernetes 1.15.x+ 上，您可选择只将 KubeSphere 升级到 v3.0.0 或者同时升级 Kubernetes（到高版本）和 KubeSphere (到 v3.0.0) 。
  如果您的 KubeSphere v2.1.x 安装在 Kubernetes 1.14.x上，您必须同时升级 Kubernetes (到 1.15.x+) 和 KubeSphere (到 v3.0.0) 。
  警告
与之前的1.14.x 和1.15.x 相比，Kubernetes 1.16.x 的 API 有一些重要的改动。更多详细信息，请参阅 Deprecated APIs Removed In 1.16: Here’s What You Need To Know。 因此，如果您打算将 Kubernetes 1.14.x/1.15.x 升级到 1.</description>
    </item>
    
    <item>
      <title>Upgrade with KubeKey</title>
      <link>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-with-kubekey/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-with-kubekey/</guid>
      <description>KubeKey is recommended for users whose KubeSphere and Kubernetes were both deployed by KubeSphere Installer. If your Kubernetes cluster was provisioned by yourself or cloud providers, please refer to Upgrade with ks-installer.
Prerequisites  You need to have a KubeSphere cluster running version 2.1.1.  警告
If your KubeSphere version is v2.1.0 or earlier, please upgrade to v2.1.1 first.  Download KubeKey.  Download KubeKey from GitHub Release Page or use the following command directly.</description>
    </item>
    
    <item>
      <title>Upgrade with ks-installer</title>
      <link>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-with-ks-installer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-with-ks-installer/</guid>
      <description>ks-installer is recommended for users whose Kubernetes clusters were not set up via KubeSphere Installer, but hosted by cloud vendors. This tutorial is for upgrading KubeSphere only. Cluster operators are responsible for upgrading Kubernetes themselves beforehand.
Prerequisites  You need to have a KubeSphere cluster running version 2.1.1.  警告
If your KubeSphere version is v2.1.0 or earlier, please upgrade to v2.1.1 first.  Make sure you read Release Notes For 3.</description>
    </item>
    
    <item>
      <title>Changes after Upgrade</title>
      <link>https://cnimages.github.io/website/zh/docs/upgrade/what-changed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/upgrade/what-changed/</guid>
      <description>This section covers the changes after upgrade for existing settings in previous versions. If you want to know all the new features and enhancements in KubeSphere 3.0.0, see Release Notes for 3.0.0 directly.
Access Control The definition of custom roles has been simplified. Some closely-related permission items have been aggregated into permission groups. Custom roles will not change during the upgrade and can be used directly after the upgrade if they conform to new policy rules for authorization assignment.</description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/upgrade/upgrade-faq/</guid>
      <description>How to upgrade Qingcloud CSI after upgrading? Currently Qingcloud CSI will not be upgraded by KubeKey. You can run the following command to upgrade CSI manually after finishing KubeSphere upgrade:
git clone https://github.com/yunify/qingcloud-csi.gitcd qingcloud-csi/git checkout v1.1.1kubectl delete -f deploy/disk/kubernetes/releases/qingcloud-csi-disk-v1.1.1.yamlkubectl delete sc csi-qingcloudhelm repo add test https://charts.kubesphere.io/testhelm install test/csi-qingcloud --name-template csi-qingcloud --namespace kube-system \--set config.qy_access_key_id=KEY,config.qy_secret_access_key=SECRET,config.zone=ZONE,sc.type=2Wait until csi controller and daemonset are running
$ kubectl get po -n kube-system | grep csicsi-qingcloud-controller-56979d46cb-qk9ck 5/5 Running 0 24hcsi-qingcloud-node-4s8n5 2/2 Running 0 24hcsi-qingcloud-node-65dqn 2/2 Running 0 24hcsi-qingcloud-node-khk49 2/2 Running 0 24hcsi-qingcloud-node-nz9q9 2/2 Running 0 24hcsi-qingcloud-node-pxr56 2/2 Running 0 24hcsi-qingcloud-node-whqhk 2/2 Running 0 24hThen run the following command to check csi image version is 1.</description>
    </item>
    
    <item>
      <title>Alerting Policy (Node Level)</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/alerting-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/alerting-policy/</guid>
      <description>Objective KubeSphere provides alert policies for nodes and workloads. This guide demonstrates how you can create alert policies for nodes in the cluster and configure mail notifications. See Alerting Policy (Workload Level) to learn how to configure alert policies for workloads.
Prerequisites  KubeSphere Alerting and Notification needs to be enabled. Mail Server needs to be configured.  Hands-on Lab Task 1: Create an Alert Policy   Log in the console with one account granted the role platform-admin.</description>
    </item>
    
    <item>
      <title>Alerting Message (Node Level)</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/alerting-message/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-wide-alerting-and-notification/alerting-message/</guid>
      <description>Alert messages record detailed information of alerts triggered based on alert rules, including monitoring targets, alert policies, recent notifications and comments.
Prerequisites You have created a node-level alert policy and received alert notifications of it. If it is not ready, please refer to Alert Policy (Node Level) to create one first.
Hands-on Lab Task 1: View Alert Message   Log in the console with one account granted the role platform-admin.</description>
    </item>
    
    <item>
      <title>邮件服务器</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/mail-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/cluster-settings/mail-server/</guid>
      <description>目标 本指南演示了告警策略的电子邮件通知设置（支持自定义设置）。 您可以指定用户电子邮件地址以接收告警消息。
前提条件 KubeSphere Alerting and Notification 需要启用。
动手实验室  使用具有  platform-admin 角色的一个帐户登录 Web 控制台。 点击左上角的平台管理，然后选择集群管理。   从列表中选择一个集群并输入它（如果您未启用多集群功能，则将直接转到概述页面）。 在群集设置下选择邮件服务器。 在页面中，提供您的邮件服务器配置和 SMTP 身份验证信息，如下所示：  SMTP 服务器地址：填写可以提供邮件服务的 SMTP 服务器地址。 端口通常是 25。 使用 SSL 安全连接：SSL 可用于加密邮件，从而提高了邮件传输信息的安全性。 通常，您必须为邮件服务器配置证书。 SMTP 验证信息：如下填写 SMTP 用户，SMTP 密码，发件人电子邮件地址等    完成上述设置后，单击保存。 您可以发送测试电子邮件以验证服务器配置是否成功。  </description>
    </item>
    
    <item>
      <title>Cluster Shutdown and Restart</title>
      <link>https://cnimages.github.io/website/zh/docs/cluster-administration/shut-down-and-restart-cluster-gracefully/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/cluster-administration/shut-down-and-restart-cluster-gracefully/</guid>
      <description>This document describes the process of gracefully shutting down your cluster and how to restart it. You might need to temporarily shut down your cluster for maintenance reasons.
警告
Shutting down a cluster is very dangerous. You must fully understand the operation and its consequences. Please make an etcd backup before you proceed. Usually, it is recommended to maintain your nodes one by one instead of restarting the whole cluster. Prerequisites  Take an etcd backup prior to shutting down a cluster.</description>
    </item>
    
    <item>
      <title>Questions about KubeSphere Console</title>
      <link>https://cnimages.github.io/website/zh/docs/faq/console-faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/faq/console-faq/</guid>
      <description>What kind of browsers does KubeSphere support?
In general, KubeSphere console supports some popular browsers including Chrome, Firefox, Safari, Opera, and Edge. You only need to consider the supported versions of these browsers listed in the green box of the table below:</description>
    </item>
    
    <item>
      <title>Telemetry in KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/faq/telemetry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/faq/telemetry/</guid>
      <description>Telemetry collects aggregate information about the size of KubeSphere clusters installed, KubeSphere and Kubernetes versions, components enabled, cluster running time, error logs, etc. KubeSphere promises that the information is only used by the KubeSphere community to improve products and will not be shared with any third parties.
What Information Is Collected  External network IP Download date Kubernetes version KubeSphere version Kubernetes cluster size The type of the operating system Installer error logs Components enabled The running time of Kubernetes clusters The running time of KubeSphere clusters Cluster ID Machine ID  Disable Telemetry Telemetry is enabled by default when you install KubeSphere, while you also have the option to disable it either before or after the installation.</description>
    </item>
    
    <item>
      <title>Anchnet</title>
      <link>https://cnimages.github.io/website/zh/case/anchnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/case/anchnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Aqara</title>
      <link>https://cnimages.github.io/website/zh/case/aqara/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/case/aqara/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benlai</title>
      <link>https://cnimages.github.io/website/zh/case/benlai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/case/benlai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>contribution request</title>
      <link>https://cnimages.github.io/website/zh/contribution/request/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/contribution/request/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Huaxia Bank</title>
      <link>https://cnimages.github.io/website/zh/case/huaxia-bank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/case/huaxia-bank/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KubeSphere 2.1.1 发布！全面支持 Kubernetes 1.17！多项功能与用户体验优化！</title>
      <link>https://cnimages.github.io/website/zh/blogs/release-210/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/release-210/</guid>
      <description>2.1.0 正式发布 2019 年 11 月 11 日，KubeSphere 开源社区激动地向大家宣布，KubeSphere 2.1.0 正式发布！2.1.0 版本不仅在安装上提供了最快速方便的安装方式，解耦了核心的功能组件并提供了可插拔的安装方式，还提供了非常多的让开源社区用户期待已久的新功能，并修复了已知的 Bug。
同时，社区对 KubeSphere 组件的高可用进行了深度优化与测试，因此，该版本也是被定义为 Prodcution-ready 的，支持用户在生产环境部署和使用。我们在此对社区用户提交的 issue、PR、Bug 反馈、需求建议、文档改进等一系列贡献表示由衷的感谢，并对 2.1.0 版本做出巨大贡献的开发者们深表谢意。
在新版本中，KubeSphere 对 安装部署、DevOps、应用商店、存储、可观察性、认证与权限 等模块提供了诸多新功能和深度优化，更好地帮助企业用户在测试生产环境快速落地云原生技术和运维 Kubernetes，使开发者能够更专注在业务本身，赋能运维和测试人员高效地管理集群资源，实现业务快速发布与持续迭代的需求。同时，功能组件的可插拔安装能够满足不同用户的个性化需求，下面先通过一张图来快速介绍 2.1.0 版本各功能模块的新功能与优化项。
应用商店 KubeSphere 是一个 以应用为中心 的容器平台，基于自研的开源项目 OpenPitrix (openpitrix.io) 构建了应用商店、内置应用仓库与应用生命周期管理，KubeSphere 应用商店 对内可作为团队间共享企业内部的中间件、大数据、业务应用等，以应用模板的形式方便用户快速地一键上传和部署应用到 Kubernetes 中；对外可作为根据行业特性构建行业交付标准、交付流程和交付路径的基础，作为行业通用的应用商店，可根据不同需求应对不同的业务场景。
在 2.1.0 版本中，KubeSphere 从业务视角实现了应用的生命周期管理，支持 Helm 应用的 上传提交、应用审核、测试部署、应用上架、应用分类、应用升级、应用下架，帮助开发者或 ISV 将应用共享和交付给普通用户。同时，应用商店内置了多个常用的 Helm 应用方便开发测试。未来将提供基于应⽤的监控指标、应⽤⽇志关键字段告警能⼒，以及计量计费等运营功能。
DevOps DevOps 是云原生时代在开发测试与持续交付场景下最核心的一环，KubeSphere 2.1.0 对 DevOps 系统进行了深度优化，流水线、S2I、B2I 提供了代码依赖缓存支持，使构建速度大幅提升。在 CI/CD 流水线集成了更多 Jenkins 插件和版本，优化了流水线 Agent 节点选择，新增了对 PV、PVC、Network Policy 的支持，并将这一系列优化成果贡献给了 Jenkins 社区。</description>
    </item>
    
    <item>
      <title>KubeSphere 3.0.0 GA：面向应用的容器混合云</title>
      <link>https://cnimages.github.io/website/zh/blogs/kubesphere-3.0.0-ga-announcement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/kubesphere-3.0.0-ga-announcement/</guid>
      <description>KubeSphere 3.0.0 GA：面向应用的容器混合云 2020 年 8 月 31 日，KubeSphere 开源社区官方宣布 KubeSphere 3.0.0 GA 版本正式发布！ KubeSphere 3.0.0 主打 “面向应用的容器混合云”，专为 多云、多集群、多团队、多租户 而设计，大幅增强了 集群管理、可观察性、存储管理、网络管理、多租户安全、应用商店、安装部署 等特性，并且进一步提升了交互设计与用户体验，KubeSphere 3.0.0 是 KubeSphere 至今为止最重要的版本更新。作为多云与多集群的统一控制平面，KubeSphere 3.0.0 带来的新功能将帮助企业加速落地 多云与混合云策略，降低企业对任何基础设施之上的 Kubernetes 集群运维管理的门槛，实现现代化应用在容器场景下的快速交付，为企业在生产环境构建云原生技术栈提供了 完整的平台级解决方案。
IDC 预测，到 2022 年有 70% 的企业会采用 Kubernetes 作为多云与混合云管理工具，到 2023 年将会有一半以上的企业级应用部署在容器化的混合云与多云环境，同时还会有超过 5 亿的数字化应用与服务将会以云原生的方式来开发与构建。
面对这一发展趋势，KubeSphere 研发总监周小四表示，KubeSphere 3.0.0 已经提前为 以混合云与云原生架构 为变革核心的技术演进提供了最易用的工具型解决方案，在未来还将提供以边缘计算、大数据与和 AI 为应用场景的容器平台。KubeSphere 设计理念始终以应用为中心，专注于用户目标，以极简的交互体验和最低的学习成本，把 Kubernetes 变成了看不见的水和电。
值得一提的是，与以往相比，3.0.0 版本得到了来自开源社区用户和青云QingCloud 之外的企业 多方支持和参与，无论是功能开发、功能测试、缺陷报告、需求建议、企业最佳实践，还是提供 Bug 修复、多云环境部署测试、国际化翻译、文档贡献，这些来自开源社区的贡献都为 3.0.0 的发布和推广提供了极大的帮助，我们将在文末予以特别致谢！
解读 3.0.0 重大更新 KubeSphere 3.0.0 在多集群管理中支持跨云的联邦部署和多集群的统一管理，帮助企业将应用一键分发到不同的公有云和私有化的基础设施之上。同时，3.0.0 打造了业界可观察性最丰富的容器平台，提供从集群层级到应用层级的监控、日志、告警通知、审计、事件，支持多维度与多租户查询，还支持了第三方应用的自定义监控，让开发者和集群管理员能够清晰掌握应用与集群的运行状况。存储与网络管理能力在 3.</description>
    </item>
    
    <item>
      <title>KubeSphere Api Documents</title>
      <link>https://cnimages.github.io/website/zh/api/crd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/api/crd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KubeSphere Api Documents</title>
      <link>https://cnimages.github.io/website/zh/api/kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/api/kubesphere/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KubeSphere 前端开源，社区架构首次公布</title>
      <link>https://cnimages.github.io/website/zh/blogs/console-opensource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/console-opensource/</guid>
      <description>Console 开源 从 KubeSphere 第一行代码至今，项目经历了一年多时间的迅速发展，开源社区也这个期间完成孵化，并初具规模。为了让 KubeSphere 项目能够更好地以开源社区的形式发展和演进，让社区开发者能够方便地参与到 KubeSphere 项目的建设，社区宣布将 KubeSphere 前端项目 console 开源，当前开源的版本包含了 KubeSphere 最新发布 v2.1 所有功能的代码，前端的 Feature Map 可以通过这张图快速了解。
Feature Map
前端项目的代码已在 github.com/kubesphere/console 可见，欢迎大家 Star + Fork。此前已有多位社区用户与开发者表示，希望能参与到 KubeSphere 项目的前端贡献，现在大家已经可以从 github.com/kubesphere/kubesphere/issues 通过标签 area/console 找到前端相关的 issue，包括 Bug、feature and design。
至此，KubeSphere 开源社区发布的项目已涵盖了容器平台（KubeSphere）、多云应用管理平台（OpenPitrix）、网络插件（Porter LB 插件、Hostnic-CNI）、存储插件（CSI）、CI/CD（S2i-operator）、日志插件（Fluentbit-operator）、通知告警（Alert &amp;amp; Notification）、身份认证（IAM）等。
KubeSphere 社区架构 KubeSphere 相信 Community over code，一个健康良好的开源社区发展必定离不开 Contributor 的参与。为了让社区相关的事情更加成体系，让社区同学更有归属感，KubeSphere 首次建立了社区架构，第一次公开的架构包括 Developer Group 和 User Group。
 SIG（特别兴趣小组）由开发者和用户共同组成，目前架构中暂未划分主题，未来将根据社区用户的参与和关注方向进行划分。
 Developer Group Developer Group 将以开发者对 KubeSphere 组织下的所有开源项目的贡献数量和质量作为参考，可贡献的项目包括前后端、存储与网络插件、官网文档等项目。
 Active Contributor：2 个月内贡献过超过 4 个 PR，这样即可获得邀请。 Reviewer： 从 Active Contributor 中诞生，当 Active Contributor 对该模块拥有比较深度的贡献，并且得到 2 个或 2 个以上 Maintainer 的提名时，将被邀请成为该模块的 Reviewer，具有 Review PR 的义务。 Maintainer：即该功能模块的组织者，负责项目某个功能模块的代码与版本开发与维护，社区日常运营，包括组织会议，解答疑问等。Tech Lead 需要为项目的管理和成长负责，责任重大。目前暂由 KubeSphere 内部成员担任，将来可根据贡献程度由社区开发者一起担任，共同为项目的进步而努力。  User Group KubeSphere 社区是由开发者和用户共建的，随着 KubeSphere 用户群体愈发壮大，用户在使用过程中遇到的问题反馈及实践经验，对于 KubeSphere 产品的完善及应用推广有着不可忽视的重要作用。</description>
    </item>
    
    <item>
      <title>KubeSphere 容器平台发布 2.1.1，全面支持 Kubernetes 1.17</title>
      <link>https://cnimages.github.io/website/zh/blogs/kubesphere-release-note-post/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/kubesphere-release-note-post/</guid>
      <description>农历二月二，KubeSphere 开源社区激动地向大家宣布，KubeSphere 容器平台 2.1.1 正式发布！
KubeSphere 作为 开源的企业级容器平台，对 2.1.1 版本定义的是 进一步增强生产可用性，修复了多个组件的 Bug，升级了内置的多个开源组件。借助 KubeSphere，您可以快速安装与管理原生的 Kubernetes，KubeSphere 2.1.1 已支持至 Kubernetes 1.17，帮助您上手 Kubernetes 新版本中新增的特性。并且，还向前兼容与支持 Kubernetes 1.17 之前的 3 个版本，您可以按需进行安装。
KubeSphere 能够帮助企业快速构建一个功能丰富的容器云平台。让企业在享受 Kubernetes 的弹性伸缩与敏捷部署的同时，还可以在容器平台拥有 IaaS 平台的存储与网络能力，获得与 IaaS 平台一样稳定的用户体验。比如，我们在 KubeSphere 2.1.1 新增了对阿里云与腾讯云块存储插件的集成，支持为 Pod 挂载公有云的存储，为有状态应用提供更稳定的持久化存储的能力。
除此之外，我们还将安装步骤再一次简化。2.1.1 简化了在已有 Kubernetes 上安装的步骤，无需再像 2.1.0 安装一样，配置集群 CA 证书路径。并且，也将 etcd 监控作为了可选安装项。真正实现了一条命令即可在已有的 Kubernetes 集群上快速安装 KubeSphere。
关于 2.1.1 的更新详情，请参考 Release Note。
下面演示两种最简单的安装方法，解锁如何最快尝鲜 KubeSphere 2.1.1。
如何在 Linux 快速安装 2.1.1  本文将演示 All-in-One 安装，请准备一台干净的机器（虚拟机或物理机），安装前关闭防火墙，并确保您的机器符合以下的最小要求：    机器配置:</description>
    </item>
    
    <item>
      <title>KubeSphere 部署 SkyWalking 至 Kubernetes 开启无侵入 APM</title>
      <link>https://cnimages.github.io/website/zh/blogs/skywalking-kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/skywalking-kubesphere/</guid>
      <description>Kubernetes 天然适合分布式的微服务应用。然而，当开发者们将应用从传统的架构迁移到 Kubernetes 以后，会发现分布式的应用依旧存在各种各样的问题，例如大量微服务间的调用关系复杂、系统耗时或瓶颈难以排查、服务异常定位困难等一系列应用性能管理问题，而 APM 正是实时监控并管理微服务应用性能的利器。
为什么需要 APM APM 无疑是在大规模的微服务开发与运维场景下是必不可少的一环，APM 需要主要从这三个角度去解决三大场景问题：
 测试角度：性能测试调优监控总览，包括容器总体资源状况（如 CPU、内存、IO）与链路总体状况 研发角度：链路服务的细节颗粒追踪，数据分析与数据安全 运维角度：跟踪请求的处理过程，来对系统在前后端处理、服务端调用的性能消耗进行跟踪，实时感知并追踪访问体验差的业务  为什么选择 Apache SkyWalking 社区拥有很丰富的 APM 解决方案，比如著名的 Pinpoint、Zipkin、SkyWalking、CAT 等。在经过一番调研后，KubeSphere 选择将 Apache SkyWalking 作为面向 Kubernetes 的 APM 开源解决方案，将 Apache SkyWalking 集成到了 KubeSphere，作为应用模板在 KubeSphere 容器平台 提供给用户一键部署至 Kubernetes 的能力，进一步增强在微服务应用维度的可观察性。
Apache SkyWalking 在 2019 年 4 月 17 正式成为 Apache 顶级项目，提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。Apache SkyWalking 专为微服务、云原生和基于容器的架构而设计。这是 KubeSphere 选择 Apache SkyWalking 的主要原因。
并且，Apache SkyWalking 本身还具有很多优势，包括多语言自动探针，比如 Java、.NET Core 和 Node.JS，能够实现无侵入式的探针接入 APM 检测，轻量高效，多种后端存储支持，提供链路拓扑与 Tracing 等优秀的可视化方案，模块化，提供 UI、存储、集群管理多种机制可选，并且支持告警。同时，Apache SkyWalking 还很容易与 SpringCloud 应用进行集成。</description>
    </item>
    
    <item>
      <title>KubeSphere 部署 TiDB 云原生分布式数据库</title>
      <link>https://cnimages.github.io/website/zh/blogs/tidb-on-kbesphere-using-qke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/tidb-on-kbesphere-using-qke/</guid>
      <description>TiDB 简介 TiDB 是 PingCAP 公司自主设计、研发的开源分布式关系型数据库，具备水平扩容或者缩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 5.7 协议和 MySQL 生态等重要特性。TiDB 适合高可用、强一致要求较高、数据规模较大等各种应用场景。
KubeSphere 简介 KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。
部署环境准备 KubeSphere 是由青云 QingCloud 开源的容器平台，支持在任何基础设施上安装部署。在青云公有云上支持一键部署 KubeSphere（QKE）。
下面以在青云云平台快速启用 KubeSphere 容器平台为例部署 TiDB 分布式数据库，至少需要准备 3 个可调度的 node 节点。你也可以在任何 Kubernetes 集群或 Linux 系统上安装 KubeSphere，可以参考 KubeSphere 官方文档。
 登录青云控制台：https://console.qingcloud.com/，点击左侧容器平台，选择 KubeSphere，点击创建并选择合适的集群规格：  创建完成后登录到 KubeSphere 平台界面：  点击下方的 Web Kubectl 集群客户端命令行工具，连接到 Kubectl 命令行界面。执行以下命令安装 TiDB Operator CRD：  kubectl apply -f https://raw.githubusercontent.com/pingcap/TiDB-Operator/v1.1.6/manifests/crd.yaml 执行后的返回结果如下：  点击左上角平台管理，选择访问控制，新建企业空间，这里命名为 dev-workspace  进入企业空间，选择应用仓库，添加一个 TiDB 的应用仓库：  将 PingCap 官方 Helm 仓库添加到 KubeSphere 容器平台，Helm 仓库地址如下：  https://charts.</description>
    </item>
    
    <item>
      <title>Maxnerva</title>
      <link>https://cnimages.github.io/website/zh/case/maxnerva/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/case/maxnerva/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NetApp 存储在 KubeSphere 上的实践</title>
      <link>https://cnimages.github.io/website/zh/blogs/netapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/netapp/</guid>
      <description>NetApp 是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。 Ontap数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。 Trident是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂持久性需求。 KubeSphere 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。
整体方案 在 VMware Workstation 环境下安装 ONTAP; ONTAP 系统上创建 SVM(Storage Virtual Machine) 且对接 nfs 协议；在已有 k8s 环境下部署 Trident,Trident 将使用 ONTAP 系统上提供的信息（svm、managementLIF 和 dataLIF）作为后端来提供卷；在已创建的 k8s 和StorageClass 卷下部署 KubeSphere。
版本信息  Ontap: 9.5 Trident: v19.07 k8s: 1.15 kubesphere: 2.0.2  步骤 主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。
OnTap 搭建及配置 在 VMware Workstation 上 Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide 运行，Ontap 启动之后，按下面操作配置，其中以 cluster base license、feature licenses for the non-ESX build 配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名: cluster1、密码等信息。</description>
    </item>
    
    <item>
      <title>OpenPitrix Insight</title>
      <link>https://cnimages.github.io/website/zh/blogs/openpitrix-insight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/openpitrix-insight/</guid>
      <description>云计算在今天已经被绝大多数的企业所采用，具知名云服务厂商 RightScale 最近的调查显示，已经有越来越多的厂商采用多云管理。客户有太多的理由来选择多云管理了，其中最大的原因莫过于采用单一的供应商，会导致被锁定。因此，如何管理多云环境，并在多云的环境下进行自动化，正成为众多企业的刚需，而在这其中，应用程序的管理显得尤为的重要。进一步讲，颇具挑战的是创建一个一站式的应用管理平台，来管理不同类型的应用程序，其中包括传统的应用（或者称之为单体应用，或者传统的主从、分片、peer-to-peer 架构的企业分布式应用）、微服务应用、以及近来发展迅猛的 Serverless 应用等，OpenPitrix 就是为了解决这些问题而生的。用一句话来描述 OpenPitrix：
 OpenPitrix 是一款开源项目，用来在多云环境下打包、部署和管理不同类型的应用，包括传统应用、微服务应用以及 Serverless 应用等，其中云平台包括 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等。
 微服务，即众所周知的微服务架构，这是程序设计的必然趋势，企业创建新的应用时选择的主要方式。另外，开源项目 Kubernetes 已经成为事实上的编排平台的领导者，其在自动化部署、扩展性、以及管理容器化的应用有着独特的优势。但是，仍然有大量的传统遗留应用用户想在毋须改变其架构的情况下迁入到云平台中，而且对很多用户来讲，采用微服务架构，或者是 Serverless 架构还是比较遥远的事情，所以，我们需要帮助这些用户将他们的传统应用迁入到云计算平台中，这也是 OpenPitrix 很重要的一个功能。
在2017年3月27日，QingCloud 发布 AppCenter，一款旨在为传统企业应用开发商和云用户之间架设友好桥梁的平台，该平台最大的亮点在于其可以让开发者以极低的学习成本就可以将传统的应用程序移植到 QingCloud 中运行，并且具有云计算的所有特性，如敏捷性、伸缩性、稳定性、监控等。通常，一位开发者只需花上几个小时就可以理解整个工作流程，然后，再花一到两周的时间(这具体要取决于应用的复杂性)将应用移植到云平台中。该平台上线之后一直颇受用户的青睐和夸赞，但有一些用户提出更多的需求，希望将之部署到他们内部来管理他们的多云环境。为了满足用户的需求，QingCloud 将之扩展，即在多云的环境下管理多种类型的应用程序，并且采用开源的方法来进行项目的良性发展。
俗语有云：&amp;ldquo;知易行难&amp;rdquo;，尽管 OpenPitrix 原始团队在云计算应用开发有着足够丰富的经验，并成功的开发出了稳定的商业化产品：AppCenter，要知道，等待在前方的依然有很多困难要克服。OpenPitrix 从一开始就是以开源的方式来进行，并且在2017年的8月份在 GitHub 上创建了组织和项目，一直到2018年2月24日才写下第一行功能代码，在此期间，团队的所有成员都在思考系统的每个关键点，这些讨论的细节均可在 GitHub 上公开访问。
以上便是 OpenPitrix 项目的来龙去脉介绍，接下来会解释一些详细的功能和设计细节。
主要的功能 OpenPitrix 所希望实现的功能包括以下内容：
 支持多个云平台，如 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等等; 云平台的支持是高度可扩展和插拔的; 支持多种应用程序的类型：传统应用、微服务应用、Serverless 应用; 应用程序的支持也是高度可扩展的，这也就意味着无论将来出现哪种新的应用程序类型，OpenPitrix 平台都可以通过添加相应的插件来支持它; 应用程序的仓库是可配置的，这也就意味着由 OpenPitrix 所驱动的商店，其应用均是可以用来交易的; 应用程序库的可见性是可配置的，包括公开、私有或仅让某特定的一组用户可访问，由 OpenPitrix 所驱动的市场，每个供应商都能够操作属于她/他自己的应用商店。  用户场景实例 OpenPitrix 典型的用户场景有：
 某企业是采用了多云的系统（包括混合云），要实现一站式的应用管理平台，从而实现应用的部署和管理； 云管平台（CMP）可以将 OpenPitrix 视为其其中一个组件，以实现在多云环境下管理应用； 可以作为 Kubernetes 的一个应用管理系统。OpenPitrix 和 Helm 有着本质上的不同，虽然 OpenPitrix 底层用了 Helm 来部署 Kubernetes 应用，但 OpenPitrix 着眼于应用的全生命周期管理，比如在企业中，通常会按照应用的状态来分类，如开发、测试、预览、生产等；甚至有些组织还会按照部门来归类，而这是 Helm 所没有的。  架构概览 OpenPitrix 设计的最根本的思想就是解耦应用和应用运行时环境（此处使用运行时环境代替云平台，下同），如下图所示。应用程序能够运行在哪个环境，除了需要匹配 provider 信息之外，还需要匹配应用所在仓库的选择器 (selector) 和运行时环境的标签 (label)，即当某个最终用户从商店里选择了某个具体的应用，然后尝试部署它时，系统会自动选择运行时环境。如果有多个运行时环境可以运行此应用的话，则系统会弹出相应的对话框来让用户自行选择，更多设计细节请参考 OpenPitrix 设计文档。</description>
    </item>
    
    <item>
      <title>partner request</title>
      <link>https://cnimages.github.io/website/zh/partner/request/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/partner/request/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Porter-面向裸金属环境的 Kubernetes 开源负载均衡器</title>
      <link>https://cnimages.github.io/website/zh/conferences/porter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/conferences/porter/</guid>
      <description>我们知道，在 Kubernetes 集群中可以使用 “LoadBalancer” 类型的服务将后端工作负载暴露在外部。云厂商通常为 Kubernetes 提供云上的 LB 插件，但这需要将集群部署在特定 IaaS 平台上。然而，许多企业用户通常都将 Kubernetes 集群部署在裸机上，尤其是用于生产环境时。而且对于本地裸机集群，Kubernetes 不提供 LB 实施。Porter 是一个专为裸金属 Kubernetes 集群环境而设计的开源的负载均衡器项目，可完美地解决此类问题。
Kubernetes 服务介绍 在 Kubernetes 集群中，网络是非常基础也非常重要的一部分。对于大规模的节点和容器来说，要保证网络的连通性、网络转发的高效，同时能做的 IP 和 Port 自动化分配和管理，并提供给用户非常直观和简单的方式来访问需要的应用，这是非常复杂且细致的设计。
Kubernetes 本身在这方面下了很大的功夫，它通过 CNI、Service、DNS、Ingress 等一系列概念，解决了服务发现、负载均衡的问题，也大大简化了用户的使用和配置。其中的 Service 是 Kubernetes 微服务的基础，Kubernetes 是通过 kube-proxy 这个组件来实现服务的。kube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，通过管理 iptables 来实现网络的转发。用户可以创建多种形式的 Service，比如基于 Label Selector 、Headless 或者 ExternalName 的 Service，kube-proxy 会为 Service 创建一个虚拟的 IP（即 Cluster IP），用于集群内部访问服务。
暴露服务的三种方式 如果需要从集群外部访问服务，即将服务暴露给用户使用，Kubernetes Service 本身提供了两种方式，一种是 NodePort，另外一种是 LoadBalancer。另外 Ingress 也是一种常用的暴露服务的方式。
NodePort 如果将服务的类型设置为 NodePort，kube-proxy 就会为这个服务申请一个 30000 以上的端口号（默认情况下），然后在集群所有主机上配置 IPtables 规则，这样用户就能通过集群中的任意节点加上这个分配的端口号访问服务了，如下图</description>
    </item>
    
    <item>
      <title>VNG</title>
      <link>https://cnimages.github.io/website/zh/case/vng/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/case/vng/</guid>
      <description></description>
    </item>
    
    <item>
      <title>一文说清 KubeSphere 容器平台的价值</title>
      <link>https://cnimages.github.io/website/zh/blogs/kubesphere-values/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/kubesphere-values/</guid>
      <description>KubeSphere 作为云原生家族 后起之秀，开源近两年的时间以来收获了诸多用户与开发者的认可。本文通过大白话从零诠释 KubeSphere 的定位与价值，以及不同团队为什么会选择 KubeSphere。
对于企业 KubeSphere 是什么 KubeSphere 是在 Kubernetes 之上构建的 多租户 容器平台，以应用为中心，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。使用 KubeSphere 不仅能够帮助企业在公有云或私有化数据中心快速搭建 Kubernetes 集群，还提供了一套功能丰富的向导式操作界面。
KubeSphere 能够帮助企业快速构建一个功能丰富的容器云平台，让企业在享受 Kubernetes 的弹性伸缩与敏捷部署的同时，还可以在容器平台拥有 IaaS 平台的存储与网络能力，获得与 IaaS 一样稳定的用户体验。比如在 KubeSphere 2.1.1 新增了对阿里云与腾讯云块存储插件的集成，支持为 Pod 挂载公有云的存储，为有状态应用提供更稳定的持久化存储的能力。
在日常的运维开发中，我们可能需要使用与管理大量的开源工具，频繁地在不同工具的 GUI 和 CLI 窗口操作，每一个工具的单独安装、使用与运维都会带来一定的学习成本，而 KubeSphere 容器平台能够统一纳管与对接这些工具，提供一致性的用户体验。这意味着，我们不需要再去多线程频繁地在各种开源组件的控制面板窗口和命令行终端切换，极大赋能企业中的开发和运维团队，提高生产效率。
对于开发者 KubeSphere 是什么 有很多用户习惯把 KubeSphere 定义为 “云原生全家桶”。不难理解，KubeSphere 就像是一个一揽子解决方案，我们设计了一套完整的管理界面，开发与运维在一个统一的平台中，可以非常方便地安装与管理用户最常用的云原生工具，从业务视角提供了一致的用户体验来降低复杂性。为了不影响底层 Kubernetes 本身的灵活性，也为了让用户能够按需安装，KubeSphere 所有功能组件都是可插拔的。
KubeSphere 基于 OpenPitrix 和 Helm 提供了应用商店，对内可作为团队间共享企业内部的中间件、大数据、APM 和业务应用等，方便开发者一键部署应用至 Kubernetes 中；对外可作为根据行业特性构建行业交付标准、交付流程和应用生命周期管理的基础，作为行业通用的应用商店，可根据不同需求应对不同的业务场景。在 3.0 版本还将支持计量 (Metering)，方便企业对应用与集群资源消耗的成本进行管理。
对于运维 KubeSphere 是什么 可观察性是容器云平台非常关键的一环，狭义上主要包含监控、日志和追踪等，广义上还包括告警、事件、审计等。对于 Kubernetes 运维人员来说，通常需要搭建和运维一整套可观察性的技术架构，例如 Prometheus + Grafana + AlertManager、EFK 等等。并且，企业通常还需要对不同租户能够看到的监控、日志、事件、审计等信息，实现按不同租户隔离，这些需求的引入无疑会增大企业的运维成本与复杂性。</description>
    </item>
    
    <item>
      <title>云原生可观察性之日志管理</title>
      <link>https://cnimages.github.io/website/zh/conferences/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/conferences/logging/</guid>
      <description>日志通常含有非常有价值的信息，日志管理是云原生可观察性的重要组成部分。不同于物理机或虚拟机，在容器与 Kubernetes 环境中，日志有标准的输出方式(stdout)，这使得进行平台级统一的日志收集、分析与管理水到渠成，并体现出日志数据独特的价值。本文将介绍云原生领域比较主流的日志管理方案 EFK 、 KubeSphere 团队开发的 FluentBit Operator 以及 KubeSphere 在多租户日志管理方面的实践。此外还将介绍受 Prometheus 启发专为 Kubernetes 日志管理开发，具有低成本可扩展等特性的开源软件 Loki。
什么是可观察性 近年来随着以 Kubernetes 为代表的云原生技术的崛起，可观察性 ( Observability ) 作为一种新的理念逐渐走入人们的视野。云原生基金会 ( CNCF ) 在其 Landscape 里已经将可观察性单独列为一个分类，狭义上主要包含监控、日志和追踪等，广义上还包括告警、事件、审计等。在此领域陆续涌现出了众多新兴开源软件如 Prometheus, Grafana, Fluentd, Loki, Jaeger 等。
日志作为可观察性的重要组成部分在开发、运维、测试、审计等过程中起着非常重要的作用。著名的应用开发十二要素中提到：“日志使得应用程序运行的动作变得透明，应用本身从不考虑存储自己的输出流。 不应该试图去写或者管理日志文件。每一个运行的进程都会直接输出到标准输出（stdout）。每个进程的输出流由运行环境截获，并将其他输出流整理在一起，然后一并发送给一个或多个最终的处理程序，用于查看或是长期存档。”
在物理机或者虚拟机的环境中，日志通常是输出到文件，并由用户自己管理，这使得日志的集中管理和分析变得困难和不便。而 Kubernetes 、docker 等容器技术直接将日志输出到 stdout，这使得日志的集中管理和分析变得更为便捷和水到渠成。
Kubernetes 官网文档给出的通用日志架构如下图所示，包含日志 Agent，后端服务和前端控制台等三个部分。无论是成熟的日志解决方案如 ELK/EFK , 还是云原生领域 2018 年开源的 Loki 都具有相似的架构，下面将分别介绍 ELK/EFK , Loki 以及 KubeSphere) 在这方面的贡献。
新旧势力的联姻：从 ELK 到 EFK，从 Fluentd 到 Fluent Bit ELK 是 Elasticsearch, Logstash, Kibana 的简称，是目前比较主流的开源日志解决方案。 而 2019 年 4 月从 CNCF 毕业用 C 和 Ruby 编写的 Fluentd 作为通用日志采集器，以其高效、灵活、易用的特性逐渐取代了用 Java 编写的 Logstash 成为新的日志解决方案 EFK 中的重要一员，并在云原生领域得到广泛认可与应用。Google 的云端日志服务 Stackdriver 也用修改后的 Fluentd 作为 Agent 。然而 Fluentd 开发团队并没有停滞不前，推出了更为轻量级的完全用 C 编写的产品 Fluent Bit，两者的对比如下图所示：</description>
    </item>
    
    <item>
      <title>使用 KubeSphere DevOps 搭建自动化测试系统</title>
      <link>https://cnimages.github.io/website/zh/blogs/devops-automatic-testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/devops-automatic-testing/</guid>
      <description>测试分层 测试的目的是为了验证预期的功能，发现潜在的缺陷。测试增强了交付合格产品的信心，也给敏捷迭代带来了可能。可以说，测试决定了产品的开发进度。
网络模型有七层的 OSI 、四层的 TCP，而开发模式有 MTV、MVC、MVP、MVVM 等。高内聚、低耦合，划分职责、分模块、分层。然后结构化、标准化，技术逐步走向成熟。
测试也分为，UI 测试、API 测试、单元测试。测试并不是一项新技术，更多是产出与成本的一种平衡。
如上图，是一个测试金字塔。越往上，需要的成本越高，对环境要求越高，执行时间越长，维护越麻烦，但更贴近终端用户的场景。在 《Google软件测试之道》中，按照谷歌的经验，各层测试用例比例是 70：20：10，也就是 70% 的单元测试，20% 的 API 测试，10% 的 UI 测试。
本篇主要讲的是如何在 KubeSphere 平台上使用 KubeSphere DevOps 系统 运行自动化测试。
什么是 KubeSphere DevOps KubeSphere 针对容器与 Kubernetes 的应用场景，基于 Jenkins 提供了一站式 DevOps 系统，包括丰富的 CI/CD 流水线构建与插件管理功能，还提供 Binary-to-Image（B2I）、Source-to-Image（S2I），为流水线、S2I、B2I 提供代码依赖缓存支持，以及代码质量管理与流水线日志等功能。
KubeSphere 内置的 DevOps 系统将应用的开发和自动发布与容器平台进行了很好的结合，还支持对接第三方的私有镜像仓库和代码仓库形成完善的私有场景下的 CI/CD，提供了端到端的用户体验。
但是，很少有用户知道，KubeSphere DevOps 还可以用来搭建自动化测试系统，为自动化的单元测试、API 测试和 UI 测试带来极大的便利性，提高测试人员的工作效率。
单元测试 单元测试的运行频率非常高，每次提交代码都应该触发一次。单元测试的依赖少，通常只需要一个容器运行环境即可。
下面是一个使用 golang:latest 跑单元测试的例子。
pipeline {agent {node {label &#39;go&#39;}}stages {stage(&#39;testing&#39;) {steps {container(&#39;go&#39;) {sh &#39;&#39;&#39;git clone https://github.</description>
    </item>
    
    <item>
      <title>使用 KubeSphere 在 Kubernetes 安装 cert-manager 为网站启用 HTTPS</title>
      <link>https://cnimages.github.io/website/zh/blogs/install-cert-managner-on-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/install-cert-managner-on-k8s/</guid>
      <description>什么是 cert-manager cert-manager（https://cert-manager.io/）是 Kubernetes 原生的证书管理控制器。它可以帮助从各种来源颁发证书，例如 Let&amp;rsquo;s Encrypt，HashiCorp Vault，Venafi，简单的签名密钥对或自签名。它将确保证书有效并且是最新的，并在证书到期前尝试在配置的时间续订证书。它大致基于 kube-lego 的原理，并从其他类似项目（例如 kube-cert-manager）中借鉴了一些智慧。
准备工作  需要一个公网可访问的 IP，例如 139.198.121.121 需要一个域名，并且已经解析到到对应的IP，例如  A kubesphere.io 139.198.121.121，我们将 staging.kubesphere.io 域名解析到了 139.198.121.121 在KubeSphere上已经运行网站对应的服务，例如本例中的ks-console  启用项目网关 登录 KubeSphere，进入任意一个企业空间下的项目中。
在 KubeSphere 中启用对应项目下的网关。
 我们开启的是一个NodePort类型的网关，需要在集群外部使用 LoadBalancer 转发到网关的端口，将 139.198.121.121 绑定到 LoadBalancer 上，这样我们就可以通过公网IP直接访问我们的服务了； 如果 Kubernetes 集群是在物理机上，可以安装 Porter（https://porter.kubesphere.io）负载均衡器对外暴露集群服务； 如果在公有云上，可以安装和配置公有云支持的负载均衡器插件，然后创建 LoadBalancer 类型的网关，填入公网IP对应的 eip，会自动创建好负载均衡器，并将端口转发到网关。
 安装 cert-manager 详细安装文档可以参考 cert-manager。
 cert-manager 部署时会创建一个 webhook 来校验 cert-manager 相关对象是否符合格式，不过也会增加部署的复杂性。这里我们使用官方提供的一个 no-webhook 版本安装。
 可以在 KubeSphere 右下角的工具箱中，打开 Web Kubectl。
在 Web Kubectl 执行下列命令安装 cert-manager：</description>
    </item>
    
    <item>
      <title>在 KubeSphere 安装 Orion vGPU 使用 TensorFlow 运行深度学习训练</title>
      <link>https://cnimages.github.io/website/zh/blogs/kubesphere-orion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/kubesphere-orion/</guid>
      <description>概览 本文将使用 KubeSphere 容器平台，在 Kubernetes 上部署 Orion vGPU 软件 进行深度学习加速，并基于 Orion vGPU 软件使用经典的 Jupyter Notebook 进行模型训练与推理。
在开始安装 Orion vGPU 和演示深度学习训练之前，先简单了解一下，什么是 vGPU 以及什么是 Orion vGPU。
什么是 vGPU vGPU 又称 虚拟 GPU，早在几年前就由 NVIDIA 推出了这个概念以及相关的产品。vGPU 是通过对数据中心（物理机）的 GPU 进行虚拟化，用户可在多个虚拟机或容器中 共享该数据中心的物理 GPU 资源，有效地提高性能并降低成本。vGPU 使得 GPU 与用户之间的关系不再是一对一，而是 一对多。
为什么需要 vGPU 随着 AI 技术的快速发展，越来越多的企业开始将 AI 技术应用到自身业务之中。目前，云端 AI 算力主要由三类 AI 加速器来提供：GPU，FPGA 和 AI ASIC 芯片。这些加速器的优点是性能非常高，缺点是 成本高昂，缺少异构加速管理和调度。大部分企业因无法构建高效的加速器资源池，而不得不独占式地使用这些昂贵的加速器资源，导致 资源利用率低，成本高。
以 GPU 为例，通过创新的 vGPU 虚拟化技术，能够帮助用户无需任务修改就能透明地共享和使用数据中心内任何服务器之上的 AI 加速器，不但能够帮助用户提高资源利用率，而且可以 极大便利 AI 应用的部署，构建数据中心级的 AI 加速器资源池。</description>
    </item>
    
    <item>
      <title>在 VMware vSphere 安装 KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/docs/installing-on-linux/on-premises/install-kubesphere-on-vmware-vsphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/docs/installing-on-linux/on-premises/install-kubesphere-on-vmware-vsphere/</guid>
      <description>在 VMware vSphere 部署高可用的 KubeSphere 对于生产环境，我们需要考虑集群的高可用性。如果关键组件（例如 kube-apiserver，kube-scheduler 和 kube-controller-manager）都在同一主节点上运行，则一旦主节点出现故障，Kubernetes 和 KubeSphere 将不可用。因此，我们需要通过为负载均衡器配置多个主节点来设置高可用性集群。您可以使用任何云负载平衡器或任何硬件负载平衡器（例如F5）。另外，Keepalived 和HAproxy 或 Nginx 也是创建高可用性集群的替代方法。
本教程为您提供了一个示例，说明如何使用 keepalived &amp;#43; haproxy 对 kube-apiserver 进行负载均衡，实现高可用 kubernetes 集群。
前提条件  请遵循该指南，确保您已经知道如何将 KubeSphere 与多节点集群一起安装。有关用于安装的 config yaml 文件的详细信息，请参阅多节点安装。本教程重点介绍如何配置负载均衡器。 您需要一个 VMware vSphere 帐户来创建VM资源。 考虑到数据的持久性，对于生产环境，我们建议您准备持久化存储。若搭建开发和测试，您可以直接使用默认集成的 OpenEBS 准备 LocalPV。  部署架构 创建主机 本示例创建 8 台 CentOS Linux release 7.6.1810（Core） 的虚拟机，默认的最小化安装，每台配置为 2 Core 4 GB 40 G 即可。
   主机 IP 主机名称 角色     10.</description>
    </item>
    
    <item>
      <title>基于 CSI Kubernetes 存储插件的开发实践</title>
      <link>https://cnimages.github.io/website/zh/conferences/csi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/conferences/csi/</guid>
      <description>现在很多用户都会将自己的应用迁移到 Kubernetes 容器平台中。在 Kubernetes 容器平台中，存储是支撑用户应用的基石。随着用户不断的将自己的应用深度部署在 K8S 容器平台中，但是我们现有的 Kubernetes 存储插件无论从多样性还是存储的功能来说，都无法满足用户日益增长的需求。我们急需开发新的存储插件，将我们的存储服务和 Kubernetes 容器平台相对接。
Kubernetes 存储插件分类 今天的主题是基于 CSI Kubernetes 存储插件的开发实践，我们会通过以下四部分为大家详细讲解 CSI 插件有什么功能，如何部署一个 CSI 插件，以及 CSI 的实践原理。
首先，我们会介绍 Kubernetes 存储插件的分类情况；然后为大家介绍如何开发一款 QingCloud 云平台 CSI 插件；之后，会介绍如何将 QingCloud 云平台 CSI 插件部署到 Kubernetes 容器平台中；最后，介绍如何对开发的 CSI 插件进行质量管理。
在 Kubernetes 容器平台中，Kubernetes 可以调用某类存储插件，对接后端存储服务，如调用 GCE 存储插件对接后端 GCE 存储服务。Kubernetes 里的存储插件可以分为 In-tree 和 Out-of-tree 这两大类。
首先，In-tree 存储插件的代码是在 Kubernetes 核心代码库里，In-tree 存储插件运行在 Kubernetes 核心组件里。Kubernetes 容器平台要使用后端某类存储服务，需要调用相应的 In-tree 存储插件，比如 Kubernetes 容器平台要使用后端 AWS 存储服务，需要调用 In-tree AWS 存储插件才能对接后端 AWS 存储服务。</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 的 CI/CD 利器 — Prow 入门指南</title>
      <link>https://cnimages.github.io/website/zh/blogs/prow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/prow/</guid>
      <description>Prow是k8s使用的CI/CD系统(https://github.com/kubernetes/test-infra/tree/master/prow)，用于管理k8s的issue和pr。如果你经常去k8s社区查看pr或者提交过一些Pr后，就会经常看到一个叫k8s-ci-bot的机器人在各个Pr中回复，并且还能合并pr。在k8s-ci-bot中背后工作的就是Prow。Prow是为了弥补github上一些功能上的缺陷，它也是Jenkins-X的一部分，它具备这些功能：
 执行各种Job，包括测试，批处理和制品发布等，能够基于github webhook配置job执行的时间和内容。 一个可插拔的机器人功能（Tide），能够接受/foo这种样式的指令。 自动合并Pr 自带一个网页，能够查看当前任务的执行情况以及Pr的状况，也包括一些帮助信息 基于OWNER文件在同一个repo里配置模块的负责人 能够同时处理很多repo的很多pr 能够导出Prometheus指标  Prow拥有自己的CI/CD系统，但是也能与我们常见的CI/CD一起协作，所以如果你已经习惯了Jenkins或者travis，都可以使用Prow。
安装指南  官方repo提供了一个基于GKE快速安装指南，本文将基于青云的Iaas搭建Prow环境。不用担心，其中大部分步骤都是平台无关的，整个安装过程能够很方便的在其他平台上使用。
 一、 准备一个kubernetes集群 有以下多种方式准备一个集群
 利用kubeadm自建集群 在青云控制台上点击左侧的容器平台，选择其中的QKE，简单设置一些参数之后，就可以很快创建一个kubernetes集群。 将集群的kubeconfig复制到本地，请确保在本地运行kubectl cluster-info正确无误  二、 准备一个github机器人账号  如果没有机器人账号，用个人账号也可以。机器人账号便于区分哪些Prow的行为，所以正式使用时应该用机器人账号。
   在想要用prow管理的仓库中将机器人账号设置为管理员。
  在账号设置中添加一个[personal access token][1]，此token需要有以下权限：
 必须：public_repo 和 repo:status 可选：repo假如需要用于一些私有repo 可选：admin_org:hook 如果想要用于一个组织    将此Token保存在文件中，比如${HOME}/secrets/oauth
  用openssl rand -hex 20生成一个随机字符串用于验证webhook。将此字符串保存在本地，比如${HOME}/secrets/h-mac
  注意最后两步创建的token一定需要保存好，除了需要上传到k8s，后续配置也要用到，用于双向验证
三、 配置k8s集群  这里使用的default命名空间配置prow，如果需要配置在其他命名空间，需要在相关kubectl的命令中配置-n参数，并且在部署的yaml中配置命名空间。 建议将本repo克隆到本地，这个repo带有很多帮助配置Prow的小工具。
  将上一步中创建token和hmac保存在k8s集群中  # openssl rand -hex 20 &amp;gt; ${HOME}/secrets/h-mac kubectl create secret generic hmac-token --from-file=hmac=${HOME}/secrets/h-mac kubectl create secret generic oauth-token --from-file=oauth=${HOME}/secrets/oauth 部署Prow。由于Prow官方yaml中使用了grc.</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 的 Serverless Jenkins —  Jenkins X</title>
      <link>https://cnimages.github.io/website/zh/conferences/jenkins-x/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/conferences/jenkins-x/</guid>
      <description>在云原生时代，应用模块不断被拆分，使得模块的数量不断上涨并且关系也越加复杂。企业在落地云原生技术的时候同事也需要有强大的 DevOps 手段，没有 DevOps 的云原生不可能是成功的。Jenkins X 是 CDF（持续交付基金会）与 Jenkins 社区在云原生时代的 DevOps产品，本文我们将介绍 Jenkins X 以及 Jenkins X 背后的技术。
背景 Jenkins 在2004年诞生。根据官网的数据统计（截止2019年3月）有 250,000 的 Jenkins 服务器正在运行、15,000,000+ Jenkins 用户、1000+ Jenkins插件。Jenkins 在 DevOps 领域取得了巨大的成功，但随着技术的不断发展与用户数量的不断上升，传统 Jenkins 所暴露出来的问题也越来越多。在这里我们将介绍传统 Jenkins 所遇到的挑战。
Jenkins 所遇到的挑战 - 单点故障 在传统的 Jenkins 当中，我们首先会遇到的问题就是 Jenkins 的单点故障问题。 Jenkins 的历史非常悠久，在当时大多数程序都是单机程序，Jenkins也不例外。 对比其他系统，Jenkins 的单点故障问题会更加凸显，熟悉 Jenkins 的用户都知道，它是一个基于插件的系统，而我们会经常安装插件，这时候我们就需要重启 Jenkins 服务器。这将导致共用这个平台的所有用户都无法使用。
Jenkins 所遇到的挑战 - JVM消耗资源多 Jenkins 是 Java 系的程序，这使得 Jenkins 需要使用 JVM，而 JVM 将会消耗大量的内存。 CI/CD 任务往往都是在代码提交时被触发，在非工作时间，这些资源消耗是可以大大降低的。
Jenkins 所遇到的挑战 - Job 的调度方式使 CI/CD 变得困难 在 Jenkins 诞生的年代，机器资源并没有像现在一样丰富、可调度，导致 Jenkins 的调度模式使得不适合现代的环境。 Jenkins 的调度模式是一种尽量能够节省资源的方式进行调度的。在一般的调度过程中 Jenkins 需要经历以下几个阶段: 检查有没有可用的 agent -&amp;gt; 如果没有的可用的agent，计算是否有 agent 预计将要运行完任务 -&amp;gt; 等待一段时间-&amp;gt; 启动动态的 agent -&amp;gt; agent 与 master建立连接。 这种方式使 CI/CD 任务被执行的太慢，我们往往都需要等待几十秒甚至更长时间来准备 CI/CD的执行环境。</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 部署 node.js APP</title>
      <link>https://cnimages.github.io/website/zh/blogs/nodejs-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/nodejs-app/</guid>
      <description>什么是 Kubernetes Kubernetes 是一个开源容器编排引擎，可以帮助开发者或运维人员部署和管理容器化的应用，能够轻松完成日常开发运维过程中诸如 滚动更新，横向自动扩容，服务发现，负载均衡等需求。了解更多
安装 Kubernetes 可以通过快速安装 kubernetes 集群：
 KubeSphere Installer minikube kubeadm  Kubernetes 术语介绍 Pod Pod 是 Kubernetes 最小调度单位，是一个或一组容器的集合。
Deployment 提供对 Pod 的声明式副本控制。指定 Pod 模版，Pod 副本数量, 更新策略等。
Service Service 定义了 Pod 的逻辑分组和一种可以访问它们的策略。借助Service，应用可以方便的实现服务发现与负载均衡。
Label &amp;amp;amp; Selector Kubernetes 中使用 Label 去关联各个资源。
 通过资源对象(Deployment, etc.)上定义的 Label Selector 来筛选 Pod 数量。 通过 Service 的 Label Selector 来选择对应的 Pod， 自动建立起每个 Service 到对应 Pod 的请求转发路由表。 通过对某些 Node 定义特定的 Label，并且在 Pod 中添加 NodeSelector 属性，可以实现 Pod 的定向调度(运行在哪些节点上)。  Nodejs 模板项目 node-express-realworld-example-app 是一款 node.</description>
    </item>
    
    <item>
      <title>基于 KubeSphere 的 Spring Could 微服务 CI/CD 实践</title>
      <link>https://cnimages.github.io/website/zh/blogs/spring-cloud-on-kubeshpere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/spring-cloud-on-kubeshpere/</guid>
      <description>本文以 Pig 为例介绍如何在 KubeSphere 上发布一个基于 Spring Cloud 微服务的 CI/CD 项目。
背景简介 Pig Pig (http://pig4cloud.cn/) 是一个基于 Spring Cloud 的开源微服务开发平台，也是微服务最佳实践。在国内拥有大量拥护者。同时也有商业版本提供技术支持。
KubeSphere KubeSphere (https://Kubesphere.io) 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。
通过 KubeSphere 我们可以以简洁的方式将 Pig 项目部署至 Kubernetes 中。运维人员可以轻松的完成 Spring Cloud 运维任务。
前提条件 具备 Spring Cloud 及 Pig 基础知识
Jenkins 基础知识（非必备）
KubeSphere 3.0 集群环境一套，并启用 DevOps 插件
 搭建 KubeSphere 集群不再本文覆盖范围，根据您的环境参考相关部署文档: https://KubeSphere.com.cn/docs/installing-on-kubernetes/
 架构设计 Spring Cloud 有一个丰富、完备的插件体系，以实现各种运行时概念，作为应用栈的一部分。因此，这些微服务自身有库和运行时代理，来做客户端的服务发现，配置管理，负载均衡，熔断，监控，服务跟踪等功能。由于本篇重点在于如何快速建立 CI/CD 运维体系，因此对 Spring Cloud 与 Kubernetes 的深度整合不做过多讨论。我们将继续使用 Spring Cloud 底层的这些能力，同时利用 Kubernetes 实现滚动升级，健康检查，服务自动恢复等缺失的功能。</description>
    </item>
    
    <item>
      <title>如何创建跨 Kubernetes 集群的流水线</title>
      <link>https://cnimages.github.io/website/zh/blogs/create-pipeline-across-multi-clusters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/create-pipeline-across-multi-clusters/</guid>
      <description>随着 Kubernetes 被广泛的作为基础设施，各大云厂商都相继推出了各自的 Kubernetes 集群服务。那么在多个集群上，如何跨集群实践 DevOps 流水线呢？本文将主要以示例的形式给出回答。
 提示，本文需要对 KubeSphere 和 DevOps 相关的知识具有一定了解，具体包括 Kubernetes 资源创建、生成 Sonarqube Token、获取集群 kubeconfig 等。
 KubeSphere DevOps 跨集群架构概览 集群角色说明 在多集群架构中，我们对集群进行了角色定义。每个集群的角色，在安装的配置文件中指定。具体详情，可以参考文档: kubefed-in-kubesphere 。一共有三种角色，host、member、none 。
 host  安装完整的 KubeSphere 核心组件，通过前端页面可以对各个集群进行管理。
 member  没有安装 KubeSpher Console 前端组件，不提供独立的页面管理入口，可以通过 host 集群的前端入口进行管理。
 none  没有定义角色，主要是为了兼容单集群模式。
多集群下 DevOps 的部署架构 上图，是 KubeSphere DevOps 在多集群上的部署架构。在每一个集群上，我们都可以选择性安装 DevOps 组件。具体安装步骤，与单集群开启 DevOps 组件没有区别。
在创建 DevOps 工程时，前端会对可选集群进行标记，仅开启 DevOps 组件的集群能够被选择。
用户场景描述 上图是一个 Demo 场景，通过多集群隔离不同的部署环境。一共有三个集群，开发集群、测试集群、生成集群。
开发人员在提交代码之后，可以触发流水线执行，依次完成，单元测试、代码检测、构建和推送镜像，然后直接部署到开发集群。开发集群交给开发人员自主管控，作为他们的自测验证环境。经过审批之后，可以发布到测试环境，进行更严格的验证。最后，经过授权之后，发布到正式环境，用于对外提供服务。
创建一条跨集群的流水线 准备集群 这里准备了三个集群，分别为：</description>
    </item>
    
    <item>
      <title>微服务进阶之路 容器落地避坑指南</title>
      <link>https://cnimages.github.io/website/zh/blogs/microservice-blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/microservice-blog/</guid>
      <description>微服务架构相对于单体架构有很大的变化，也产生了一些新的设计模式，比如 sidecar，如何开发一个微服务应用是一件有很大挑战性的事情，我们经常会听到有人讨论如何划分微服务，多细的颗粒度才是微服务等问题。初学者经常会处于一个“忐忑不安”的状态，所以我们急需要知道如何才能走上正确的微服务道路，或者需要一些最佳实践指导我们如何设计、开发一个微服务应用。
不骄不躁不跟风 知己知彼方可百战不殆 虽然现在已经进入到一个不谈微服务就落伍的时代，但作为 IT 从业者，我们一定要站在切身利益出发，多思考几个“为什么”，不要急于跟风。原因很简单，不管外面如何风吹雨打，只要你的房子足够结实、安全、舒服，那一般情况下就不需要拆除重建，所以在决定继续沿用单体架构还是转向微服务架构之前，我们一定要做两件事情：
第一件事，从外部了解两种架构各自的优劣： 可以看到，单体应用并不是一无是处。
第二件事，审视我们自己的业务：  上述单体架构列出的一些问题是否已经严重影响了我们的业务？ 企业新的业务系统是否要满足快速迭代、弹性等需求？ 团队内是否有 DevOps 氛围？ 企业内是否有足够的动力和技术储备去接触新的技术？  了解了单体应用和微服务应用的优劣特点，分析了企业自身的业务诉求和实际情况，最终还是决定转型微服务架构，那么我们也要清楚这不是一朝一夕的事情，需要分阶段逐步推进。
蒙眼狂奔不可取 循序渐进方可顺利进阶 第一阶段试炼—— 开发新应用 对于初次接触微服务的企业，选择新应用入手是正确的方式。
第一步可以选择 web-scale、无状态类型的新应用上手，比如基于 nginx 的网站、文档等，这类应用非常简单且容易实现，而且能体验到微服务在容器平台上的各种功能。
有了一定的经验之后，第二步就可以开发有状态类型的新应用，有状态服务的最大挑战就是数据管理。
敲重点，跟以往单体应用的共享数据库不同，微服务应用中的每一个服务“独享”自己的数据库，服务之间需要通过 API、事件或消息传递的方式来相互访问对方的数据，而不是通过直接访问对方数据库的方式。
换句话说，理想中的微服务是封装自己的数据，通过API暴露数据出去，从而避免数据耦合，这样每个微服务的数据格式发生变化也不影响其它微服务的数据调用。开发过和升级过大型企业单体应用的人对此会深有体会，一旦有人改变了数据库 schema，整个应用都有可能启动不起来，团队开发效率会大大降低。
微服务架构并不尽善尽美，适合自己的方案才是王道。
不难理解，微服务数据是牺牲强一致性而通过最终一致性的方式来管理，这对数据的划分带来很大难度，比如不能再用 join 的方式访问不同服务之间的数据表，实际当中也比较难做到或者做起来很麻烦，现在也没有成熟且好用的库或框架提供微服务的数据管理，而且某些应用确实需要强一致性。
而此时，我们不能通盘否定此类应用微服务化的可行性，应该适当折中或“妥协”，采用 miniservice。
Miniservice 在开发与部署的独立性和敏捷性方面类似于微服务(microservice)，但没有微服务那么强的约束。通常情况下，一个 miniservcie 可以提供多个功能，这些功能之间可以共享数据库。这个时候千万不要害怕混合架构，不要害怕自己的微服务应用是否“正统”，“think big，start small，move fast“才是我们应该遵循的哲学。
因此，一个企业应用里既有 microservice 也有 miniservice，甚至有单体部分（可以称之为 macroservice）都是可以接受的。
以一个电商平台举例，在整个场景里面，业务开发人员面对的主要压力来自前端频繁的变动，因为要应对频繁的促销、推广、降价等活动，所以面对消费者最前端的业务需要快速迭代。消费者会不停的浏览商品，最终产生交易的请求数量要远低于获取商品信息的请求数量，因此将前端业务无状态化，进行微服务拆分、解耦，便可以快速应对市场变化，灵活做出改变。
那是不是把整个平台都做到微服务级别会变得更好？答案是“不确定”，因为当微服务量级到达一定程度，由此产生的管理和运维压力是指数级增长的。而实际上，对于有些业务来讲也没有必要微服务化，比如很多电商平台都有 2B 的业务，其业务变化的频度和压力没有 2C 那么大，那以 macroservices 或者 miniservices 的方式去交付也是可以的。
开发人员应该分析在整个应用架构体系中，哪些适合微服务化，哪些亟需微服务化。
实践出真知 在上面的电商案例中，我们提到了服务无状态化，之所以期望服务无状态化，是因为无状态应用可以做到快速的扩缩容，可以应对井喷流量，可以最大效率的利用计算资源。
我们经常听到，以无状态为荣，以有状态为耻，说的就是对于一个服务要尽量无状态化它，比如用户 session 管理，以前我们在业务逻辑模块进行管理，导致这些模块不能按照无状态方式任意伸缩。我们可以把这些 session 的管理抽取出来放到一个高可用或分布式的缓存中管理，业务模块通过调用API的方式去获取 session，这样就实现了这些模块的无状态化。</description>
    </item>
    
    <item>
      <title>手把手从零部署与运营生产级的 Kubernetes 集群与 KubeSphere</title>
      <link>https://cnimages.github.io/website/zh/blogs/kubernetes-kubesphere-ha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/kubernetes-kubesphere-ha/</guid>
      <description>本文来自 KubeSphere 社区用户 Liu_wt 投稿，欢迎所有社区用户参与投稿或分享经验案例。
 本文将从零开始，在干净的机器上安装 Docker、Kubernetes (使用 kubeadm)、Calico、Helm 与 KubeSphere，通过手把手的教程演示如何搭建一个高可用生产级的 Kubernetes，并在 Kubernetes 集群之上安装 KubeSphere 容器平台可视化运营集群环境。
一、准备环境 开始部署之前，请先确定当前满足如下条件，本次集群搭建，所有机器处于同一内网网段，并且可以互相通信。
⚠️⚠️⚠️：请详细阅读第一部分，后面的所有操作都是基于这个环境的，为了避免后面部署集群出现各种各样的问题，强烈建议你完全满足第一部分的环境要求
  两台以上主机 每台主机的主机名、Mac 地址、UUID 不相同 CentOS 7（本文用 7.6/7.7） 每台机器最好有 2G 内存或以上 Control-plane/Master至少 2U 或以上 各个主机之间网络相通 禁用交换分区 禁用 SELINUX 关闭防火墙（我自己的选择，你也可以设置相关防火墙规则） Control-plane/Master和Worker节点分别开放如下端口   Master节点
   协议 方向 端口范围 作用 使用者     TCP 入站 6443* Kubernetes API 服务器 所有组件   TCP 入站 2379-2380 etcd server client API kube-apiserver, etcd   TCP 入站 10250 Kubelet API kubelet 自身、控制平面组件   TCP 入站 10251 kube-scheduler kube-scheduler 自身   TCP 入站 10252 kube-controller-manager kube-controller-manager 自身    Worker节点</description>
    </item>
    
    <item>
      <title>本来生活的 DevOps 升级之路</title>
      <link>https://cnimages.github.io/website/zh/blogs/benlai-devops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cnimages.github.io/website/zh/blogs/benlai-devops/</guid>
      <description>我叫杨杨，就职于本来生活网（Benlai.com），负责发布系统架构。我们公司咋说呢，简单说就是卖水果、蔬菜的😄，下面还是来一段官方介绍。
本来生活简介 本来生活网创办于 2012 年，是一个专注于食品、水果、蔬菜的电商网站，从优质食品供应基地、供应商中精挑细选，剔除中间环节，提供冷链配送、食材食品直送到家服务。致力于通过保障食品安全、提供冷链宅配、基地直送来改善中国食品安全现状，成为中国优质食品提供者。
技术现状 基础设施   部署在 IDC 机房 拥有 100 多台物理机 虚拟化部署   存在的问题   物理机 95% 以上的占用率 相当多的资源闲置 应用扩容比较慢   拥抱 DevOps 与 Kubernetes 公司走上容器平台的 DevOps 这条康庄大道主要目标有三：
 1、提高资源利用率
2、提高发布效率
3、降低运维的工作成本等等
 其实最主要的还是 省钱，对就是 省钱。接下来就是介绍我们本来生活的 DevOps 升级之路：
Level 1：工具选型 我们从初步接触 DevOps 相关知识，在此期间偶然了解到开源的 KubeSphere (kubesphere.io)。KubeSphere 是在 Kubernetes 之上构建的以应用为中心的企业级容器平台，支持敏捷开发与自动化运维、DevOps、微服务治理、灰度发布、多租户管理、监控告警、日志查询与收集、应用商店、存储管理、网络管理等多种业务场景。
KubeSphere 内置的基于 Jenkins 的 DevOps 流水线非常适合我们，并且还打通了我们日常运维开发中需要的云原生工具生态，这个平台正是我们当初希望自己开发实现的。
于是，我们开始学习 KubeSphere 与 Jenkins 的各种操作、语法、插件等，开始构建适合我们自己的 CI/CD 的整个流程。最终结合 KubeSphere 容器平台，初步实现了第一级的 CI/CD 流程。</description>
    </item>
    
  </channel>
</rss>