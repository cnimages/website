<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术博客 on </title>
    <link>https://kubesphere-v3.netlify.app/zh/blogs/</link>
    <description>Recent content in 技术博客 on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    
	<atom:link href="https://kubesphere-v3.netlify.app/zh/blogs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>KubeSphere 2.1.1 发布！全面支持 Kubernetes 1.17！多项功能与用户体验优化！</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/release-210/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/release-210/</guid>
      <description>2.1.0 正式发布 2019 年 11 月 11 日，KubeSphere 开源社区激动地向大家宣布，KubeSphere 2.1.0 正式发布！2.1.0 版本不仅在安装上提供了最快速方便的安装方式，解耦了核心的功能组件并提供了可插拔的安装方式，还提供了非常多的让开源社区用户期待已久的新功能，并修复了已知的 Bug。
同时，社区对 KubeSphere 组件的高可用进行了深度优化与测试，因此，该版本也是被定义为 Prodcution-ready 的，支持用户在生产环境部署和使用。我们在此对社区用户提交的 issue、PR、Bug 反馈、需求建议、文档改进等一系列贡献表示由衷的感谢，并对 2.1.0 版本做出巨大贡献的开发者们深表谢意。
在新版本中，KubeSphere 对 安装部署、DevOps、应用商店、存储、可观察性、认证与权限 等模块提供了诸多新功能和深度优化，更好地帮助企业用户在测试生产环境快速落地云原生技术和运维 Kubernetes，使开发者能够更专注在业务本身，赋能运维和测试人员高效地管理集群资源，实现业务快速发布与持续迭代的需求。同时，功能组件的可插拔安装能够满足不同用户的个性化需求，下面先通过一张图来快速介绍 2.1.0 版本各功能模块的新功能与优化项。
应用商店 KubeSphere 是一个 以应用为中心 的容器平台，基于自研的开源项目 OpenPitrix (openpitrix.io) 构建了应用商店、内置应用仓库与应用生命周期管理，KubeSphere 应用商店 对内可作为团队间共享企业内部的中间件、大数据、业务应用等，以应用模板的形式方便用户快速地一键上传和部署应用到 Kubernetes 中；对外可作为根据行业特性构建行业交付标准、交付流程和交付路径的基础，作为行业通用的应用商店，可根据不同需求应对不同的业务场景。
在 2.1.0 版本中，KubeSphere 从业务视角实现了应用的生命周期管理，支持 Helm 应用的 上传提交、应用审核、测试部署、应用上架、应用分类、应用升级、应用下架，帮助开发者或 ISV 将应用共享和交付给普通用户。同时，应用商店内置了多个常用的 Helm 应用方便开发测试。未来将提供基于应⽤的监控指标、应⽤⽇志关键字段告警能⼒，以及计量计费等运营功能。
DevOps DevOps 是云原生时代在开发测试与持续交付场景下最核心的一环，KubeSphere 2.1.0 对 DevOps 系统进行了深度优化，流水线、S2I、B2I 提供了代码依赖缓存支持，使构建速度大幅提升。在 CI/CD 流水线集成了更多 Jenkins 插件和版本，优化了流水线 Agent 节点选择，新增了对 PV、PVC、Network Policy 的支持，并将这一系列优化成果贡献给了 Jenkins 社区。</description>
    </item>
    
    <item>
      <title>KubeSphere 3.0.0 GA：面向应用的容器混合云</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-3.0.0-ga-announcement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-3.0.0-ga-announcement/</guid>
      <description>KubeSphere 3.0.0 GA：面向应用的容器混合云 2020 年 8 月 31 日，KubeSphere 开源社区官方宣布 KubeSphere 3.0.0 GA 版本正式发布！ KubeSphere 3.0.0 主打 “面向应用的容器混合云”，专为 多云、多集群、多团队、多租户 而设计，大幅增强了 集群管理、可观察性、存储管理、网络管理、多租户安全、应用商店、安装部署 等特性，并且进一步提升了交互设计与用户体验，KubeSphere 3.0.0 是 KubeSphere 至今为止最重要的版本更新。作为多云与多集群的统一控制平面，KubeSphere 3.0.0 带来的新功能将帮助企业加速落地 多云与混合云策略，降低企业对任何基础设施之上的 Kubernetes 集群运维管理的门槛，实现现代化应用在容器场景下的快速交付，为企业在生产环境构建云原生技术栈提供了 完整的平台级解决方案。
IDC 预测，到 2022 年有 70% 的企业会采用 Kubernetes 作为多云与混合云管理工具，到 2023 年将会有一半以上的企业级应用部署在容器化的混合云与多云环境，同时还会有超过 5 亿的数字化应用与服务将会以云原生的方式来开发与构建。
面对这一发展趋势，KubeSphere 研发总监周小四表示，KubeSphere 3.0.0 已经提前为 以混合云与云原生架构 为变革核心的技术演进提供了最易用的工具型解决方案，在未来还将提供以边缘计算、大数据与和 AI 为应用场景的容器平台。KubeSphere 设计理念始终以应用为中心，专注于用户目标，以极简的交互体验和最低的学习成本，把 Kubernetes 变成了看不见的水和电。
值得一提的是，与以往相比，3.0.0 版本得到了来自开源社区用户和青云QingCloud 之外的企业 多方支持和参与，无论是功能开发、功能测试、缺陷报告、需求建议、企业最佳实践，还是提供 Bug 修复、多云环境部署测试、国际化翻译、文档贡献，这些来自开源社区的贡献都为 3.0.0 的发布和推广提供了极大的帮助，我们将在文末予以特别致谢！
解读 3.0.0 重大更新 KubeSphere 3.0.0 在多集群管理中支持跨云的联邦部署和多集群的统一管理，帮助企业将应用一键分发到不同的公有云和私有化的基础设施之上。同时，3.0.0 打造了业界可观察性最丰富的容器平台，提供从集群层级到应用层级的监控、日志、告警通知、审计、事件，支持多维度与多租户查询，还支持了第三方应用的自定义监控，让开发者和集群管理员能够清晰掌握应用与集群的运行状况。存储与网络管理能力在 3.</description>
    </item>
    
    <item>
      <title>KubeSphere 前端开源，社区架构首次公布</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/console-opensource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/console-opensource/</guid>
      <description>Console 开源 从 KubeSphere 第一行代码至今，项目经历了一年多时间的迅速发展，开源社区也这个期间完成孵化，并初具规模。为了让 KubeSphere 项目能够更好地以开源社区的形式发展和演进，让社区开发者能够方便地参与到 KubeSphere 项目的建设，社区宣布将 KubeSphere 前端项目 console 开源，当前开源的版本包含了 KubeSphere 最新发布 v2.1 所有功能的代码，前端的 Feature Map 可以通过这张图快速了解。
Feature Map
前端项目的代码已在 github.com/kubesphere/console 可见，欢迎大家 Star + Fork。此前已有多位社区用户与开发者表示，希望能参与到 KubeSphere 项目的前端贡献，现在大家已经可以从 github.com/kubesphere/kubesphere/issues 通过标签 area/console 找到前端相关的 issue，包括 Bug、feature and design。
至此，KubeSphere 开源社区发布的项目已涵盖了容器平台（KubeSphere）、多云应用管理平台（OpenPitrix）、网络插件（Porter LB 插件、Hostnic-CNI）、存储插件（CSI）、CI/CD（S2i-operator）、日志插件（Fluentbit-operator）、通知告警（Alert &amp;amp; Notification）、身份认证（IAM）等。
KubeSphere 社区架构 KubeSphere 相信 Community over code，一个健康良好的开源社区发展必定离不开 Contributor 的参与。为了让社区相关的事情更加成体系，让社区同学更有归属感，KubeSphere 首次建立了社区架构，第一次公开的架构包括 Developer Group 和 User Group。
 SIG（特别兴趣小组）由开发者和用户共同组成，目前架构中暂未划分主题，未来将根据社区用户的参与和关注方向进行划分。
 Developer Group Developer Group 将以开发者对 KubeSphere 组织下的所有开源项目的贡献数量和质量作为参考，可贡献的项目包括前后端、存储与网络插件、官网文档等项目。
 Active Contributor：2 个月内贡献过超过 4 个 PR，这样即可获得邀请。 Reviewer： 从 Active Contributor 中诞生，当 Active Contributor 对该模块拥有比较深度的贡献，并且得到 2 个或 2 个以上 Maintainer 的提名时，将被邀请成为该模块的 Reviewer，具有 Review PR 的义务。 Maintainer：即该功能模块的组织者，负责项目某个功能模块的代码与版本开发与维护，社区日常运营，包括组织会议，解答疑问等。Tech Lead 需要为项目的管理和成长负责，责任重大。目前暂由 KubeSphere 内部成员担任，将来可根据贡献程度由社区开发者一起担任，共同为项目的进步而努力。  User Group KubeSphere 社区是由开发者和用户共建的，随着 KubeSphere 用户群体愈发壮大，用户在使用过程中遇到的问题反馈及实践经验，对于 KubeSphere 产品的完善及应用推广有着不可忽视的重要作用。</description>
    </item>
    
    <item>
      <title>KubeSphere 容器平台发布 2.1.1，全面支持 Kubernetes 1.17</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-release-note-post/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-release-note-post/</guid>
      <description>农历二月二，KubeSphere 开源社区激动地向大家宣布，KubeSphere 容器平台 2.1.1 正式发布！
KubeSphere 作为 开源的企业级容器平台，对 2.1.1 版本定义的是 进一步增强生产可用性，修复了多个组件的 Bug，升级了内置的多个开源组件。借助 KubeSphere，您可以快速安装与管理原生的 Kubernetes，KubeSphere 2.1.1 已支持至 Kubernetes 1.17，帮助您上手 Kubernetes 新版本中新增的特性。并且，还向前兼容与支持 Kubernetes 1.17 之前的 3 个版本，您可以按需进行安装。
KubeSphere 能够帮助企业快速构建一个功能丰富的容器云平台。让企业在享受 Kubernetes 的弹性伸缩与敏捷部署的同时，还可以在容器平台拥有 IaaS 平台的存储与网络能力，获得与 IaaS 平台一样稳定的用户体验。比如，我们在 KubeSphere 2.1.1 新增了对阿里云与腾讯云块存储插件的集成，支持为 Pod 挂载公有云的存储，为有状态应用提供更稳定的持久化存储的能力。
除此之外，我们还将安装步骤再一次简化。2.1.1 简化了在已有 Kubernetes 上安装的步骤，无需再像 2.1.0 安装一样，配置集群 CA 证书路径。并且，也将 etcd 监控作为了可选安装项。真正实现了一条命令即可在已有的 Kubernetes 集群上快速安装 KubeSphere。
关于 2.1.1 的更新详情，请参考 Release Note。
下面演示两种最简单的安装方法，解锁如何最快尝鲜 KubeSphere 2.1.1。
如何在 Linux 快速安装 2.1.1  本文将演示 All-in-One 安装，请准备一台干净的机器（虚拟机或物理机），安装前关闭防火墙，并确保您的机器符合以下的最小要求：    机器配置:</description>
    </item>
    
    <item>
      <title>KubeSphere 部署 SkyWalking 至 Kubernetes 开启无侵入 APM</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/skywalking-kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/skywalking-kubesphere/</guid>
      <description>Kubernetes 天然适合分布式的微服务应用。然而，当开发者们将应用从传统的架构迁移到 Kubernetes 以后，会发现分布式的应用依旧存在各种各样的问题，例如大量微服务间的调用关系复杂、系统耗时或瓶颈难以排查、服务异常定位困难等一系列应用性能管理问题，而 APM 正是实时监控并管理微服务应用性能的利器。
为什么需要 APM APM 无疑是在大规模的微服务开发与运维场景下是必不可少的一环，APM 需要主要从这三个角度去解决三大场景问题：
 测试角度：性能测试调优监控总览，包括容器总体资源状况（如 CPU、内存、IO）与链路总体状况 研发角度：链路服务的细节颗粒追踪，数据分析与数据安全 运维角度：跟踪请求的处理过程，来对系统在前后端处理、服务端调用的性能消耗进行跟踪，实时感知并追踪访问体验差的业务  为什么选择 Apache SkyWalking 社区拥有很丰富的 APM 解决方案，比如著名的 Pinpoint、Zipkin、SkyWalking、CAT 等。在经过一番调研后，KubeSphere 选择将 Apache SkyWalking 作为面向 Kubernetes 的 APM 开源解决方案，将 Apache SkyWalking 集成到了 KubeSphere，作为应用模板在 KubeSphere 容器平台 提供给用户一键部署至 Kubernetes 的能力，进一步增强在微服务应用维度的可观察性。
Apache SkyWalking 在 2019 年 4 月 17 正式成为 Apache 顶级项目，提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。Apache SkyWalking 专为微服务、云原生和基于容器的架构而设计。这是 KubeSphere 选择 Apache SkyWalking 的主要原因。
并且，Apache SkyWalking 本身还具有很多优势，包括多语言自动探针，比如 Java、.NET Core 和 Node.JS，能够实现无侵入式的探针接入 APM 检测，轻量高效，多种后端存储支持，提供链路拓扑与 Tracing 等优秀的可视化方案，模块化，提供 UI、存储、集群管理多种机制可选，并且支持告警。同时，Apache SkyWalking 还很容易与 SpringCloud 应用进行集成。</description>
    </item>
    
    <item>
      <title>KubeSphere 部署 TiDB 云原生分布式数据库</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/tidb-on-kbesphere-using-qke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/tidb-on-kbesphere-using-qke/</guid>
      <description>TiDB 简介 TiDB 是 PingCAP 公司自主设计、研发的开源分布式关系型数据库，具备水平扩容或者缩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 5.7 协议和 MySQL 生态等重要特性。TiDB 适合高可用、强一致要求较高、数据规模较大等各种应用场景。
KubeSphere 简介 KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。
部署环境准备 KubeSphere 是由青云 QingCloud 开源的容器平台，支持在任何基础设施上安装部署。在青云公有云上支持一键部署 KubeSphere（QKE）。
下面以在青云云平台快速启用 KubeSphere 容器平台为例部署 TiDB 分布式数据库，至少需要准备 3 个可调度的 node 节点。你也可以在任何 Kubernetes 集群或 Linux 系统上安装 KubeSphere，可以参考 KubeSphere 官方文档。
 登录青云控制台：https://console.qingcloud.com/，点击左侧容器平台，选择 KubeSphere，点击创建并选择合适的集群规格：  创建完成后登录到 KubeSphere 平台界面：  点击下方的 Web Kubectl 集群客户端命令行工具，连接到 Kubectl 命令行界面。执行以下命令安装 TiDB Operator CRD：  kubectl apply -f https://raw.githubusercontent.com/pingcap/TiDB-Operator/v1.1.6/manifests/crd.yaml 执行后的返回结果如下：  点击左上角平台管理，选择访问控制，新建企业空间，这里命名为 dev-workspace  进入企业空间，选择应用仓库，添加一个 TiDB 的应用仓库：  将 PingCap 官方 Helm 仓库添加到 KubeSphere 容器平台，Helm 仓库地址如下：  https://charts.</description>
    </item>
    
    <item>
      <title>NetApp 存储在 KubeSphere 上的实践</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/netapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/netapp/</guid>
      <description>NetApp 是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。 Ontap数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。 Trident是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂持久性需求。 KubeSphere 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。
整体方案 在 VMware Workstation 环境下安装 ONTAP; ONTAP 系统上创建 SVM(Storage Virtual Machine) 且对接 nfs 协议；在已有 k8s 环境下部署 Trident,Trident 将使用 ONTAP 系统上提供的信息（svm、managementLIF 和 dataLIF）作为后端来提供卷；在已创建的 k8s 和StorageClass 卷下部署 KubeSphere。
版本信息  Ontap: 9.5 Trident: v19.07 k8s: 1.15 kubesphere: 2.0.2  步骤 主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。
OnTap 搭建及配置 在 VMware Workstation 上 Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide 运行，Ontap 启动之后，按下面操作配置，其中以 cluster base license、feature licenses for the non-ESX build 配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名: cluster1、密码等信息。</description>
    </item>
    
    <item>
      <title>OpenPitrix Insight</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/openpitrix-insight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/openpitrix-insight/</guid>
      <description>云计算在今天已经被绝大多数的企业所采用，具知名云服务厂商 RightScale 最近的调查显示，已经有越来越多的厂商采用多云管理。客户有太多的理由来选择多云管理了，其中最大的原因莫过于采用单一的供应商，会导致被锁定。因此，如何管理多云环境，并在多云的环境下进行自动化，正成为众多企业的刚需，而在这其中，应用程序的管理显得尤为的重要。进一步讲，颇具挑战的是创建一个一站式的应用管理平台，来管理不同类型的应用程序，其中包括传统的应用（或者称之为单体应用，或者传统的主从、分片、peer-to-peer 架构的企业分布式应用）、微服务应用、以及近来发展迅猛的 Serverless 应用等，OpenPitrix 就是为了解决这些问题而生的。用一句话来描述 OpenPitrix：
 OpenPitrix 是一款开源项目，用来在多云环境下打包、部署和管理不同类型的应用，包括传统应用、微服务应用以及 Serverless 应用等，其中云平台包括 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等。
 微服务，即众所周知的微服务架构，这是程序设计的必然趋势，企业创建新的应用时选择的主要方式。另外，开源项目 Kubernetes 已经成为事实上的编排平台的领导者，其在自动化部署、扩展性、以及管理容器化的应用有着独特的优势。但是，仍然有大量的传统遗留应用用户想在毋须改变其架构的情况下迁入到云平台中，而且对很多用户来讲，采用微服务架构，或者是 Serverless 架构还是比较遥远的事情，所以，我们需要帮助这些用户将他们的传统应用迁入到云计算平台中，这也是 OpenPitrix 很重要的一个功能。
在2017年3月27日，QingCloud 发布 AppCenter，一款旨在为传统企业应用开发商和云用户之间架设友好桥梁的平台，该平台最大的亮点在于其可以让开发者以极低的学习成本就可以将传统的应用程序移植到 QingCloud 中运行，并且具有云计算的所有特性，如敏捷性、伸缩性、稳定性、监控等。通常，一位开发者只需花上几个小时就可以理解整个工作流程，然后，再花一到两周的时间(这具体要取决于应用的复杂性)将应用移植到云平台中。该平台上线之后一直颇受用户的青睐和夸赞，但有一些用户提出更多的需求，希望将之部署到他们内部来管理他们的多云环境。为了满足用户的需求，QingCloud 将之扩展，即在多云的环境下管理多种类型的应用程序，并且采用开源的方法来进行项目的良性发展。
俗语有云：&amp;ldquo;知易行难&amp;rdquo;，尽管 OpenPitrix 原始团队在云计算应用开发有着足够丰富的经验，并成功的开发出了稳定的商业化产品：AppCenter，要知道，等待在前方的依然有很多困难要克服。OpenPitrix 从一开始就是以开源的方式来进行，并且在2017年的8月份在 GitHub 上创建了组织和项目，一直到2018年2月24日才写下第一行功能代码，在此期间，团队的所有成员都在思考系统的每个关键点，这些讨论的细节均可在 GitHub 上公开访问。
以上便是 OpenPitrix 项目的来龙去脉介绍，接下来会解释一些详细的功能和设计细节。
主要的功能 OpenPitrix 所希望实现的功能包括以下内容：
 支持多个云平台，如 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等等; 云平台的支持是高度可扩展和插拔的; 支持多种应用程序的类型：传统应用、微服务应用、Serverless 应用; 应用程序的支持也是高度可扩展的，这也就意味着无论将来出现哪种新的应用程序类型，OpenPitrix 平台都可以通过添加相应的插件来支持它; 应用程序的仓库是可配置的，这也就意味着由 OpenPitrix 所驱动的商店，其应用均是可以用来交易的; 应用程序库的可见性是可配置的，包括公开、私有或仅让某特定的一组用户可访问，由 OpenPitrix 所驱动的市场，每个供应商都能够操作属于她/他自己的应用商店。  用户场景实例 OpenPitrix 典型的用户场景有：
 某企业是采用了多云的系统（包括混合云），要实现一站式的应用管理平台，从而实现应用的部署和管理； 云管平台（CMP）可以将 OpenPitrix 视为其其中一个组件，以实现在多云环境下管理应用； 可以作为 Kubernetes 的一个应用管理系统。OpenPitrix 和 Helm 有着本质上的不同，虽然 OpenPitrix 底层用了 Helm 来部署 Kubernetes 应用，但 OpenPitrix 着眼于应用的全生命周期管理，比如在企业中，通常会按照应用的状态来分类，如开发、测试、预览、生产等；甚至有些组织还会按照部门来归类，而这是 Helm 所没有的。  架构概览 OpenPitrix 设计的最根本的思想就是解耦应用和应用运行时环境（此处使用运行时环境代替云平台，下同），如下图所示。应用程序能够运行在哪个环境，除了需要匹配 provider 信息之外，还需要匹配应用所在仓库的选择器 (selector) 和运行时环境的标签 (label)，即当某个最终用户从商店里选择了某个具体的应用，然后尝试部署它时，系统会自动选择运行时环境。如果有多个运行时环境可以运行此应用的话，则系统会弹出相应的对话框来让用户自行选择，更多设计细节请参考 OpenPitrix 设计文档。</description>
    </item>
    
    <item>
      <title>一文说清 KubeSphere 容器平台的价值</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-values/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-values/</guid>
      <description>KubeSphere 作为云原生家族 后起之秀，开源近两年的时间以来收获了诸多用户与开发者的认可。本文通过大白话从零诠释 KubeSphere 的定位与价值，以及不同团队为什么会选择 KubeSphere。
对于企业 KubeSphere 是什么 KubeSphere 是在 Kubernetes 之上构建的 多租户 容器平台，以应用为中心，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。使用 KubeSphere 不仅能够帮助企业在公有云或私有化数据中心快速搭建 Kubernetes 集群，还提供了一套功能丰富的向导式操作界面。
KubeSphere 能够帮助企业快速构建一个功能丰富的容器云平台，让企业在享受 Kubernetes 的弹性伸缩与敏捷部署的同时，还可以在容器平台拥有 IaaS 平台的存储与网络能力，获得与 IaaS 一样稳定的用户体验。比如在 KubeSphere 2.1.1 新增了对阿里云与腾讯云块存储插件的集成，支持为 Pod 挂载公有云的存储，为有状态应用提供更稳定的持久化存储的能力。
在日常的运维开发中，我们可能需要使用与管理大量的开源工具，频繁地在不同工具的 GUI 和 CLI 窗口操作，每一个工具的单独安装、使用与运维都会带来一定的学习成本，而 KubeSphere 容器平台能够统一纳管与对接这些工具，提供一致性的用户体验。这意味着，我们不需要再去多线程频繁地在各种开源组件的控制面板窗口和命令行终端切换，极大赋能企业中的开发和运维团队，提高生产效率。
对于开发者 KubeSphere 是什么 有很多用户习惯把 KubeSphere 定义为 “云原生全家桶”。不难理解，KubeSphere 就像是一个一揽子解决方案，我们设计了一套完整的管理界面，开发与运维在一个统一的平台中，可以非常方便地安装与管理用户最常用的云原生工具，从业务视角提供了一致的用户体验来降低复杂性。为了不影响底层 Kubernetes 本身的灵活性，也为了让用户能够按需安装，KubeSphere 所有功能组件都是可插拔的。
KubeSphere 基于 OpenPitrix 和 Helm 提供了应用商店，对内可作为团队间共享企业内部的中间件、大数据、APM 和业务应用等，方便开发者一键部署应用至 Kubernetes 中；对外可作为根据行业特性构建行业交付标准、交付流程和应用生命周期管理的基础，作为行业通用的应用商店，可根据不同需求应对不同的业务场景。在 3.0 版本还将支持计量 (Metering)，方便企业对应用与集群资源消耗的成本进行管理。
对于运维 KubeSphere 是什么 可观察性是容器云平台非常关键的一环，狭义上主要包含监控、日志和追踪等，广义上还包括告警、事件、审计等。对于 Kubernetes 运维人员来说，通常需要搭建和运维一整套可观察性的技术架构，例如 Prometheus + Grafana + AlertManager、EFK 等等。并且，企业通常还需要对不同租户能够看到的监控、日志、事件、审计等信息，实现按不同租户隔离，这些需求的引入无疑会增大企业的运维成本与复杂性。</description>
    </item>
    
    <item>
      <title>使用 KubeSphere DevOps 搭建自动化测试系统</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/devops-automatic-testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/devops-automatic-testing/</guid>
      <description>测试分层 测试的目的是为了验证预期的功能，发现潜在的缺陷。测试增强了交付合格产品的信心，也给敏捷迭代带来了可能。可以说，测试决定了产品的开发进度。
网络模型有七层的 OSI 、四层的 TCP，而开发模式有 MTV、MVC、MVP、MVVM 等。高内聚、低耦合，划分职责、分模块、分层。然后结构化、标准化，技术逐步走向成熟。
测试也分为，UI 测试、API 测试、单元测试。测试并不是一项新技术，更多是产出与成本的一种平衡。
如上图，是一个测试金字塔。越往上，需要的成本越高，对环境要求越高，执行时间越长，维护越麻烦，但更贴近终端用户的场景。在 《Google软件测试之道》中，按照谷歌的经验，各层测试用例比例是 70：20：10，也就是 70% 的单元测试，20% 的 API 测试，10% 的 UI 测试。
本篇主要讲的是如何在 KubeSphere 平台上使用 KubeSphere DevOps 系统 运行自动化测试。
什么是 KubeSphere DevOps KubeSphere 针对容器与 Kubernetes 的应用场景，基于 Jenkins 提供了一站式 DevOps 系统，包括丰富的 CI/CD 流水线构建与插件管理功能，还提供 Binary-to-Image（B2I）、Source-to-Image（S2I），为流水线、S2I、B2I 提供代码依赖缓存支持，以及代码质量管理与流水线日志等功能。
KubeSphere 内置的 DevOps 系统将应用的开发和自动发布与容器平台进行了很好的结合，还支持对接第三方的私有镜像仓库和代码仓库形成完善的私有场景下的 CI/CD，提供了端到端的用户体验。
但是，很少有用户知道，KubeSphere DevOps 还可以用来搭建自动化测试系统，为自动化的单元测试、API 测试和 UI 测试带来极大的便利性，提高测试人员的工作效率。
单元测试 单元测试的运行频率非常高，每次提交代码都应该触发一次。单元测试的依赖少，通常只需要一个容器运行环境即可。
下面是一个使用 golang:latest 跑单元测试的例子。
pipeline {agent {node {label &#39;go&#39;}}stages {stage(&#39;testing&#39;) {steps {container(&#39;go&#39;) {sh &#39;&#39;&#39;git clone https://github.</description>
    </item>
    
    <item>
      <title>使用 KubeSphere 在 Kubernetes 安装 cert-manager 为网站启用 HTTPS</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/install-cert-managner-on-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/install-cert-managner-on-k8s/</guid>
      <description>什么是 cert-manager cert-manager（https://cert-manager.io/）是 Kubernetes 原生的证书管理控制器。它可以帮助从各种来源颁发证书，例如 Let&amp;rsquo;s Encrypt，HashiCorp Vault，Venafi，简单的签名密钥对或自签名。它将确保证书有效并且是最新的，并在证书到期前尝试在配置的时间续订证书。它大致基于 kube-lego 的原理，并从其他类似项目（例如 kube-cert-manager）中借鉴了一些智慧。
准备工作  需要一个公网可访问的 IP，例如 139.198.121.121 需要一个域名，并且已经解析到到对应的IP，例如  A kubesphere.io 139.198.121.121，我们将 staging.kubesphere.io 域名解析到了 139.198.121.121 在KubeSphere上已经运行网站对应的服务，例如本例中的ks-console  启用项目网关 登录 KubeSphere，进入任意一个企业空间下的项目中。
在 KubeSphere 中启用对应项目下的网关。
 我们开启的是一个NodePort类型的网关，需要在集群外部使用 LoadBalancer 转发到网关的端口，将 139.198.121.121 绑定到 LoadBalancer 上，这样我们就可以通过公网IP直接访问我们的服务了； 如果 Kubernetes 集群是在物理机上，可以安装 Porter（https://porter.kubesphere.io）负载均衡器对外暴露集群服务； 如果在公有云上，可以安装和配置公有云支持的负载均衡器插件，然后创建 LoadBalancer 类型的网关，填入公网IP对应的 eip，会自动创建好负载均衡器，并将端口转发到网关。
 安装 cert-manager 详细安装文档可以参考 cert-manager。
 cert-manager 部署时会创建一个 webhook 来校验 cert-manager 相关对象是否符合格式，不过也会增加部署的复杂性。这里我们使用官方提供的一个 no-webhook 版本安装。
 可以在 KubeSphere 右下角的工具箱中，打开 Web Kubectl。
在 Web Kubectl 执行下列命令安装 cert-manager：</description>
    </item>
    
    <item>
      <title>在 KubeSphere 安装 Orion vGPU 使用 TensorFlow 运行深度学习训练</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-orion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/kubesphere-orion/</guid>
      <description>概览 本文将使用 KubeSphere 容器平台，在 Kubernetes 上部署 Orion vGPU 软件 进行深度学习加速，并基于 Orion vGPU 软件使用经典的 Jupyter Notebook 进行模型训练与推理。
在开始安装 Orion vGPU 和演示深度学习训练之前，先简单了解一下，什么是 vGPU 以及什么是 Orion vGPU。
什么是 vGPU vGPU 又称 虚拟 GPU，早在几年前就由 NVIDIA 推出了这个概念以及相关的产品。vGPU 是通过对数据中心（物理机）的 GPU 进行虚拟化，用户可在多个虚拟机或容器中 共享该数据中心的物理 GPU 资源，有效地提高性能并降低成本。vGPU 使得 GPU 与用户之间的关系不再是一对一，而是 一对多。
为什么需要 vGPU 随着 AI 技术的快速发展，越来越多的企业开始将 AI 技术应用到自身业务之中。目前，云端 AI 算力主要由三类 AI 加速器来提供：GPU，FPGA 和 AI ASIC 芯片。这些加速器的优点是性能非常高，缺点是 成本高昂，缺少异构加速管理和调度。大部分企业因无法构建高效的加速器资源池，而不得不独占式地使用这些昂贵的加速器资源，导致 资源利用率低，成本高。
以 GPU 为例，通过创新的 vGPU 虚拟化技术，能够帮助用户无需任务修改就能透明地共享和使用数据中心内任何服务器之上的 AI 加速器，不但能够帮助用户提高资源利用率，而且可以 极大便利 AI 应用的部署，构建数据中心级的 AI 加速器资源池。</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 的 CI/CD 利器 — Prow 入门指南</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/prow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/prow/</guid>
      <description>Prow是k8s使用的CI/CD系统(https://github.com/kubernetes/test-infra/tree/master/prow)，用于管理k8s的issue和pr。如果你经常去k8s社区查看pr或者提交过一些Pr后，就会经常看到一个叫k8s-ci-bot的机器人在各个Pr中回复，并且还能合并pr。在k8s-ci-bot中背后工作的就是Prow。Prow是为了弥补github上一些功能上的缺陷，它也是Jenkins-X的一部分，它具备这些功能：
 执行各种Job，包括测试，批处理和制品发布等，能够基于github webhook配置job执行的时间和内容。 一个可插拔的机器人功能（Tide），能够接受/foo这种样式的指令。 自动合并Pr 自带一个网页，能够查看当前任务的执行情况以及Pr的状况，也包括一些帮助信息 基于OWNER文件在同一个repo里配置模块的负责人 能够同时处理很多repo的很多pr 能够导出Prometheus指标  Prow拥有自己的CI/CD系统，但是也能与我们常见的CI/CD一起协作，所以如果你已经习惯了Jenkins或者travis，都可以使用Prow。
安装指南  官方repo提供了一个基于GKE快速安装指南，本文将基于青云的Iaas搭建Prow环境。不用担心，其中大部分步骤都是平台无关的，整个安装过程能够很方便的在其他平台上使用。
 一、 准备一个kubernetes集群 有以下多种方式准备一个集群
 利用kubeadm自建集群 在青云控制台上点击左侧的容器平台，选择其中的QKE，简单设置一些参数之后，就可以很快创建一个kubernetes集群。 将集群的kubeconfig复制到本地，请确保在本地运行kubectl cluster-info正确无误  二、 准备一个github机器人账号  如果没有机器人账号，用个人账号也可以。机器人账号便于区分哪些Prow的行为，所以正式使用时应该用机器人账号。
   在想要用prow管理的仓库中将机器人账号设置为管理员。
  在账号设置中添加一个[personal access token][1]，此token需要有以下权限：
 必须：public_repo 和 repo:status 可选：repo假如需要用于一些私有repo 可选：admin_org:hook 如果想要用于一个组织    将此Token保存在文件中，比如${HOME}/secrets/oauth
  用openssl rand -hex 20生成一个随机字符串用于验证webhook。将此字符串保存在本地，比如${HOME}/secrets/h-mac
  注意最后两步创建的token一定需要保存好，除了需要上传到k8s，后续配置也要用到，用于双向验证
三、 配置k8s集群  这里使用的default命名空间配置prow，如果需要配置在其他命名空间，需要在相关kubectl的命令中配置-n参数，并且在部署的yaml中配置命名空间。 建议将本repo克隆到本地，这个repo带有很多帮助配置Prow的小工具。
  将上一步中创建token和hmac保存在k8s集群中  # openssl rand -hex 20 &amp;gt; ${HOME}/secrets/h-mac kubectl create secret generic hmac-token --from-file=hmac=${HOME}/secrets/h-mac kubectl create secret generic oauth-token --from-file=oauth=${HOME}/secrets/oauth 部署Prow。由于Prow官方yaml中使用了grc.</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 部署 node.js APP</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/nodejs-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/nodejs-app/</guid>
      <description>什么是 Kubernetes Kubernetes 是一个开源容器编排引擎，可以帮助开发者或运维人员部署和管理容器化的应用，能够轻松完成日常开发运维过程中诸如 滚动更新，横向自动扩容，服务发现，负载均衡等需求。了解更多
安装 Kubernetes 可以通过快速安装 kubernetes 集群：
 KubeSphere Installer minikube kubeadm  Kubernetes 术语介绍 Pod Pod 是 Kubernetes 最小调度单位，是一个或一组容器的集合。
Deployment 提供对 Pod 的声明式副本控制。指定 Pod 模版，Pod 副本数量, 更新策略等。
Service Service 定义了 Pod 的逻辑分组和一种可以访问它们的策略。借助Service，应用可以方便的实现服务发现与负载均衡。
Label &amp;amp;amp; Selector Kubernetes 中使用 Label 去关联各个资源。
 通过资源对象(Deployment, etc.)上定义的 Label Selector 来筛选 Pod 数量。 通过 Service 的 Label Selector 来选择对应的 Pod， 自动建立起每个 Service 到对应 Pod 的请求转发路由表。 通过对某些 Node 定义特定的 Label，并且在 Pod 中添加 NodeSelector 属性，可以实现 Pod 的定向调度(运行在哪些节点上)。  Nodejs 模板项目 node-express-realworld-example-app 是一款 node.</description>
    </item>
    
    <item>
      <title>基于 KubeSphere 的 Spring Could 微服务 CI/CD 实践</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/spring-cloud-on-kubeshpere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/spring-cloud-on-kubeshpere/</guid>
      <description>本文以 Pig 为例介绍如何在 KubeSphere 上发布一个基于 Spring Cloud 微服务的 CI/CD 项目。
背景简介 Pig Pig (http://pig4cloud.cn/) 是一个基于 Spring Cloud 的开源微服务开发平台，也是微服务最佳实践。在国内拥有大量拥护者。同时也有商业版本提供技术支持。
KubeSphere KubeSphere (https://Kubesphere.io) 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。
通过 KubeSphere 我们可以以简洁的方式将 Pig 项目部署至 Kubernetes 中。运维人员可以轻松的完成 Spring Cloud 运维任务。
前提条件 具备 Spring Cloud 及 Pig 基础知识
Jenkins 基础知识（非必备）
KubeSphere 3.0 集群环境一套，并启用 DevOps 插件
 搭建 KubeSphere 集群不再本文覆盖范围，根据您的环境参考相关部署文档: https://KubeSphere.com.cn/docs/installing-on-kubernetes/
 架构设计 Spring Cloud 有一个丰富、完备的插件体系，以实现各种运行时概念，作为应用栈的一部分。因此，这些微服务自身有库和运行时代理，来做客户端的服务发现，配置管理，负载均衡，熔断，监控，服务跟踪等功能。由于本篇重点在于如何快速建立 CI/CD 运维体系，因此对 Spring Cloud 与 Kubernetes 的深度整合不做过多讨论。我们将继续使用 Spring Cloud 底层的这些能力，同时利用 Kubernetes 实现滚动升级，健康检查，服务自动恢复等缺失的功能。</description>
    </item>
    
    <item>
      <title>如何创建跨 Kubernetes 集群的流水线</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/create-pipeline-across-multi-clusters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/create-pipeline-across-multi-clusters/</guid>
      <description>随着 Kubernetes 被广泛的作为基础设施，各大云厂商都相继推出了各自的 Kubernetes 集群服务。那么在多个集群上，如何跨集群实践 DevOps 流水线呢？本文将主要以示例的形式给出回答。
 提示，本文需要对 KubeSphere 和 DevOps 相关的知识具有一定了解，具体包括 Kubernetes 资源创建、生成 Sonarqube Token、获取集群 kubeconfig 等。
 KubeSphere DevOps 跨集群架构概览 集群角色说明 在多集群架构中，我们对集群进行了角色定义。每个集群的角色，在安装的配置文件中指定。具体详情，可以参考文档: kubefed-in-kubesphere 。一共有三种角色，host、member、none 。
 host  安装完整的 KubeSphere 核心组件，通过前端页面可以对各个集群进行管理。
 member  没有安装 KubeSpher Console 前端组件，不提供独立的页面管理入口，可以通过 host 集群的前端入口进行管理。
 none  没有定义角色，主要是为了兼容单集群模式。
多集群下 DevOps 的部署架构 上图，是 KubeSphere DevOps 在多集群上的部署架构。在每一个集群上，我们都可以选择性安装 DevOps 组件。具体安装步骤，与单集群开启 DevOps 组件没有区别。
在创建 DevOps 工程时，前端会对可选集群进行标记，仅开启 DevOps 组件的集群能够被选择。
用户场景描述 上图是一个 Demo 场景，通过多集群隔离不同的部署环境。一共有三个集群，开发集群、测试集群、生成集群。
开发人员在提交代码之后，可以触发流水线执行，依次完成，单元测试、代码检测、构建和推送镜像，然后直接部署到开发集群。开发集群交给开发人员自主管控，作为他们的自测验证环境。经过审批之后，可以发布到测试环境，进行更严格的验证。最后，经过授权之后，发布到正式环境，用于对外提供服务。
创建一条跨集群的流水线 准备集群 这里准备了三个集群，分别为：</description>
    </item>
    
    <item>
      <title>微服务进阶之路 容器落地避坑指南</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/microservice-blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/microservice-blog/</guid>
      <description>微服务架构相对于单体架构有很大的变化，也产生了一些新的设计模式，比如 sidecar，如何开发一个微服务应用是一件有很大挑战性的事情，我们经常会听到有人讨论如何划分微服务，多细的颗粒度才是微服务等问题。初学者经常会处于一个“忐忑不安”的状态，所以我们急需要知道如何才能走上正确的微服务道路，或者需要一些最佳实践指导我们如何设计、开发一个微服务应用。
不骄不躁不跟风 知己知彼方可百战不殆 虽然现在已经进入到一个不谈微服务就落伍的时代，但作为 IT 从业者，我们一定要站在切身利益出发，多思考几个“为什么”，不要急于跟风。原因很简单，不管外面如何风吹雨打，只要你的房子足够结实、安全、舒服，那一般情况下就不需要拆除重建，所以在决定继续沿用单体架构还是转向微服务架构之前，我们一定要做两件事情：
第一件事，从外部了解两种架构各自的优劣： 可以看到，单体应用并不是一无是处。
第二件事，审视我们自己的业务：  上述单体架构列出的一些问题是否已经严重影响了我们的业务？ 企业新的业务系统是否要满足快速迭代、弹性等需求？ 团队内是否有 DevOps 氛围？ 企业内是否有足够的动力和技术储备去接触新的技术？  了解了单体应用和微服务应用的优劣特点，分析了企业自身的业务诉求和实际情况，最终还是决定转型微服务架构，那么我们也要清楚这不是一朝一夕的事情，需要分阶段逐步推进。
蒙眼狂奔不可取 循序渐进方可顺利进阶 第一阶段试炼—— 开发新应用 对于初次接触微服务的企业，选择新应用入手是正确的方式。
第一步可以选择 web-scale、无状态类型的新应用上手，比如基于 nginx 的网站、文档等，这类应用非常简单且容易实现，而且能体验到微服务在容器平台上的各种功能。
有了一定的经验之后，第二步就可以开发有状态类型的新应用，有状态服务的最大挑战就是数据管理。
敲重点，跟以往单体应用的共享数据库不同，微服务应用中的每一个服务“独享”自己的数据库，服务之间需要通过 API、事件或消息传递的方式来相互访问对方的数据，而不是通过直接访问对方数据库的方式。
换句话说，理想中的微服务是封装自己的数据，通过API暴露数据出去，从而避免数据耦合，这样每个微服务的数据格式发生变化也不影响其它微服务的数据调用。开发过和升级过大型企业单体应用的人对此会深有体会，一旦有人改变了数据库 schema，整个应用都有可能启动不起来，团队开发效率会大大降低。
微服务架构并不尽善尽美，适合自己的方案才是王道。
不难理解，微服务数据是牺牲强一致性而通过最终一致性的方式来管理，这对数据的划分带来很大难度，比如不能再用 join 的方式访问不同服务之间的数据表，实际当中也比较难做到或者做起来很麻烦，现在也没有成熟且好用的库或框架提供微服务的数据管理，而且某些应用确实需要强一致性。
而此时，我们不能通盘否定此类应用微服务化的可行性，应该适当折中或“妥协”，采用 miniservice。
Miniservice 在开发与部署的独立性和敏捷性方面类似于微服务(microservice)，但没有微服务那么强的约束。通常情况下，一个 miniservcie 可以提供多个功能，这些功能之间可以共享数据库。这个时候千万不要害怕混合架构，不要害怕自己的微服务应用是否“正统”，“think big，start small，move fast“才是我们应该遵循的哲学。
因此，一个企业应用里既有 microservice 也有 miniservice，甚至有单体部分（可以称之为 macroservice）都是可以接受的。
以一个电商平台举例，在整个场景里面，业务开发人员面对的主要压力来自前端频繁的变动，因为要应对频繁的促销、推广、降价等活动，所以面对消费者最前端的业务需要快速迭代。消费者会不停的浏览商品，最终产生交易的请求数量要远低于获取商品信息的请求数量，因此将前端业务无状态化，进行微服务拆分、解耦，便可以快速应对市场变化，灵活做出改变。
那是不是把整个平台都做到微服务级别会变得更好？答案是“不确定”，因为当微服务量级到达一定程度，由此产生的管理和运维压力是指数级增长的。而实际上，对于有些业务来讲也没有必要微服务化，比如很多电商平台都有 2B 的业务，其业务变化的频度和压力没有 2C 那么大，那以 macroservices 或者 miniservices 的方式去交付也是可以的。
开发人员应该分析在整个应用架构体系中，哪些适合微服务化，哪些亟需微服务化。
实践出真知 在上面的电商案例中，我们提到了服务无状态化，之所以期望服务无状态化，是因为无状态应用可以做到快速的扩缩容，可以应对井喷流量，可以最大效率的利用计算资源。
我们经常听到，以无状态为荣，以有状态为耻，说的就是对于一个服务要尽量无状态化它，比如用户 session 管理，以前我们在业务逻辑模块进行管理，导致这些模块不能按照无状态方式任意伸缩。我们可以把这些 session 的管理抽取出来放到一个高可用或分布式的缓存中管理，业务模块通过调用API的方式去获取 session，这样就实现了这些模块的无状态化。</description>
    </item>
    
    <item>
      <title>手把手从零部署与运营生产级的 Kubernetes 集群与 KubeSphere</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/kubernetes-kubesphere-ha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/kubernetes-kubesphere-ha/</guid>
      <description>本文来自 KubeSphere 社区用户 Liu_wt 投稿，欢迎所有社区用户参与投稿或分享经验案例。
 本文将从零开始，在干净的机器上安装 Docker、Kubernetes (使用 kubeadm)、Calico、Helm 与 KubeSphere，通过手把手的教程演示如何搭建一个高可用生产级的 Kubernetes，并在 Kubernetes 集群之上安装 KubeSphere 容器平台可视化运营集群环境。
一、准备环境 开始部署之前，请先确定当前满足如下条件，本次集群搭建，所有机器处于同一内网网段，并且可以互相通信。
⚠️⚠️⚠️：请详细阅读第一部分，后面的所有操作都是基于这个环境的，为了避免后面部署集群出现各种各样的问题，强烈建议你完全满足第一部分的环境要求
  两台以上主机 每台主机的主机名、Mac 地址、UUID 不相同 CentOS 7（本文用 7.6/7.7） 每台机器最好有 2G 内存或以上 Control-plane/Master至少 2U 或以上 各个主机之间网络相通 禁用交换分区 禁用 SELINUX 关闭防火墙（我自己的选择，你也可以设置相关防火墙规则） Control-plane/Master和Worker节点分别开放如下端口   Master节点
   协议 方向 端口范围 作用 使用者     TCP 入站 6443* Kubernetes API 服务器 所有组件   TCP 入站 2379-2380 etcd server client API kube-apiserver, etcd   TCP 入站 10250 Kubelet API kubelet 自身、控制平面组件   TCP 入站 10251 kube-scheduler kube-scheduler 自身   TCP 入站 10252 kube-controller-manager kube-controller-manager 自身    Worker节点</description>
    </item>
    
    <item>
      <title>本来生活的 DevOps 升级之路</title>
      <link>https://kubesphere-v3.netlify.app/zh/blogs/benlai-devops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere-v3.netlify.app/zh/blogs/benlai-devops/</guid>
      <description>我叫杨杨，就职于本来生活网（Benlai.com），负责发布系统架构。我们公司咋说呢，简单说就是卖水果、蔬菜的😄，下面还是来一段官方介绍。
本来生活简介 本来生活网创办于 2012 年，是一个专注于食品、水果、蔬菜的电商网站，从优质食品供应基地、供应商中精挑细选，剔除中间环节，提供冷链配送、食材食品直送到家服务。致力于通过保障食品安全、提供冷链宅配、基地直送来改善中国食品安全现状，成为中国优质食品提供者。
技术现状 基础设施   部署在 IDC 机房 拥有 100 多台物理机 虚拟化部署   存在的问题   物理机 95% 以上的占用率 相当多的资源闲置 应用扩容比较慢   拥抱 DevOps 与 Kubernetes 公司走上容器平台的 DevOps 这条康庄大道主要目标有三：
 1、提高资源利用率
2、提高发布效率
3、降低运维的工作成本等等
 其实最主要的还是 省钱，对就是 省钱。接下来就是介绍我们本来生活的 DevOps 升级之路：
Level 1：工具选型 我们从初步接触 DevOps 相关知识，在此期间偶然了解到开源的 KubeSphere (kubesphere.io)。KubeSphere 是在 Kubernetes 之上构建的以应用为中心的企业级容器平台，支持敏捷开发与自动化运维、DevOps、微服务治理、灰度发布、多租户管理、监控告警、日志查询与收集、应用商店、存储管理、网络管理等多种业务场景。
KubeSphere 内置的基于 Jenkins 的 DevOps 流水线非常适合我们，并且还打通了我们日常运维开发中需要的云原生工具生态，这个平台正是我们当初希望自己开发实现的。
于是，我们开始学习 KubeSphere 与 Jenkins 的各种操作、语法、插件等，开始构建适合我们自己的 CI/CD 的整个流程。最终结合 KubeSphere 容器平台，初步实现了第一级的 CI/CD 流程。</description>
    </item>
    
  </channel>
</rss>